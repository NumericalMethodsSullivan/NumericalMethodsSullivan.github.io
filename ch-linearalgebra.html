<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Linear Algebra | Numerical Methods</title>
  <meta name="description" content="Inquiry Based Numerical Methods" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Linear Algebra | Numerical Methods" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Inquiry Based Numerical Methods" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Linear Algebra | Numerical Methods" />
  
  <meta name="twitter:description" content="Inquiry Based Numerical Methods" />
  

<meta name="author" content="Eric Sullivan" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ch-calculus.html"/>
<link rel="next" href="ch-odes.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Front Matter</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#resources"><i class="fa fa-check"></i>Resources</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#preface"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#creative-commons"><i class="fa fa-check"></i>Creative Commons</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#to-the-student"><i class="fa fa-check"></i>To The Student</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#the-inquiry-based-approach"><i class="fa fa-check"></i>The Inquiry-Based Approach</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#to-the-instructor"><i class="fa fa-check"></i>To the Instructor</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#the-inquiry-based-approach-1"><i class="fa fa-check"></i>The Inquiry-Based Approach</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#the-projects"><i class="fa fa-check"></i>The Projects</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#coding"><i class="fa fa-check"></i>Coding</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#pacing"><i class="fa fa-check"></i>Pacing</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#other-considerations"><i class="fa fa-check"></i>Other Considerations:</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ch-intro.html"><a href="ch-intro.html"><i class="fa fa-check"></i><b>1</b> Preliminary Topics</a><ul>
<li class="chapter" data-level="1.1" data-path="ch-intro.html"><a href="ch-intro.html#what-is-numerical-analysis"><i class="fa fa-check"></i><b>1.1</b> What Is Numerical Analysis?</a></li>
<li class="chapter" data-level="1.2" data-path="ch-intro.html"><a href="ch-intro.html#arithmetic-in-base-2"><i class="fa fa-check"></i><b>1.2</b> Arithmetic in Base 2</a></li>
<li class="chapter" data-level="1.3" data-path="ch-intro.html"><a href="ch-intro.html#floating-point-arithmetic"><i class="fa fa-check"></i><b>1.3</b> Floating Point Arithmetic</a></li>
<li class="chapter" data-level="1.4" data-path="ch-intro.html"><a href="ch-intro.html#sec:Taylor"><i class="fa fa-check"></i><b>1.4</b> Approximating Functions</a></li>
<li class="chapter" data-level="1.5" data-path="ch-intro.html"><a href="ch-intro.html#approximation-error-with-taylor-series"><i class="fa fa-check"></i><b>1.5</b> Approximation Error with Taylor Series</a></li>
<li class="chapter" data-level="1.6" data-path="ch-intro.html"><a href="ch-intro.html#exercises"><i class="fa fa-check"></i><b>1.6</b> Exercises</a><ul>
<li class="chapter" data-level="1.6.1" data-path="ch-intro.html"><a href="ch-intro.html#coding-exercises"><i class="fa fa-check"></i><b>1.6.1</b> Coding Exercises</a></li>
<li class="chapter" data-level="1.6.2" data-path="ch-intro.html"><a href="ch-intro.html#applying-what-youve-learned"><i class="fa fa-check"></i><b>1.6.2</b> Applying What You’ve Learned</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-algebra.html"><a href="ch-algebra.html"><i class="fa fa-check"></i><b>2</b> Algebra</a><ul>
<li class="chapter" data-level="2.1" data-path="ch-algebra.html"><a href="ch-algebra.html#intro-to-numerical-root-finding"><i class="fa fa-check"></i><b>2.1</b> Intro to Numerical Root Finding</a></li>
<li class="chapter" data-level="2.2" data-path="ch-algebra.html"><a href="ch-algebra.html#the-bisection-method"><i class="fa fa-check"></i><b>2.2</b> The Bisection Method</a><ul>
<li class="chapter" data-level="2.2.1" data-path="ch-algebra.html"><a href="ch-algebra.html#intuition-and-implementation"><i class="fa fa-check"></i><b>2.2.1</b> Intuition and Implementation</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch-algebra.html"><a href="ch-algebra.html#analysis"><i class="fa fa-check"></i><b>2.2.2</b> Analysis</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="ch-algebra.html"><a href="ch-algebra.html#the-regula-falsi-method"><i class="fa fa-check"></i><b>2.3</b> The Regula Falsi Method</a><ul>
<li class="chapter" data-level="2.3.1" data-path="ch-algebra.html"><a href="ch-algebra.html#intuition-and-implementation-1"><i class="fa fa-check"></i><b>2.3.1</b> Intuition and Implementation</a></li>
<li class="chapter" data-level="2.3.2" data-path="ch-algebra.html"><a href="ch-algebra.html#analysis-1"><i class="fa fa-check"></i><b>2.3.2</b> Analysis</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="ch-algebra.html"><a href="ch-algebra.html#newtons-method"><i class="fa fa-check"></i><b>2.4</b> Newton’s Method</a><ul>
<li class="chapter" data-level="2.4.1" data-path="ch-algebra.html"><a href="ch-algebra.html#intuition-and-implementation-2"><i class="fa fa-check"></i><b>2.4.1</b> Intuition and Implementation</a></li>
<li class="chapter" data-level="2.4.2" data-path="ch-algebra.html"><a href="ch-algebra.html#analysis-2"><i class="fa fa-check"></i><b>2.4.2</b> Analysis</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="ch-algebra.html"><a href="ch-algebra.html#the-secant-method"><i class="fa fa-check"></i><b>2.5</b> The Secant Method</a><ul>
<li class="chapter" data-level="2.5.1" data-path="ch-algebra.html"><a href="ch-algebra.html#intuition-and-implementation-3"><i class="fa fa-check"></i><b>2.5.1</b> Intuition and Implementation</a></li>
<li class="chapter" data-level="2.5.2" data-path="ch-algebra.html"><a href="ch-algebra.html#analysis-3"><i class="fa fa-check"></i><b>2.5.2</b> Analysis</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="ch-algebra.html"><a href="ch-algebra.html#exercises-1"><i class="fa fa-check"></i><b>2.6</b> Exercises</a><ul>
<li class="chapter" data-level="2.6.1" data-path="ch-algebra.html"><a href="ch-algebra.html#algorithm-summaries"><i class="fa fa-check"></i><b>2.6.1</b> Algorithm Summaries</a></li>
<li class="chapter" data-level="2.6.2" data-path="ch-algebra.html"><a href="ch-algebra.html#applying-what-youve-learned-1"><i class="fa fa-check"></i><b>2.6.2</b> Applying What You’ve Learned</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="ch-algebra.html"><a href="ch-algebra.html#projects"><i class="fa fa-check"></i><b>2.7</b> Projects</a><ul>
<li class="chapter" data-level="2.7.1" data-path="ch-algebra.html"><a href="ch-algebra.html#basins-of-attraction"><i class="fa fa-check"></i><b>2.7.1</b> Basins of Attraction</a></li>
<li class="chapter" data-level="2.7.2" data-path="ch-algebra.html"><a href="ch-algebra.html#artillery"><i class="fa fa-check"></i><b>2.7.2</b> Artillery</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch-calculus.html"><a href="ch-calculus.html"><i class="fa fa-check"></i><b>3</b> Calculus</a><ul>
<li class="chapter" data-level="3.1" data-path="ch-calculus.html"><a href="ch-calculus.html#intro-to-numerical-calculus"><i class="fa fa-check"></i><b>3.1</b> Intro to Numerical Calculus</a></li>
<li class="chapter" data-level="3.2" data-path="ch-calculus.html"><a href="ch-calculus.html#differentiation"><i class="fa fa-check"></i><b>3.2</b> Differentiation</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ch-calculus.html"><a href="ch-calculus.html#the-first-derivative"><i class="fa fa-check"></i><b>3.2.1</b> The First Derivative</a></li>
<li class="chapter" data-level="3.2.2" data-path="ch-calculus.html"><a href="ch-calculus.html#error-analysis"><i class="fa fa-check"></i><b>3.2.2</b> Error Analysis</a></li>
<li class="chapter" data-level="3.2.3" data-path="ch-calculus.html"><a href="ch-calculus.html#efficient-coding"><i class="fa fa-check"></i><b>3.2.3</b> Efficient Coding</a></li>
<li class="chapter" data-level="3.2.4" data-path="ch-calculus.html"><a href="ch-calculus.html#a-better-first-derivative"><i class="fa fa-check"></i><b>3.2.4</b> A Better First Derivative</a></li>
<li class="chapter" data-level="3.2.5" data-path="ch-calculus.html"><a href="ch-calculus.html#the-second-derivative"><i class="fa fa-check"></i><b>3.2.5</b> The Second Derivative</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ch-calculus.html"><a href="ch-calculus.html#integration"><i class="fa fa-check"></i><b>3.3</b> Integration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="ch-calculus.html"><a href="ch-calculus.html#riemann-sums"><i class="fa fa-check"></i><b>3.3.1</b> Riemann Sums</a></li>
<li class="chapter" data-level="3.3.2" data-path="ch-calculus.html"><a href="ch-calculus.html#trapezoidal-rule"><i class="fa fa-check"></i><b>3.3.2</b> Trapezoidal Rule</a></li>
<li class="chapter" data-level="3.3.3" data-path="ch-calculus.html"><a href="ch-calculus.html#simpsons-rule"><i class="fa fa-check"></i><b>3.3.3</b> Simpsons Rule</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ch-calculus.html"><a href="ch-calculus.html#optimization"><i class="fa fa-check"></i><b>3.4</b> Optimization</a><ul>
<li class="chapter" data-level="3.4.1" data-path="ch-calculus.html"><a href="ch-calculus.html#single-variable-optimization"><i class="fa fa-check"></i><b>3.4.1</b> Single Variable Optimization</a></li>
<li class="chapter" data-level="3.4.2" data-path="ch-calculus.html"><a href="ch-calculus.html#multivariable-optimization"><i class="fa fa-check"></i><b>3.4.2</b> Multivariable Optimization</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="ch-calculus.html"><a href="ch-calculus.html#calculus-with-numpy-and-scipy"><i class="fa fa-check"></i><b>3.5</b> Calculus with <code>numpy</code> and <code>scipy</code></a><ul>
<li class="chapter" data-level="3.5.1" data-path="ch-calculus.html"><a href="ch-calculus.html#differentiation-1"><i class="fa fa-check"></i><b>3.5.1</b> Differentiation</a></li>
<li class="chapter" data-level="3.5.2" data-path="ch-calculus.html"><a href="ch-calculus.html#integration-1"><i class="fa fa-check"></i><b>3.5.2</b> Integration</a></li>
<li class="chapter" data-level="3.5.3" data-path="ch-calculus.html"><a href="ch-calculus.html#optimization-1"><i class="fa fa-check"></i><b>3.5.3</b> Optimization</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="ch-calculus.html"><a href="ch-calculus.html#least-squares-curve-fitting"><i class="fa fa-check"></i><b>3.6</b> Least Squares Curve Fitting</a></li>
<li class="chapter" data-level="3.7" data-path="ch-calculus.html"><a href="ch-calculus.html#exercises-2"><i class="fa fa-check"></i><b>3.7</b> Exercises</a><ul>
<li class="chapter" data-level="3.7.1" data-path="ch-calculus.html"><a href="ch-calculus.html#algorithm-summaries-1"><i class="fa fa-check"></i><b>3.7.1</b> Algorithm Summaries</a></li>
<li class="chapter" data-level="3.7.2" data-path="ch-calculus.html"><a href="ch-calculus.html#applying-what-youve-learned-2"><i class="fa fa-check"></i><b>3.7.2</b> Applying What You’ve Learned</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="ch-calculus.html"><a href="ch-calculus.html#projects-1"><i class="fa fa-check"></i><b>3.8</b> Projects</a><ul>
<li class="chapter" data-level="3.8.1" data-path="ch-calculus.html"><a href="ch-calculus.html#galaxy-integration"><i class="fa fa-check"></i><b>3.8.1</b> Galaxy Integration</a></li>
<li class="chapter" data-level="3.8.2" data-path="ch-calculus.html"><a href="ch-calculus.html#higher-order-integration"><i class="fa fa-check"></i><b>3.8.2</b> Higher Order Integration</a></li>
<li class="chapter" data-level="3.8.3" data-path="ch-calculus.html"><a href="ch-calculus.html#dam-integration"><i class="fa fa-check"></i><b>3.8.3</b> Dam Integration</a></li>
<li class="chapter" data-level="3.8.4" data-path="ch-calculus.html"><a href="ch-calculus.html#edge-detection-in-images"><i class="fa fa-check"></i><b>3.8.4</b> Edge Detection in Images</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-linearalgebra.html"><a href="ch-linearalgebra.html"><i class="fa fa-check"></i><b>4</b> Linear Algebra</a><ul>
<li class="chapter" data-level="4.1" data-path="ch-linearalgebra.html"><a href="ch-linearalgebra.html#intro-to-numerical-linear-algebra"><i class="fa fa-check"></i><b>4.1</b> Intro to Numerical Linear Algebra</a></li>
<li class="chapter" data-level="4.2" data-path="ch-linearalgebra.html"><a href="ch-linearalgebra.html#vectors-and-matrices-in-python"><i class="fa fa-check"></i><b>4.2</b> Vectors and Matrices in Python</a></li>
<li class="chapter" data-level="4.3" data-path="ch-linearalgebra.html"><a href="ch-linearalgebra.html#matrix-and-vector-operations"><i class="fa fa-check"></i><b>4.3</b> Matrix and Vector Operations</a><ul>
<li class="chapter" data-level="4.3.1" data-path="ch-linearalgebra.html"><a href="ch-linearalgebra.html#the-dot-product"><i class="fa fa-check"></i><b>4.3.1</b> The Dot Product</a></li>
<li class="chapter" data-level="4.3.2" data-path="ch-linearalgebra.html"><a href="ch-linearalgebra.html#matrix-multiplication"><i class="fa fa-check"></i><b>4.3.2</b> Matrix Multiplication</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="ch-linearalgebra.html"><a href="ch-linearalgebra.html#the-lu-factorization"><i class="fa fa-check"></i><b>4.4</b> The LU Factorization</a><ul>
<li class="chapter" data-level="4.4.1" data-path="ch-linearalgebra.html"><a href="ch-linearalgebra.html#a-recap-of-row-reduction"><i class="fa fa-check"></i><b>4.4.1</b> A Recap of Row Reduction</a></li>
<li class="chapter" data-level="4.4.2" data-path="ch-linearalgebra.html"><a href="ch-linearalgebra.html#the-lu-decomposition"><i class="fa fa-check"></i><b>4.4.2</b> The LU Decomposition</a></li>
<li class="chapter" data-level="4.4.3" data-path="ch-linearalgebra.html"><a href="ch-linearalgebra.html#solving-triangular-systems"><i class="fa fa-check"></i><b>4.4.3</b> Solving Triangular Systems</a></li>
<li class="chapter" data-level="4.4.4" data-path="ch-linearalgebra.html"><a href="ch-linearalgebra.html#solving-systems-with-lu"><i class="fa fa-check"></i><b>4.4.4</b> Solving Systems with LU</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="ch-linearalgebra.html"><a href="ch-linearalgebra.html#the-qr-factorization"><i class="fa fa-check"></i><b>4.5</b> The QR Factorization</a></li>
<li class="chapter" data-level="4.6" data-path="ch-linearalgebra.html"><a href="ch-linearalgebra.html#over-determined-systems-and-curve-fitting"><i class="fa fa-check"></i><b>4.6</b> Over Determined Systems and Curve Fitting</a></li>
<li class="chapter" data-level="4.7" data-path="ch-linearalgebra.html"><a href="ch-linearalgebra.html#the-eigenvalue-eigenvector-problem"><i class="fa fa-check"></i><b>4.7</b> The Eigenvalue-Eigenvector Problem</a></li>
<li class="chapter" data-level="4.8" data-path="ch-linearalgebra.html"><a href="ch-linearalgebra.html#exercises-3"><i class="fa fa-check"></i><b>4.8</b> Exercises</a><ul>
<li class="chapter" data-level="4.8.1" data-path="ch-linearalgebra.html"><a href="ch-linearalgebra.html#algorithm-summaries-2"><i class="fa fa-check"></i><b>4.8.1</b> Algorithm Summaries</a></li>
<li class="chapter" data-level="4.8.2" data-path="ch-linearalgebra.html"><a href="ch-linearalgebra.html#applying-what-youve-learned-3"><i class="fa fa-check"></i><b>4.8.2</b> Applying What You’ve Learned</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="ch-linearalgebra.html"><a href="ch-linearalgebra.html#projects-2"><i class="fa fa-check"></i><b>4.9</b> Projects</a><ul>
<li class="chapter" data-level="4.9.1" data-path="ch-linearalgebra.html"><a href="ch-linearalgebra.html#the-google-page-rank-algorithm"><i class="fa fa-check"></i><b>4.9.1</b> The Google Page Rank Algorithm</a></li>
<li class="chapter" data-level="4.9.2" data-path="ch-linearalgebra.html"><a href="ch-linearalgebra.html#alternative-methods-to-solving-a-boldsymbolx-boldsymbolb"><i class="fa fa-check"></i><b>4.9.2</b> Alternative Methods To Solving <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ch-odes.html"><a href="ch-odes.html"><i class="fa fa-check"></i><b>5</b> Ordinary Differential Equations</a><ul>
<li class="chapter" data-level="5.1" data-path="ch-odes.html"><a href="ch-odes.html#intro-to-numerical-odes"><i class="fa fa-check"></i><b>5.1</b> Intro to Numerical ODEs</a></li>
<li class="chapter" data-level="5.2" data-path="ch-odes.html"><a href="ch-odes.html#recalling-the-basics-of-odes"><i class="fa fa-check"></i><b>5.2</b> Recalling the Basics of ODEs</a></li>
<li class="chapter" data-level="5.3" data-path="ch-odes.html"><a href="ch-odes.html#eulers-method"><i class="fa fa-check"></i><b>5.3</b> Euler’s Method</a></li>
<li class="chapter" data-level="5.4" data-path="ch-odes.html"><a href="ch-odes.html#the-midpoint-method"><i class="fa fa-check"></i><b>5.4</b> The Midpoint Method</a></li>
<li class="chapter" data-level="5.5" data-path="ch-odes.html"><a href="ch-odes.html#the-runge-kutta-4-method"><i class="fa fa-check"></i><b>5.5</b> The Runge-Kutta 4 Method</a></li>
<li class="chapter" data-level="5.6" data-path="ch-odes.html"><a href="ch-odes.html#animating-ode-solutions"><i class="fa fa-check"></i><b>5.6</b> Animating ODE Solutions</a><ul>
<li class="chapter" data-level="5.6.1" data-path="ch-odes.html"><a href="ch-odes.html#ipywidgets.interactive"><i class="fa fa-check"></i><b>5.6.1</b> <code>ipywidgets.interactive</code></a></li>
<li class="chapter" data-level="5.6.2" data-path="ch-odes.html"><a href="ch-odes.html#matplotlib.animation"><i class="fa fa-check"></i><b>5.6.2</b> <code>matplotlib.animation</code></a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="ch-odes.html"><a href="ch-odes.html#the-backwards-euler-method"><i class="fa fa-check"></i><b>5.7</b> The Backwards Euler Method</a></li>
<li class="chapter" data-level="5.8" data-path="ch-odes.html"><a href="ch-odes.html#fitting-ode-models-to-data"><i class="fa fa-check"></i><b>5.8</b> Fitting ODE Models to Data</a></li>
<li class="chapter" data-level="5.9" data-path="ch-odes.html"><a href="ch-odes.html#exercises-4"><i class="fa fa-check"></i><b>5.9</b> Exercises</a><ul>
<li class="chapter" data-level="5.9.1" data-path="ch-odes.html"><a href="ch-odes.html#algorithm-summaries-3"><i class="fa fa-check"></i><b>5.9.1</b> Algorithm Summaries</a></li>
<li class="chapter" data-level="5.9.2" data-path="ch-odes.html"><a href="ch-odes.html#applying-what-youve-learned-4"><i class="fa fa-check"></i><b>5.9.2</b> Applying What You’ve Learned</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="ch-odes.html"><a href="ch-odes.html#projects-3"><i class="fa fa-check"></i><b>5.10</b> Projects</a><ul>
<li class="chapter" data-level="5.10.1" data-path="ch-odes.html"><a href="ch-odes.html#the-covid-19-pandemic"><i class="fa fa-check"></i><b>5.10.1</b> The COVID-19 Pandemic</a></li>
<li class="chapter" data-level="5.10.2" data-path="ch-odes.html"><a href="ch-odes.html#pain-management"><i class="fa fa-check"></i><b>5.10.2</b> Pain Management</a></li>
<li class="chapter" data-level="5.10.3" data-path="ch-odes.html"><a href="ch-odes.html#the-h1n1-virus"><i class="fa fa-check"></i><b>5.10.3</b> The H1N1 Virus</a></li>
<li class="chapter" data-level="5.10.4" data-path="ch-odes.html"><a href="ch-odes.html#the-artillery-problem"><i class="fa fa-check"></i><b>5.10.4</b> The Artillery Problem</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-pdes.html"><a href="ch-pdes.html"><i class="fa fa-check"></i><b>6</b> Partial Differential Equations</a><ul>
<li class="chapter" data-level="6.1" data-path="ch-pdes.html"><a href="ch-pdes.html#intro-to-pdes"><i class="fa fa-check"></i><b>6.1</b> Intro to PDEs</a></li>
<li class="chapter" data-level="6.2" data-path="ch-pdes.html"><a href="ch-pdes.html#solutions-to-pdes"><i class="fa fa-check"></i><b>6.2</b> Solutions to PDEs</a></li>
<li class="chapter" data-level="6.3" data-path="ch-pdes.html"><a href="ch-pdes.html#boundary-conditions"><i class="fa fa-check"></i><b>6.3</b> Boundary Conditions</a></li>
<li class="chapter" data-level="6.4" data-path="ch-pdes.html"><a href="ch-pdes.html#the-heat-equation"><i class="fa fa-check"></i><b>6.4</b> The Heat Equation</a></li>
<li class="chapter" data-level="6.5" data-path="ch-pdes.html"><a href="ch-pdes.html#stability-of-the-heat-equation-solution"><i class="fa fa-check"></i><b>6.5</b> Stability of the Heat Equation Solution</a></li>
<li class="chapter" data-level="6.6" data-path="ch-pdes.html"><a href="ch-pdes.html#the-wave-equation"><i class="fa fa-check"></i><b>6.6</b> The Wave Equation</a></li>
<li class="chapter" data-level="6.7" data-path="ch-pdes.html"><a href="ch-pdes.html#traveling-waves"><i class="fa fa-check"></i><b>6.7</b> Traveling Waves</a></li>
<li class="chapter" data-level="6.8" data-path="ch-pdes.html"><a href="ch-pdes.html#the-laplace-and-poisson-equations"><i class="fa fa-check"></i><b>6.8</b> The Laplace and Poisson Equations</a></li>
<li class="chapter" data-level="6.9" data-path="ch-pdes.html"><a href="ch-pdes.html#exercises-5"><i class="fa fa-check"></i><b>6.9</b> Exercises</a><ul>
<li class="chapter" data-level="6.9.1" data-path="ch-pdes.html"><a href="ch-pdes.html#algorithm-summaries-4"><i class="fa fa-check"></i><b>6.9.1</b> Algorithm Summaries</a></li>
<li class="chapter" data-level="6.9.2" data-path="ch-pdes.html"><a href="ch-pdes.html#applying-what-youve-learned-5"><i class="fa fa-check"></i><b>6.9.2</b> Applying What You’ve Learned</a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="ch-pdes.html"><a href="ch-pdes.html#projects-4"><i class="fa fa-check"></i><b>6.10</b> Projects</a><ul>
<li class="chapter" data-level="6.10.1" data-path="ch-pdes.html"><a href="ch-pdes.html#hunting-and-diffusion"><i class="fa fa-check"></i><b>6.10.1</b> Hunting and Diffusion</a></li>
<li class="chapter" data-level="6.10.2" data-path="ch-pdes.html"><a href="ch-pdes.html#heating-adobe-houses"><i class="fa fa-check"></i><b>6.10.2</b> Heating Adobe Houses</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li class="chapter" data-level="A" data-path="ch-python.html"><a href="ch-python.html"><i class="fa fa-check"></i><b>A</b> Introduction to Python</a><ul>
<li class="chapter" data-level="A.1" data-path="ch-python.html"><a href="ch-python.html#why-python"><i class="fa fa-check"></i><b>A.1</b> Why Python?</a></li>
<li class="chapter" data-level="A.2" data-path="ch-python.html"><a href="ch-python.html#getting-started"><i class="fa fa-check"></i><b>A.2</b> Getting Started</a></li>
<li class="chapter" data-level="A.3" data-path="ch-python.html"><a href="ch-python.html#hello-world"><i class="fa fa-check"></i><b>A.3</b> Hello, World!</a></li>
<li class="chapter" data-level="A.4" data-path="ch-python.html"><a href="ch-python.html#python-programming-basics"><i class="fa fa-check"></i><b>A.4</b> Python Programming Basics</a><ul>
<li class="chapter" data-level="A.4.1" data-path="ch-python.html"><a href="ch-python.html#variables"><i class="fa fa-check"></i><b>A.4.1</b> Variables</a></li>
<li class="chapter" data-level="A.4.2" data-path="ch-python.html"><a href="ch-python.html#indexing-and-lists"><i class="fa fa-check"></i><b>A.4.2</b> Indexing and Lists</a></li>
<li class="chapter" data-level="A.4.3" data-path="ch-python.html"><a href="ch-python.html#list-operations"><i class="fa fa-check"></i><b>A.4.3</b> List Operations</a></li>
<li class="chapter" data-level="A.4.4" data-path="ch-python.html"><a href="ch-python.html#tuples"><i class="fa fa-check"></i><b>A.4.4</b> Tuples</a></li>
<li class="chapter" data-level="A.4.5" data-path="ch-python.html"><a href="ch-python.html#control-flow-loops-and-if-statements"><i class="fa fa-check"></i><b>A.4.5</b> Control Flow: Loops and If Statements</a></li>
<li class="chapter" data-level="A.4.6" data-path="ch-python.html"><a href="ch-python.html#functions"><i class="fa fa-check"></i><b>A.4.6</b> Functions</a></li>
<li class="chapter" data-level="A.4.7" data-path="ch-python.html"><a href="ch-python.html#lambda-functions"><i class="fa fa-check"></i><b>A.4.7</b> Lambda Functions</a></li>
<li class="chapter" data-level="A.4.8" data-path="ch-python.html"><a href="ch-python.html#packages"><i class="fa fa-check"></i><b>A.4.8</b> Packages</a></li>
</ul></li>
<li class="chapter" data-level="A.5" data-path="ch-python.html"><a href="ch-python.html#numerical-python-with-numpy"><i class="fa fa-check"></i><b>A.5</b> Numerical Python with <code>numpy</code></a><ul>
<li class="chapter" data-level="A.5.1" data-path="ch-python.html"><a href="ch-python.html#numpy-arrays-array-operations-and-matrix-operations"><i class="fa fa-check"></i><b>A.5.1</b> Numpy Arrays, Array Operations, and Matrix Operations</a></li>
<li class="chapter" data-level="A.5.2" data-path="ch-python.html"><a href="ch-python.html#arange-linspace-zeros-ones-and-meshgrid"><i class="fa fa-check"></i><b>A.5.2</b> <code>arange, linspace, zeros, ones</code>, and <code>meshgrid</code></a></li>
</ul></li>
<li class="chapter" data-level="A.6" data-path="ch-python.html"><a href="ch-python.html#plotting-with-matplotlib"><i class="fa fa-check"></i><b>A.6</b> Plotting with <code>matplotlib</code></a><ul>
<li class="chapter" data-level="A.6.1" data-path="ch-python.html"><a href="ch-python.html#basics-with-plt.plot"><i class="fa fa-check"></i><b>A.6.1</b> Basics with <code>plt.plot()</code></a></li>
<li class="chapter" data-level="A.6.2" data-path="ch-python.html"><a href="ch-python.html#subplots"><i class="fa fa-check"></i><b>A.6.2</b> Subplots</a></li>
<li class="chapter" data-level="A.6.3" data-path="ch-python.html"><a href="ch-python.html#logarithmic-scaling-with-semilogy-semilogx-and-loglog"><i class="fa fa-check"></i><b>A.6.3</b> Logarithmic Scaling with <code>semilogy</code>, <code>semilogx</code>, and <code>loglog</code></a></li>
</ul></li>
<li class="chapter" data-level="A.7" data-path="ch-python.html"><a href="ch-python.html#symbolic-python-with-sympy"><i class="fa fa-check"></i><b>A.7</b> Symbolic Python with <code>sympy</code></a><ul>
<li class="chapter" data-level="A.7.1" data-path="ch-python.html"><a href="ch-python.html#symbolic-variables-with-symbols"><i class="fa fa-check"></i><b>A.7.1</b> Symbolic Variables with <code>symbols</code></a></li>
<li class="chapter" data-level="A.7.2" data-path="ch-python.html"><a href="ch-python.html#symbolic-algebra"><i class="fa fa-check"></i><b>A.7.2</b> Symbolic Algebra</a></li>
<li class="chapter" data-level="A.7.3" data-path="ch-python.html"><a href="ch-python.html#symbolic-function-evaluation"><i class="fa fa-check"></i><b>A.7.3</b> Symbolic Function Evaluation</a></li>
<li class="chapter" data-level="A.7.4" data-path="ch-python.html"><a href="ch-python.html#symbolic-calculus"><i class="fa fa-check"></i><b>A.7.4</b> Symbolic Calculus</a></li>
<li class="chapter" data-level="A.7.5" data-path="ch-python.html"><a href="ch-python.html#solving-equations-symbolically"><i class="fa fa-check"></i><b>A.7.5</b> Solving Equations Symbolically</a></li>
<li class="chapter" data-level="A.7.6" data-path="ch-python.html"><a href="ch-python.html#symbolic-plotting"><i class="fa fa-check"></i><b>A.7.6</b> Symbolic Plotting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="ch-writing.html"><a href="ch-writing.html"><i class="fa fa-check"></i><b>B</b> Mathematical Writing</a><ul>
<li class="chapter" data-level="B.1" data-path="ch-writing.html"><a href="ch-writing.html#the-paper"><i class="fa fa-check"></i><b>B.1</b> The Paper</a></li>
<li class="chapter" data-level="B.2" data-path="ch-writing.html"><a href="ch-writing.html#figures-and-tables"><i class="fa fa-check"></i><b>B.2</b> Figures and Tables</a></li>
<li class="chapter" data-level="B.3" data-path="ch-writing.html"><a href="ch-writing.html#writing-style"><i class="fa fa-check"></i><b>B.3</b> Writing Style</a></li>
<li class="chapter" data-level="B.4" data-path="ch-writing.html"><a href="ch-writing.html#tips-for-writing-clear-math"><i class="fa fa-check"></i><b>B.4</b> Tips For Writing Clear Math</a><ul>
<li class="chapter" data-level="B.4.1" data-path="ch-writing.html"><a href="ch-writing.html#audience"><i class="fa fa-check"></i><b>B.4.1</b> Audience</a></li>
<li class="chapter" data-level="B.4.2" data-path="ch-writing.html"><a href="ch-writing.html#how-to-make-mathematics-readable-10-things-to-do"><i class="fa fa-check"></i><b>B.4.2</b> How To Make Mathematics Readable – 10 Things To Do</a></li>
<li class="chapter" data-level="B.4.3" data-path="ch-writing.html"><a href="ch-writing.html#some-writing-tips"><i class="fa fa-check"></i><b>B.4.3</b> Some Writing Tips</a></li>
<li class="chapter" data-level="B.4.4" data-path="ch-writing.html"><a href="ch-writing.html#mathematical-vocabulary"><i class="fa fa-check"></i><b>B.4.4</b> Mathematical Vocabulary</a></li>
</ul></li>
<li class="chapter" data-level="B.5" data-path="ch-writing.html"><a href="ch-writing.html#sensitivity-analysis"><i class="fa fa-check"></i><b>B.5</b> Sensitivity Analysis</a><ul>
<li class="chapter" data-level="B.5.1" data-path="ch-writing.html"><a href="ch-writing.html#example-of-sensitivity-analysis"><i class="fa fa-check"></i><b>B.5.1</b> Example of Sensitivity Analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="ch-extras.html"><a href="ch-extras.html"><i class="fa fa-check"></i><b>C</b> Optional Material</a><ul>
<li class="chapter" data-level="C.1" data-path="ch-extras.html"><a href="ch-extras.html#interpolation"><i class="fa fa-check"></i><b>C.1</b> Interpolation</a><ul>
<li class="chapter" data-level="C.1.1" data-path="ch-extras.html"><a href="ch-extras.html#vandermonde-interpolation"><i class="fa fa-check"></i><b>C.1.1</b> Vandermonde Interpolation</a></li>
<li class="chapter" data-level="C.1.2" data-path="ch-extras.html"><a href="ch-extras.html#lagrange-interpolation"><i class="fa fa-check"></i><b>C.1.2</b> Lagrange Interpolation</a></li>
<li class="chapter" data-level="C.1.3" data-path="ch-extras.html"><a href="ch-extras.html#chebyshev-points"><i class="fa fa-check"></i><b>C.1.3</b> Chebyshev Points</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="ch-extras.html"><a href="ch-extras.html#sec:mv_newton"><i class="fa fa-check"></i><b>C.2</b> Multi-Dimensional Newton’s Method</a></li>
<li class="chapter" data-level="C.3" data-path="ch-extras.html"><a href="ch-extras.html#the-method-of-lines"><i class="fa fa-check"></i><b>C.3</b> The Method Of Lines</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Numerical Methods</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch:linearalgebra" class="section level1">
<h1><span class="header-section-number">4</span> Linear Algebra</h1>
<div id="intro-to-numerical-linear-algebra" class="section level2">
<h2><span class="header-section-number">4.1</span> Intro to Numerical Linear Algebra</h2>

<blockquote>
<p><em>You cannot learn too much linear algebra.</em><br />
– Every mathematician</p>
</blockquote>
<p>The preceding comment says it all – linear algebra is the most important of all of the mathematical tools that you can learn as a practitioner of the mathematical sciences. The theorems, proofs, conjectures, and big ideas in almost every other mathematical field find their roots in linear algebra. Our goal in this chapter is to explore numerical algorithms for the primary questions of
linear algebra:</p>
<ul>
<li>solving systems of equations,</li>
<li>approximating solutions to over-determined systems of equations, and</li>
<li>finding eigenvalue-eigenvector pairs for a matrix.</li>
</ul>
<p>To see an introductory video to this chapter go to <a href="https://youtu.be/Sl90SQBoN-g">https://youtu.be/Sl90SQBoN-g</a>.</p>
<p>Take careful note, that in our current digital age numerical linear algebra and its fast algorithms are behind the scenes for wide varieties of computing applications. Applications of numerical linear algebra include:</p>
<ul>
<li>determining the most important web page in a Google search,</li>
<li>determine the forces on a car during a crash,</li>
<li>modeling realistic 3D environments in video games,</li>
<li>digital image processing,</li>
<li>building neural networks and AI algorithms,</li>
<li>and many many more.</li>
</ul>
<p>What’s more, researchers have found provably optimal ways to perform most of the typical tasks of linear algebra so most scientific software works very well and very quickly with linear algebra. For example, we have already seen in Chapter <a href="ch-calculus.html#ch:calculus">3</a> that programming numerical differentiation and numerical integration schemes can be done in Python with the use of vectors instead of loops. We want to use vectors specifically so that we can use the fast implementations of numerical linear algebra in the background in Python.</p>
<p>Lastly, a comment on notation. Throughout this chapter we will use the following notation conventions.</p>
<ul>
<li>A bold mathematical symbol such as <span class="math inline">\(\boldsymbol{x}\)</span> or <span class="math inline">\(\boldsymbol{u}\)</span> will represent a vector.<br />
</li>
<li>If <span class="math inline">\(\boldsymbol{u}\)</span> is a vector then <span class="math inline">\(u_j\)</span> will be the <span class="math inline">\(j^{th}\)</span> entry of the vector.</li>
<li>Vectors will typically be written vertically with parenthesis as delimiters such as
<span class="math display">\[ \boldsymbol{u} = \begin{pmatrix} 1\\2\\3 \end{pmatrix}. \]</span></li>
<li>Two bold symbols separated by a centered dot such as <span class="math inline">\(\boldsymbol{u} \cdot \boldsymbol{v}\)</span> will represent the dot product of two vectors.</li>
<li>A capital mathematical symbol such as <span class="math inline">\(A\)</span> or <span class="math inline">\(X\)</span> will represent a matrix</li>
<li>If <span class="math inline">\(A\)</span> is a matrix then <span class="math inline">\(A_{ij}\)</span> will be the element in the <span class="math inline">\(i^{th}\)</span> row and <span class="math inline">\(j^{th}\)</span> column of the matrix.</li>
<li>A matrix will typically be written with parenthesis as delimiters such as
<span class="math display">\[ A = \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; \pi \end{pmatrix}. \]</span></li>
<li>The juxtaposition of a capital symbol and a bold symbol such as <span class="math inline">\(A\boldsymbol{x}\)</span> will represent matrix-vector multiplication.</li>
<li>A lower case or Greek mathematical symbol such as <span class="math inline">\(x\)</span>, <span class="math inline">\(c\)</span>, or <span class="math inline">\(\lambda\)</span> will represent a scalar.<br />
</li>
<li>The scalar field of real numbers is given as <span class="math inline">\(\mathbb{R}\)</span> and the scalar field of complex numbers is given as <span class="math inline">\(\mathbb{C}\)</span>.<br />
</li>
<li>The symbol <span class="math inline">\(\mathbb{R}^n\)</span> represents the collection of <span class="math inline">\(n\)</span>-dimensional vectors where the elements are drawn from the real numbers.<br />
</li>
<li>The symbol <span class="math inline">\(\mathbb{C}^n\)</span> represents the collection of <span class="math inline">\(n\)</span>-dimensional vectors where the elements are drawn from the complex numbers.</li>
</ul>
<p>It is an important part of learning to read and write linear algebra to give special attention to the symbolic language so you can communicate your work easily and efficiently.</p>
<div style="page-break-after: always;"></div>
</div>
<div id="vectors-and-matrices-in-python" class="section level2">
<h2><span class="header-section-number">4.2</span> Vectors and Matrices in Python</h2>
<p>We first need to understand how Python’s <code>numpy</code> library builds and stores vectors and matrices. The following exercises will give you some experience building and working with these data structures and will point out some common pitfalls that mathematicians fall into when using Python for linear algebra.</p>
<hr />

<div class="example">
<span id="exm:unnamed-chunk-278" class="example"><strong>Example 4.1  (numpy Arrays)  </strong></span>In Python you can build a list using square brackets such as <code>[1,2,3]</code>. This is called a “Python list” and is NOT a vector in the way that we think about it mathematically. It is simply an ordered collection of objects. To build mathematical vectors in Python we need to use <code>numpy</code> arrays with <code>np.array()</code>. For example, the vector
<span class="math display">\[ \boldsymbol{u} = \begin{pmatrix} 1\\2\\3\end{pmatrix} \]</span>
would be built with the following code.
</div>

<div class="sourceCode" id="cb57"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="ch-linearalgebra.html#cb57-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb57-2"><a href="ch-linearalgebra.html#cb57-2"></a>u <span class="op">=</span> np.array([<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>])</span>
<span id="cb57-3"><a href="ch-linearalgebra.html#cb57-3"></a><span class="bu">print</span>(u)</span></code></pre></div>
<pre><code>## [1 2 3]</code></pre>
<p>Notice that Python defines the vector <code>u</code> as a matrix without a second dimension. You can see that in the following code.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="ch-linearalgebra.html#cb59-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb59-2"><a href="ch-linearalgebra.html#cb59-2"></a>u <span class="op">=</span> np.array([<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>])</span>
<span id="cb59-3"><a href="ch-linearalgebra.html#cb59-3"></a><span class="bu">print</span>(<span class="st">&quot;The length of the u vector is </span><span class="ch">\n</span><span class="st">&quot;</span>,<span class="bu">len</span>(u))</span></code></pre></div>
<pre><code>## The length of the u vector is 
##  3</code></pre>
<div class="sourceCode" id="cb61"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="ch-linearalgebra.html#cb61-1"></a><span class="bu">print</span>(<span class="st">&quot;The shape of the u vector is </span><span class="ch">\n</span><span class="st">&quot;</span>,u.shape)</span></code></pre></div>
<pre><code>## The shape of the u vector is 
##  (3,)</code></pre>
<hr />

<div class="example">
<span id="exm:unnamed-chunk-281" class="example"><strong>Example 4.2  (numpy Matrices)  </strong></span>In <code>numpy</code>, a matrix is a list of lists. For example, the matrix
<span class="math display">\[ A = \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end{pmatrix} \]</span>
is defined using <code>np.matrix()</code> where each row is an individual list, and the matrix is a collection of these lists.
</div>

<div class="sourceCode" id="cb63"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="ch-linearalgebra.html#cb63-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb63-2"><a href="ch-linearalgebra.html#cb63-2"></a>A <span class="op">=</span> np.matrix([[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>],[<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>],[<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">9</span>]])</span>
<span id="cb63-3"><a href="ch-linearalgebra.html#cb63-3"></a><span class="bu">print</span>(A)</span></code></pre></div>
<pre><code>## [[1 2 3]
##  [4 5 6]
##  [7 8 9]]</code></pre>
<p>Moreover, we can extract the shape, the number of rows, and the number of columns of <span class="math inline">\(A\)</span> using the <code>A.shape</code> command. To be a bit more clear on this one we’ll use the matrix
<span class="math display">\[ A = \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix} \]</span></p>
<div class="sourceCode" id="cb65"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb65-1"><a href="ch-linearalgebra.html#cb65-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb65-2"><a href="ch-linearalgebra.html#cb65-2"></a>A <span class="op">=</span> np.matrix([[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>],[<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>]])</span>
<span id="cb65-3"><a href="ch-linearalgebra.html#cb65-3"></a><span class="bu">print</span>(<span class="st">&quot;The shape of the A matrix is </span><span class="ch">\n</span><span class="st">&quot;</span>,A.shape)</span></code></pre></div>
<pre><code>## The shape of the A matrix is 
##  (2, 3)</code></pre>
<div class="sourceCode" id="cb67"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb67-1"><a href="ch-linearalgebra.html#cb67-1"></a><span class="bu">print</span>(<span class="st">&quot;Number of rows in A is </span><span class="ch">\n</span><span class="st">&quot;</span>,A.shape[<span class="dv">0</span>])</span></code></pre></div>
<pre><code>## Number of rows in A is 
##  2</code></pre>
<div class="sourceCode" id="cb69"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb69-1"><a href="ch-linearalgebra.html#cb69-1"></a><span class="bu">print</span>(<span class="st">&quot;Number of columns in A is </span><span class="ch">\n</span><span class="st">&quot;</span>,A.shape[<span class="dv">1</span>])</span></code></pre></div>
<pre><code>## Number of columns in A is 
##  3</code></pre>
<hr />

<div class="example">
<span id="exm:unnamed-chunk-284" class="example"><strong>Example 4.3  (Row and Column Vectors in Python)  </strong></span>You can more specifically build row or column vectors in Python using the <code>np.matrix()</code> command and then only specifying one row or column. For example, if you want the vectors
<span class="math display">\[ \boldsymbol{u} = \begin{pmatrix} 1\\2\\3\end{pmatrix} \quad \text{and} \quad \boldsymbol{v} = \begin{pmatrix} 4 &amp; 5 &amp; 6 \end{pmatrix} \]</span>
then we would use the following Python code.
</div>

<div class="sourceCode" id="cb71"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb71-1"><a href="ch-linearalgebra.html#cb71-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb71-2"><a href="ch-linearalgebra.html#cb71-2"></a>u <span class="op">=</span> np.matrix([[<span class="dv">1</span>],[<span class="dv">2</span>],[<span class="dv">3</span>]])</span>
<span id="cb71-3"><a href="ch-linearalgebra.html#cb71-3"></a><span class="bu">print</span>(<span class="st">&quot;The column vector u is </span><span class="ch">\n</span><span class="st">&quot;</span>,u)</span></code></pre></div>
<pre><code>## The column vector u is 
##  [[1]
##  [2]
##  [3]]</code></pre>
<div class="sourceCode" id="cb73"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb73-1"><a href="ch-linearalgebra.html#cb73-1"></a>v <span class="op">=</span> np.matrix([[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>]])</span>
<span id="cb73-2"><a href="ch-linearalgebra.html#cb73-2"></a><span class="bu">print</span>(<span class="st">&quot;The row vector v is </span><span class="ch">\n</span><span class="st">&quot;</span>,v)</span></code></pre></div>
<pre><code>## The row vector v is 
##  [[1 2 3]]</code></pre>
<p>Alternatively, if you want to define a column vector you can define a row vector (since there are far fewer brackets to keep track of) and then transpose the matrix to turn it into a column.</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb75-1"><a href="ch-linearalgebra.html#cb75-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb75-2"><a href="ch-linearalgebra.html#cb75-2"></a>u <span class="op">=</span> np.matrix([[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>]])</span>
<span id="cb75-3"><a href="ch-linearalgebra.html#cb75-3"></a>u <span class="op">=</span> u.transpose()</span>
<span id="cb75-4"><a href="ch-linearalgebra.html#cb75-4"></a><span class="bu">print</span>(<span class="st">&quot;The column vector u is </span><span class="ch">\n</span><span class="st">&quot;</span>,u)</span></code></pre></div>
<pre><code>## The column vector u is 
##  [[1]
##  [2]
##  [3]]</code></pre>
<hr />

<div class="example">
<p><span id="exm:unnamed-chunk-287" class="example"><strong>Example 4.4  (Matrix Indexing)  </strong></span>Python indexes all arrays, vectors, lists, and matrices starting from index 0. Let’s get used to this fact.</p>
<p>Consider the matrix <span class="math inline">\(A\)</span> defined in the previous problem. Mathematically we know that the entry in row 1 column 1 is a 1, the entry in row 1 column 2 is a 2, and so on. However, with Python we need to shift the way that we enumerate the rows and columns of a matrix. Hence we would say that the entry in row 0 column 0 is a 1, the entry in row 0 column 1 is a 2, and so on.</p>
<p>Mathematically we can view all Python matrices as follows. If <span class="math inline">\(A\)</span> is an <span class="math inline">\(n \times n\)</span> matrix then
<span class="math display">\[ A = \begin{pmatrix} A_{0,0} &amp; A_{0,1} &amp; A_{0,2} &amp; \cdots &amp; A_{0,n-1} \\
A_{1,0} &amp; A_{1,1} &amp; A_{1,2} &amp; \cdots &amp; A_{1,n-1} \\ 
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
A_{n-1,0} &amp; A_{n-1,1} &amp; A_{n-1,2} &amp; \cdots &amp; A_{n-1,n-1} \end{pmatrix} \]</span></p>
<p>Similarly, we can view all vectors as follows. If <span class="math inline">\(\boldsymbol{u}\)</span> is an <span class="math inline">\(n \times 1\)</span> vector then
<span class="math display">\[ \boldsymbol{u} = \begin{pmatrix} u_0 \\ u_1 \\ \vdots \\ u_{n-1} \end{pmatrix} \]</span></p>
</div>

<p>The following code should help to illustrate this indexing convention.</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb77-1"><a href="ch-linearalgebra.html#cb77-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb77-2"><a href="ch-linearalgebra.html#cb77-2"></a>A <span class="op">=</span> np.matrix([[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>],[<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>],[<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">9</span>]])</span>
<span id="cb77-3"><a href="ch-linearalgebra.html#cb77-3"></a><span class="bu">print</span>(<span class="st">&quot;Entry in row 0 column 0 is&quot;</span>,A[<span class="dv">0</span>,<span class="dv">0</span>])</span></code></pre></div>
<pre><code>## Entry in row 0 column 0 is 1</code></pre>
<div class="sourceCode" id="cb79"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb79-1"><a href="ch-linearalgebra.html#cb79-1"></a><span class="bu">print</span>(<span class="st">&quot;Entry in row 0 column 1 is&quot;</span>,A[<span class="dv">0</span>,<span class="dv">1</span>])</span></code></pre></div>
<pre><code>## Entry in row 0 column 1 is 2</code></pre>
<div class="sourceCode" id="cb81"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb81-1"><a href="ch-linearalgebra.html#cb81-1"></a><span class="bu">print</span>(<span class="st">&quot;Entry in the bottom right corner&quot;</span>,A[<span class="dv">2</span>,<span class="dv">2</span>])</span></code></pre></div>
<pre><code>## Entry in the bottom right corner 9</code></pre>
<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-289" class="exercise"><strong>Exercise 4.1  </strong></span>Build your own matrix in Python and practice choosing individual entries from the matrix.
</div>

<hr />

<div class="example">
<span id="exm:unnamed-chunk-290" class="example"><strong>Example 4.5  (Matrix Slicing)  </strong></span>The last thing that we need to be familiar with is <em>slicing</em> a matrix. The term “slicing” generally refers to pulling out individual rows, columns, entries, or blocks from a list, array, or matrix in Python. Examine the code below to see how to slice parts out of a <code>numpy</code> matrix.
</div>

<div class="sourceCode" id="cb83"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb83-1"><a href="ch-linearalgebra.html#cb83-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb83-2"><a href="ch-linearalgebra.html#cb83-2"></a>A <span class="op">=</span> np.matrix([[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>],[<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>],[<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">9</span>]])</span>
<span id="cb83-3"><a href="ch-linearalgebra.html#cb83-3"></a><span class="bu">print</span>(A)</span></code></pre></div>
<pre><code>## [[1 2 3]
##  [4 5 6]
##  [7 8 9]]</code></pre>
<div class="sourceCode" id="cb85"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb85-1"><a href="ch-linearalgebra.html#cb85-1"></a><span class="bu">print</span>(<span class="st">&quot;The first column of A is </span><span class="ch">\n</span><span class="st">&quot;</span>,A[:,<span class="dv">0</span>])</span></code></pre></div>
<pre><code>## The first column of A is 
##  [[1]
##  [4]
##  [7]]</code></pre>
<div class="sourceCode" id="cb87"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb87-1"><a href="ch-linearalgebra.html#cb87-1"></a><span class="bu">print</span>(<span class="st">&quot;The second row of A is </span><span class="ch">\n</span><span class="st">&quot;</span>,A[<span class="dv">1</span>,:])</span></code></pre></div>
<pre><code>## The second row of A is 
##  [[4 5 6]]</code></pre>
<div class="sourceCode" id="cb89"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb89-1"><a href="ch-linearalgebra.html#cb89-1"></a><span class="bu">print</span>(<span class="st">&quot;The top left 2x2 sub matrix of A is </span><span class="ch">\n</span><span class="st">&quot;</span>,A[:<span class="op">-</span><span class="dv">1</span>,:<span class="op">-</span><span class="dv">1</span>])</span></code></pre></div>
<pre><code>## The top left 2x2 sub matrix of A is 
##  [[1 2]
##  [4 5]]</code></pre>
<div class="sourceCode" id="cb91"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb91-1"><a href="ch-linearalgebra.html#cb91-1"></a><span class="bu">print</span>(<span class="st">&quot;The bottom right 2x2 sub matrix of A is </span><span class="ch">\n</span><span class="st">&quot;</span>,A[<span class="dv">1</span>:,<span class="dv">1</span>:])</span></code></pre></div>
<pre><code>## The bottom right 2x2 sub matrix of A is 
##  [[5 6]
##  [8 9]]</code></pre>
<div class="sourceCode" id="cb93"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb93-1"><a href="ch-linearalgebra.html#cb93-1"></a>u <span class="op">=</span> np.array([<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>])</span>
<span id="cb93-2"><a href="ch-linearalgebra.html#cb93-2"></a><span class="bu">print</span>(<span class="st">&quot;The first 3 entries of the vector u are </span><span class="ch">\n</span><span class="st">&quot;</span>,u[:<span class="dv">3</span>])</span></code></pre></div>
<pre><code>## The first 3 entries of the vector u are 
##  [1 2 3]</code></pre>
<div class="sourceCode" id="cb95"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb95-1"><a href="ch-linearalgebra.html#cb95-1"></a><span class="bu">print</span>(<span class="st">&quot;The last entry of the vector u is </span><span class="ch">\n</span><span class="st">&quot;</span>,u[<span class="op">-</span><span class="dv">1</span>])</span></code></pre></div>
<pre><code>## The last entry of the vector u is 
##  6</code></pre>
<div class="sourceCode" id="cb97"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb97-1"><a href="ch-linearalgebra.html#cb97-1"></a><span class="bu">print</span>(<span class="st">&quot;The last two entries of the vector u are </span><span class="ch">\n</span><span class="st">&quot;</span>,u[<span class="op">-</span><span class="dv">2</span>:])</span></code></pre></div>
<pre><code>## The last two entries of the vector u are 
##  [5 6]</code></pre>
<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-292" class="exercise"><strong>Exercise 4.2  </strong></span>Define the matrix <span class="math inline">\(A\)</span> and the vector <span class="math inline">\(u\)</span> in Python. Then perform all of the tasks below.
<span class="math display">\[ A = \begin{pmatrix} 1 &amp; 3 &amp; 5 &amp; 7 \\ 2 &amp; 4 &amp; 6 &amp; 8 \\ -3 &amp; -2 &amp; -1 &amp; 0 \end{pmatrix} \quad \text{and} \quad \boldsymbol{u} = \begin{pmatrix} 10\\20\\30 \end{pmatrix} \]</span></p>
<ol style="list-style-type: lower-alpha">
<li>Print the matrix <span class="math inline">\(A\)</span>, the vector <span class="math inline">\(\boldsymbol{u}\)</span>, the shape of <span class="math inline">\(A\)</span>, and the shape of <span class="math inline">\(\boldsymbol{u}\)</span>.</li>
<li>Print the first column of <span class="math inline">\(A\)</span>.</li>
<li>Print the first two rows of <span class="math inline">\(A\)</span>.</li>
<li>Print the first two entries of <span class="math inline">\(\boldsymbol{u}\)</span>.</li>
<li>Print the last two entries of <span class="math inline">\(\boldsymbol{u}\)</span>.</li>
<li>Print the bottom left <span class="math inline">\(2 \times 2\)</span> submatrix of <span class="math inline">\(A\)</span>.</li>
<li>Print the middle two elements of the middle row of <span class="math inline">\(A\)</span>.</li>
</ol>
</div>

<hr />
<div style="page-break-after: always;"></div>
</div>
<div id="matrix-and-vector-operations" class="section level2">
<h2><span class="header-section-number">4.3</span> Matrix and Vector Operations</h2>
<p>Now let’s start doing some numerical linear algebra. We start our discussion with the basics: the dot product and matrix multiplication. The numerical routines in Python’s <code>numpy</code> packages are designed to do these tasks in very efficient ways but it is a good coding exercise to build your own dot product and matrix multiplication routines just to further cement the way that Python deals with these data structures and to remind you of the mathematical algorithms. What you will find in numerical linear algebra is that the indexing and the housekeeping in the codes is the hardest part. So why don’t we start “easy”.</p>
<div id="the-dot-product" class="section level3">
<h3><span class="header-section-number">4.3.1</span> The Dot Product</h3>

<div class="exercise">
<p><span id="exr:unnamed-chunk-293" class="exercise"><strong>Exercise 4.3  </strong></span>This problem is meant to jog your memory about dot products, how to compute them, and what you might use them for. If your linear algebra is a bit rusty then read ahead a bit and then come back to this problem.</p>
<p>Consider two vectors <span class="math inline">\(\boldsymbol{u}\)</span> and <span class="math inline">\(\boldsymbol{v}\)</span> defined as
<span class="math display">\[ \boldsymbol{u} = \begin{pmatrix} 1 \\ 2 \end{pmatrix} \quad \text{and} \quad \boldsymbol{v} = \begin{pmatrix} 3\\4 \end{pmatrix}. \]</span></p>
<ol style="list-style-type: lower-alpha">
<li>Draw a picture showing both <span class="math inline">\(\boldsymbol{u}\)</span> and <span class="math inline">\(\boldsymbol{v}\)</span>.</li>
<li>What is <span class="math inline">\(\boldsymbol{u} \cdot \boldsymbol{v}\)</span>?</li>
<li>What is <span class="math inline">\(\|\boldsymbol{u}\|\)</span>?</li>
<li>What is <span class="math inline">\(\|\boldsymbol{v}\|\)</span>?</li>
<li>What is the angle between <span class="math inline">\(\boldsymbol{u}\)</span> and <span class="math inline">\(\boldsymbol{v}\)</span>?</li>
<li>Give two reasons why we know that <span class="math inline">\(\boldsymbol{u}\)</span> is not perpendicular to <span class="math inline">\(\boldsymbol{v}\)</span>.</li>
<li>What is the scalar projection of <span class="math inline">\(\boldsymbol{u}\)</span> onto <span class="math inline">\(\boldsymbol{v}\)</span>? Draw this scalar projections on your picture from part (a).</li>
<li>What is the scalar projection of <span class="math inline">\(\boldsymbol{v}\)</span> onto <span class="math inline">\(\boldsymbol{u}\)</span>? Draw this scalar projections on your picture from part (a).</li>
</ol>
</div>

<hr />
<p>Now let’s get the formal definitions of the dot product on the table.</p>

<div class="definition">
<span id="def:unnamed-chunk-294" class="definition"><strong>Definition 4.1  (The Dot Product)  </strong></span>The <strong>dot product</strong> of two vectors <span class="math inline">\(\boldsymbol{u}, \boldsymbol{v} \in \mathbb{R}^n\)</span> is
<span class="math display">\[\boldsymbol{u} \cdot \boldsymbol{v} = \sum_{j=1}^n u_j v_j.\]</span>
Without summation notation the dot product of two vectors is ,
<span class="math display">\[\boldsymbol{u} \cdot \boldsymbol{v} = u_1 v_1 + u_2 v_2 + \cdots + u_n v_n.\]</span>
</div>

<p>Alternatively, you may also recall that the dot product of two vectors is given geometrically as
<span class="math display">\[ \boldsymbol{u} \cdot \boldsymbol{v} = \|\boldsymbol{u} \| \|\boldsymbol{v}\| \cos \theta \]</span>
where <span class="math inline">\(\|\boldsymbol{u}\|\)</span> and <span class="math inline">\(\|\boldsymbol{v}\|\)</span> are the magnitudes (or lengths) of <span class="math inline">\(\boldsymbol{u}\)</span> and <span class="math inline">\(\boldsymbol{v}\)</span> respectively, and <span class="math inline">\(\theta\)</span> is the angle between the two vectors. In physical applications the dot product is often used to find the angle between two vectors (e.g. between two forces). Hence, the last form of the dot product is often rewritten as
<span class="math display">\[ \theta = \cos^{-1}\left( \frac{\boldsymbol{u} \cdot \boldsymbol{v}}{ \|\boldsymbol{u} \| \| \boldsymbol{v} \|} \right). \]</span></p>
<hr />

<div class="definition">
<span id="def:unnamed-chunk-295" class="definition"><strong>Definition 4.2  (Magnitude of a Vector)  </strong></span>The <strong>magnitude</strong> of a vector <span class="math inline">\(\boldsymbol{u} \in \mathbb{R}^n\)</span> is defined as
<span class="math display">\[ \| \boldsymbol{u} \| = \sqrt{\boldsymbol{u} \cdot \boldsymbol{u}}. \]</span>
You should note that in two dimensions this collapses to the Pythagorean Theorem, and in higher dimensions this is just a natural extension of the Pythagorean Theorm.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-296" class="exercise"><strong>Exercise 4.4  </strong></span>Verify that <span class="math inline">\(\sqrt{\boldsymbol{u} \cdot \boldsymbol{u}}\)</span> indeed gives the Pythagorean Theorem for <span class="math inline">\(\boldsymbol{u} \in \mathbb{R}^2\)</span>.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-297" class="exercise"><strong>Exercise 4.5  </strong></span>Our task now is to write a Python function that accepts two vectors (defined as <code>numpy</code> arrays) and returns the dot product. Write this code without the use any loops.
</div>

<div class="sourceCode" id="cb99"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb99-1"><a href="ch-linearalgebra.html#cb99-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb99-2"><a href="ch-linearalgebra.html#cb99-2"></a><span class="kw">def</span> myDotProduct(u,v):</span>
<span id="cb99-3"><a href="ch-linearalgebra.html#cb99-3"></a>    <span class="cf">return</span> <span class="co"># the dot product formula uses a product inside a sum.</span></span></code></pre></div>
<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-299" class="exercise"><strong>Exercise 4.6  </strong></span>Test your <code>myDotProduct()</code> function on several dot products to make sure that it works. Example code to find the dot product between
<span class="math display">\[ \boldsymbol{u} = \begin{pmatrix} 1 \\ 2\\ 3\end{pmatrix} \quad \text{and} \quad \boldsymbol{v} = \begin{pmatrix} 4\\5\\6\end{pmatrix} \]</span>
is given below. Test your code on other vectors. Then implement an error catch into your code to catch the case where the two input vectors are not the same size. You will want to use the <code>len()</code> command to find the length of the vectors.
</div>

<div class="sourceCode" id="cb100"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb100-1"><a href="ch-linearalgebra.html#cb100-1"></a>u <span class="op">=</span> np.array([<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>])</span>
<span id="cb100-2"><a href="ch-linearalgebra.html#cb100-2"></a>v <span class="op">=</span> np.array([<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>])</span>
<span id="cb100-3"><a href="ch-linearalgebra.html#cb100-3"></a>myDotProduct(u,v)</span></code></pre></div>
<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-301" class="exercise"><strong>Exercise 4.7  </strong></span>Try sending Python lists instead of <code>numpy</code> arrays into your <code>myDotProduct</code> function. What happens? Why does it happen? What is the cautionary tale here? Modify your <code>myDotProduct()</code> function one more time so that it starts by converting the input vectors into <code>numpy</code> arrays.
</div>

<div class="sourceCode" id="cb101"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb101-1"><a href="ch-linearalgebra.html#cb101-1"></a>u <span class="op">=</span> [<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>]</span>
<span id="cb101-2"><a href="ch-linearalgebra.html#cb101-2"></a>v <span class="op">=</span> [<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>]</span>
<span id="cb101-3"><a href="ch-linearalgebra.html#cb101-3"></a>myDotProduct(u,v)</span></code></pre></div>
<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-303" class="exercise"><strong>Exercise 4.8  </strong></span>The <code>numpy</code> library in Python has a built-in command for doing the dot product: <code>np.dot()</code>. Test the <code>np.dot()</code> command and be sure that it does the same thing as your <code>myDotProduct()</code> function.
</div>

<hr />
</div>
<div id="matrix-multiplication" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Matrix Multiplication</h3>

<div class="exercise">
<p><span id="exr:unnamed-chunk-304" class="exercise"><strong>Exercise 4.9  </strong></span>Next we will blow the dust off of your matrix multiplication skills. Verify that the product of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is indeed what we show below. Work out all of the details by hand.
<span class="math display">\[ A = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ 5 &amp; 6 \end{pmatrix} \qquad B = \begin{pmatrix} 7 &amp; 8 &amp; 9 \\ 10 &amp; 11 &amp; 12 \end{pmatrix} \]</span>
<span class="math display">\[ AB = \begin{pmatrix} 27 &amp; 30 &amp; 33 \\ 61 &amp; 68 &amp; 75 \\ 95 &amp; 106 &amp; 117 \end{pmatrix} \]</span></p>
</div>

<hr />
<p>Now that you’ve practiced the algorithm for matrix multiplication we can formalize the definition and then turn the algorithm into a Python function.</p>
<hr />

<div class="definition">
<p><span id="def:unnamed-chunk-305" class="definition"><strong>Definition 4.3  (Matrix Multiplicaiton)  </strong></span>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are matrices with <span class="math inline">\(A \in \mathbb{R}^{n \times p}\)</span> and <span class="math inline">\(B \in \mathbb{R}^{p \times m}\)</span> then the product <span class="math inline">\(AB\)</span> is defined as
<span class="math display">\[ \left( AB \right)_{ij} = \sum_{k=1}^p A_{ik} B_{kj}.\]</span></p>
A moment’s reflection reveals that each entry in the matrix product is actually a dot product,
<span class="math display">\[ \left( \text{Entry in row $i$ column $j$ of $AB$} \right) = \left( \text{Row $i$ of matrix $A$} \right) \cdot \left( \text{Column $j$ of matrix $B$} \right).\]</span>
</div>

<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-306" class="exercise"><strong>Exercise 4.10  </strong></span>The definition of matrix multiplication above contains the cryptic phrase <em>a moment’s reflection reveals that each entry in the matrix product is actually a dot product.</em> Let’s go back to the matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> defined above and re-evaluate the matrix multiplication algorithm to make sure that you see each entry as the end result of a dot product.</p>
<p>We want to find the product of matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> using dot products.
<span class="math display">\[A = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ 5 &amp; 6 \end{pmatrix} \qquad B =
    \begin{pmatrix} 7 &amp; 8 &amp; 9 \\ 10 &amp; 11 &amp; 12 \end{pmatrix}\]</span></p>
<ol style="list-style-type: lower-alpha">
<li>Why will the product <span class="math inline">\(AB\)</span> clear be a <span class="math inline">\(3 \times 3\)</span> matrix?</li>
<li>When we do matrix multiplication we take the product of a row from the first matrix times a column from the second matrix … at least that’s how many people think of it when they perform the operation by hand.
<ol style="list-style-type: lower-roman">
<li>The rows of <span class="math inline">\(A\)</span> can be written as the vectors
<span class="math display">\[ \boldsymbol{a}_0 = \begin{pmatrix} 1 &amp; 2 \end{pmatrix} \]</span>
<span class="math display">\[ \boldsymbol{a}_1 = \begin{pmatrix} \underline{\hspace{0.5in}} &amp; \underline{\hspace{0.5in}} \end{pmatrix} \]</span>
<span class="math display">\[ \boldsymbol{a}_2 = \begin{pmatrix} \underline{\hspace{0.5in}} &amp; \underline{\hspace{0.5in}} \end{pmatrix} \]</span></li>
<li>The columns of <span class="math inline">\(B\)</span> can be written as the vectors
<span class="math display">\[ \boldsymbol{b}_0 = \begin{pmatrix} 7 \\ 10 \end{pmatrix} \]</span>
<span class="math display">\[ \boldsymbol{b}_1 = \begin{pmatrix} \underline{\hspace{0.5in}} \\ \underline{\hspace{0.5in}} \end{pmatrix} \]</span>
<span class="math display">\[ \boldsymbol{b}_2 = \begin{pmatrix} \underline{\hspace{0.5in}} \\ \underline{\hspace{0.5in}} \end{pmatrix} \]</span></li>
</ol></li>
<li>Now let’s write each entry in the product <span class="math inline">\(AB\)</span> as a dot product.
<span class="math display">\[ AB = \begin{pmatrix} \boldsymbol{a}_0 \cdot \boldsymbol{b}_0 &amp; \underline{\hspace{0.25in}} \cdot \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} \cdot \underline{\hspace{0.25in}} \\
\underline{\hspace{0.25in}} \cdot \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} \cdot \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} \cdot \underline{\hspace{0.25in}} \\ \underline{\hspace{0.25in}} \cdot \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} \cdot \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} \cdot \underline{\hspace{0.25in}} \end{pmatrix} \]</span></li>
<li>Verify that you get
<span class="math display">\[ AB = \begin{pmatrix} 27 &amp; 30 &amp; 33 \\ 61 &amp; 68 &amp; 75 \\ 95 &amp; 106 &amp; 117 \end{pmatrix}\]</span>
when you perform all of the dot products from part (c).</li>
</ol>
</div>

<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-307" class="exercise"><strong>Exercise 4.11  </strong></span>The observation that matrix multiplication is just a bunch of dot products is what makes the code for doing matrix multiplication very fast and very streamlined. We want to write a Python function that accepts two <code>numpy</code> matrices and returns the product of the two matrices. Inside the code we will leverage the <code>np.dot()</code> command to do the appropriate dot products.</p>
Partial code is given below. Fill in all of the details and give ample comments showing what each line does.
</div>

<div class="sourceCode" id="cb102"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb102-1"><a href="ch-linearalgebra.html#cb102-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb102-2"><a href="ch-linearalgebra.html#cb102-2"></a><span class="kw">def</span> myMatrixMult(A,B):</span>
<span id="cb102-3"><a href="ch-linearalgebra.html#cb102-3"></a>    <span class="co"># Get the shapes of the matrices A and B.</span></span>
<span id="cb102-4"><a href="ch-linearalgebra.html#cb102-4"></a>    <span class="co"># Then write an if statement that catches size mismatches </span></span>
<span id="cb102-5"><a href="ch-linearalgebra.html#cb102-5"></a>    <span class="co"># in the matrices.  Next build a zeros matrix that is the </span></span>
<span id="cb102-6"><a href="ch-linearalgebra.html#cb102-6"></a>    <span class="co"># correct size for the product of A and B. </span></span>
<span id="cb102-7"><a href="ch-linearalgebra.html#cb102-7"></a>    AB <span class="op">=</span> ??? </span>
<span id="cb102-8"><a href="ch-linearalgebra.html#cb102-8"></a>    <span class="co"># AB is a zeros matix that will be filled with the values </span></span>
<span id="cb102-9"><a href="ch-linearalgebra.html#cb102-9"></a>    <span class="co"># from the product</span></span>
<span id="cb102-10"><a href="ch-linearalgebra.html#cb102-10"></a>    <span class="co"># </span></span>
<span id="cb102-11"><a href="ch-linearalgebra.html#cb102-11"></a>    <span class="co"># Next we do a double for-loop that loops through all of </span></span>
<span id="cb102-12"><a href="ch-linearalgebra.html#cb102-12"></a>    <span class="co"># the indices of the product</span></span>
<span id="cb102-13"><a href="ch-linearalgebra.html#cb102-13"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n): <span class="co"># loop over the rows of AB</span></span>
<span id="cb102-14"><a href="ch-linearalgebra.html#cb102-14"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(m): <span class="co"># loop over the columns of AB</span></span>
<span id="cb102-15"><a href="ch-linearalgebra.html#cb102-15"></a>            <span class="co"># use the np.dot() command to take the dot product</span></span>
<span id="cb102-16"><a href="ch-linearalgebra.html#cb102-16"></a>            AB[i,j] <span class="op">=</span> ???</span>
<span id="cb102-17"><a href="ch-linearalgebra.html#cb102-17"></a>    <span class="cf">return</span> AB</span></code></pre></div>
<p>Use the following test code to determine if you actually get the correct matrix product out of your code.</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb103-1"><a href="ch-linearalgebra.html#cb103-1"></a>A <span class="op">=</span> np.matrix([[<span class="dv">1</span>,<span class="dv">2</span>],[<span class="dv">3</span>,<span class="dv">4</span>],[<span class="dv">5</span>,<span class="dv">6</span>]])</span>
<span id="cb103-2"><a href="ch-linearalgebra.html#cb103-2"></a>B <span class="op">=</span> np.matrix([[<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">9</span>],[<span class="dv">10</span>,<span class="dv">11</span>,<span class="dv">12</span>]])</span>
<span id="cb103-3"><a href="ch-linearalgebra.html#cb103-3"></a>AB <span class="op">=</span> myMatrixMult(A,B)</span>
<span id="cb103-4"><a href="ch-linearalgebra.html#cb103-4"></a><span class="bu">print</span>(AB)</span></code></pre></div>
<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-310" class="exercise"><strong>Exercise 4.12  </strong></span>Try your <code>myMatrixMult()</code> function on several other matrix multiplication problems.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-311" class="exercise"><strong>Exercise 4.13  </strong></span>Build in an error catch so that your <code>myMatrixMult()</code> function catches when the input matrices do not have compatible sizes for multiplication. Write your code so that it returns an appropriate error message in this special case.
</div>

<hr />
<p>Now that you’ve been through the exercise of building a matrix multiplication function we will admit that using it inside larger coding problems would be a bit cumbersome (and perhaps annoying). It would be nice to just type <code>*</code> and have Python just <em>know</em> that you mean to do matrix multiplication. The trouble is that there are many different versions of multiplication and any programming language needs to be told explicitly which type they’re dealing with. This is where <code>numpy</code> and <code>np.matrix()</code> come in quite handy.</p>
<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-312" class="exercise"><strong>Exercise 4.14  (Matrix Multiplication with Python)  </strong></span>Python will handle matrix multiplication easily so long as the matrices are defined as <code>numpy</code> matrices with <code>np.matrix()</code>. For example, with the matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> from above if you can just type <code>A*B</code> in Python and you will get the correct result. Pretty nice!! Let’s take another moment to notice, though, that regular Python arrays do not behave in the same way. What happens if you run the following Python code?
</div>

<div class="sourceCode" id="cb104"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb104-1"><a href="ch-linearalgebra.html#cb104-1"></a>A <span class="op">=</span> [[<span class="dv">1</span>,<span class="dv">2</span>],[<span class="dv">3</span>,<span class="dv">4</span>],[<span class="dv">5</span>,<span class="dv">6</span>]] <span class="co"># a Python list of lists</span></span>
<span id="cb104-2"><a href="ch-linearalgebra.html#cb104-2"></a>B <span class="op">=</span> [[<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">9</span>],[<span class="dv">10</span>,<span class="dv">11</span>,<span class="dv">12</span>]] <span class="co"># a Python list of lists</span></span>
<span id="cb104-3"><a href="ch-linearalgebra.html#cb104-3"></a>A<span class="op">*</span>B</span></code></pre></div>
<hr />

<div class="example">
<p><span id="exm:unnamed-chunk-314" class="example"><strong>Example 4.6  (Element-by-Element Multiplication)  </strong></span>Sometimes it is convenient to do naive multiplication of matrices when you code. That is, if you have two matrices that are the same size, “naive multiplication” would just line up the matrices on top of each other and multiply the corresponding entries.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> In Python the tool to do this is <code>np.multiply()</code>. The code below demonstrates this tool with the matrices
<span class="math display">\[ A = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ 5 &amp; 6 \end{pmatrix} \quad \text{and} \quad B = \begin{pmatrix} 7 &amp; 8 \\ 9 &amp; 10 \\ 11 &amp; 12 \end{pmatrix}. \]</span></p>
(Note that the product <span class="math inline">\(AB\)</span> does not make sense under the mathematical definition of matrix multiplication, but it does make sense in terms of element-by-element (“naive”) multiplication.)
</div>

<div class="sourceCode" id="cb105"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb105-1"><a href="ch-linearalgebra.html#cb105-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb105-2"><a href="ch-linearalgebra.html#cb105-2"></a>A <span class="op">=</span> [[<span class="dv">1</span>,<span class="dv">2</span>],[<span class="dv">3</span>,<span class="dv">4</span>],[<span class="dv">5</span>,<span class="dv">6</span>]]</span>
<span id="cb105-3"><a href="ch-linearalgebra.html#cb105-3"></a>B <span class="op">=</span> [[<span class="dv">7</span>,<span class="dv">8</span>],[<span class="dv">9</span>,<span class="dv">10</span>],[<span class="dv">11</span>,<span class="dv">12</span>]]</span>
<span id="cb105-4"><a href="ch-linearalgebra.html#cb105-4"></a>np.multiply(A,B)</span></code></pre></div>
<pre><code>## array([[ 7, 16],
##        [27, 40],
##        [55, 72]])</code></pre>
<hr />
<p>The key takeaways for doing matrix multiplication in Python are as follows:</p>
<ul>
<li>If you are doing linear algebra in Python then you should define vectors with <code>np.array()</code> and matrices with <code>np.matrix()</code>.</li>
<li>If your matrices are defined with <code>np.matrix()</code> then <code>*</code> does regular matrix multiplication and <code>np.multiply()</code> does element-by-element multiplication.</li>
</ul>
<hr />
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="the-lu-factorization" class="section level2">
<h2><span class="header-section-number">4.4</span> The LU Factorization</h2>
<p>One of the many classic problems of linear algebra is to solve the
linear system <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span> where <span class="math inline">\(A\)</span> is a matrix of coefficients and <span class="math inline">\(\boldsymbol{b}\)</span> is a vector of right-hand sides. You likely recall your go-to technique for solving systems was row reduction (or Gaussian Elimination or RREF). Furthermore, you likely recall from your linear algebra class that you rarely actually did row reduction by hand, and instead you relied on a computer to do most of the computations for you. Just what was the computer doing, exactly? Do you think that it was actually following the same algorithm that you did by hand?</p>
<div id="a-recap-of-row-reduction" class="section level3">
<h3><span class="header-section-number">4.4.1</span> A Recap of Row Reduction</h3>
<p>Let’s blow the dust off your row reduction skills before we look at something better.</p>
<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-316" class="exercise"><strong>Exercise 4.15  </strong></span>Solve the following system of equations by hand.
<span class="math display">\[ \begin{array}{rl} x_0 + 2x_1 + 3x_2 &amp;= 1 \\ 
                    4x_0 + 5x_1 + 6x_2 &amp;= 0 \\
                    7x_0 + 8x_1 &amp;= 2 \end{array} \]</span>
Note that the system of equations can also be written in the matrix form
<span class="math display">\[ \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 0 \end{pmatrix} \begin{pmatrix} x_0 \\ x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \\ 2\end{pmatrix} \]</span></p>
If you need a nudge to get started then jump ahead to the next problem.
</div>

<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-317" class="exercise"><strong>Exercise 4.16  </strong></span>We want to solve the system of equations
<span class="math display">\[ \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 0 \end{pmatrix} \begin{pmatrix} x_0 \\ x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \\ 2\end{pmatrix} \]</span></p>
<p><strong>Row Reduction Process:</strong></p>
<p><strong>Note:</strong> Throughout this discussion we use Python-type indexing so the rows and columns are enumerated starting at 0. That is to say, we will talk about row 0, row 1, and row 2 of a matrix instead of rows 1, 2, and 3.</p>
<ol style="list-style-type: lower-alpha">
<li>Augment the coefficient matrix and the vector on the right-hand side to get
<span class="math display">\[ \left( \begin{array}{ccc|c} 1 &amp; 2 &amp; 3 &amp; 1 \\ 4 &amp; 5 &amp; 6 &amp; 0 \\ 7 &amp; 8 &amp; 0 &amp; 2 \end{array} \right)\]</span></li>
<li>The goal of row reduction is to perform elementary row operations until our augmented matrix gets to (or at least gets as close as possible to)
<span class="math display">\[ \left( \begin{array}{ccc|c} 1 &amp; 0 &amp; 0 &amp; \star \\ 0 &amp; 1 &amp; 0 &amp; \star \\ 0 &amp; 0 &amp; 1 &amp; \star \end{array} \right) \]</span>
The allowed elementary row operations are:
<ol style="list-style-type: lower-roman">
<li>We are allowed to scale any row.</li>
<li>We can add two rows.</li>
<li>We can interchange two rows.</li>
</ol></li>
<li>We are going to start with column 0. We already have the “<span class="math inline">\(1\)</span>” in the top left corner so we can use it to eliminate all of the other values in the first column of the matrix.
<ol style="list-style-type: lower-roman">
<li>For example, if we multiply the <span class="math inline">\(0^{th}\)</span> row by <span class="math inline">\(-4\)</span> and add it to the first row we get
<span class="math display">\[ \left( \begin{array}{ccc|c} 1 &amp; 2 &amp; 3 &amp; 1 \\ 0 &amp; -3 &amp; -6 &amp; -4 \\ 7 &amp; 8 &amp; 0 &amp; 2 \end{array} \right). \]</span></li>
<li>Multiply row 0 by a scalar and add it to row 2. Your end result should be
<span class="math display">\[ \left( \begin{array}{ccc|c} 1 &amp; 2 &amp; 3 &amp; 1 \\ 0 &amp; -3 &amp; -6 &amp; -4 \\ 0 &amp; -6 &amp; -21 &amp; -5 \end{array} \right). \]</span>
What did you multiply by? Why?</li>
</ol></li>
<li>Now we should deal with column 1.
<ol style="list-style-type: lower-roman">
<li>We want to get a 1 in row 1 column 1. We can do this by scaling row 1. What did you scale by? Why? Your end result should be
<span class="math display">\[ \left( \begin{array}{ccc|c} 1 &amp; 2 &amp; 3 &amp; 1 \\ 0 &amp; 1 &amp; 2 &amp; \frac{4}{3} \\ 0 &amp; -6 &amp; -21 &amp; -5 \end{array} \right). \]</span></li>
<li>Now scale row 1 by something and add it to row 0 so that the entry in row 0 column 1 becomes a 0.<br />
</li>
<li>Next scale row 1 by something and add it to row 2 so that the entry in row 2 column 1 becomes a 0.<br />
</li>
<li>At this point you should have the augmented system
<span class="math display">\[ \left( \begin{array}{ccc|c} 1 &amp; 0 &amp; -1 &amp; -\frac{5}{3} \\ 0 &amp; 1 &amp; 2 &amp; \frac{4}{3} \\ 0 &amp; 0 &amp; -9 &amp; 3 \end{array} \right). \]</span></li>
</ol></li>
<li>Finally we need to work with column 2.
<ol style="list-style-type: lower-roman">
<li>Make the value in row 2 column 2 a 1 by scaling row 2. What did you scale by? Why?</li>
<li>Scale row 2 by something and add it to row 1 so that the entry in row 1 column 2 becomes a 0. What did you scale by? Why?</li>
<li>Scale row 2 by something and add it to row 0 so that the entry in row 0 column 2 becomes a 0. What did you scale by? Why?</li>
<li>By the time you’ve made it this far you should have the system
<span class="math display">\[ \left( \begin{array}{ccc|c} 1 &amp; 0 &amp; 0 &amp; -2 \\ 0 &amp; 1 &amp; 0 &amp; 2 \\ 0 &amp; 0 &amp; 1 &amp; -\frac{1}{3} \end{array} \right) \]</span>
and you should be able to read off the solution to the system.</li>
</ol></li>
<li>You should verify your answer in two different ways:
<ol style="list-style-type: lower-roman">
<li>If you substitute your values into the original system then all of the equal signs should be true. Verify this.</li>
<li>If you substitute your values into the matrix equation and perform the matrix-vector multiplication on the left-hand side of the equation you should get the right-hand side of the equation. Verify this.</li>
</ol></li>
</ol>
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-318" class="exercise"><strong>Exercise 4.17  </strong></span>Summarize the process for doing Gaussian Elimination to solve a square system of linear equations.
</div>

<hr />
</div>
<div id="the-lu-decomposition" class="section level3">
<h3><span class="header-section-number">4.4.2</span> The LU Decomposition</h3>
<p>You may have used the <code>rref()</code> command either on a calculator in other software to perform row reduction in the past. You will be surprised to learn that there is no <code>rref()</code> command in Python’s <code>numpy</code> library! That’s because there are far more efficient and stable ways to solve a linear system on a computer. There is an <code>rref</code> command in Python’s <code>sympy</code> (symbolic Python) library, but given that it works with symbolic algebra it is quite slow.</p>
<p>In solving systems of equations we are interested in equations of the form <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span>. Notice that the <span class="math inline">\(\boldsymbol{b}\)</span> vector is just along for the ride, so to speak, in the row reduction process since none of the values in <span class="math inline">\(\boldsymbol{b}\)</span> actually cause you to make different decisions in the row reduction algorithm. Hence, we only really need to focus on the matrix <span class="math inline">\(A\)</span>. Furthermore, let’s change our awfully restrictive view of always seeking a matrix of the form
<span class="math display">\[ \left( \begin{array}{cccc|c} 1 &amp; 0 &amp; \cdots &amp; 0  &amp; \star \\ 0 &amp; 1 &amp; \cdots &amp; 0 &amp; \star \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; 1 &amp; \star \end{array} \right) \]</span>
and instead say:</p>
<blockquote>
<p><em>What if we just row reduce until the system is simple enough to solve by hand?</em></p>
</blockquote>
<p>That’s what the next several exercises are going to lead you to. Our goal here is to develop an algorithm that is fast to implement on a computer and simultaneously performs the same basic operations as row reduction for solving systems of linear equations.</p>
<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-319" class="exercise"><strong>Exercise 4.18  </strong></span>Let <span class="math inline">\(A\)</span> be defined as
<span class="math display">\[ A = \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 0 \end{pmatrix}. \]</span></p>
<ol style="list-style-type: lower-alpha">
<li>The first step in row reducing <span class="math inline">\(A\)</span> would be to multiply row 0 by <span class="math inline">\(-4\)</span> and add it to row 1. Do this operation by hand so that you know what the result is supposed to be. Check out the following amazing observation. Define the matrix <span class="math inline">\(L_1\)</span> as follows:
<span class="math display">\[ L_1 = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ -4 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix}. \]</span>
Now multiply <span class="math inline">\(L_1\)</span> and <span class="math inline">\(A\)</span>.
<span class="math display">\[ L_1 A = \begin{pmatrix} \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} \\ \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} \\ \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} \end{pmatrix} \]</span>
What just happened?!</li>
<li>Let’s do it again. The next step in the row reduction of your result from part (b) would be to multiply row 0 by <span class="math inline">\(-7\)</span> and add to row 2. Again, do this by hand so you know what the result should be. Then define the matrix <span class="math inline">\(L_2\)</span> as
<span class="math display">\[ L_2 = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ -7 &amp; 0 &amp; 1 \end{pmatrix} \]</span>
and find the product <span class="math inline">\(L_2 \left( L_1 A \right)\)</span>.
<span class="math display">\[ L_2 \left( L_1 A \right) = \begin{pmatrix} \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} \\ \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} \\ \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} \end{pmatrix} \]</span>
Pure insanity!!<br />
</li>
<li>Now let’s say that you want to make the entry in row 2 column 1 into a 0 by scaling row 1 by something and then adding to row 2. Determine what the scalar would be and then determine which matrix, call it <span class="math inline">\(L_3\)</span>, would do the trick so that <span class="math inline">\(L_3 (L_2 L_1 A)\)</span> would be the next row reduced step.
<span class="math display">\[ L_3 = \begin{pmatrix} 1 &amp; \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} \\ \underline{\hspace{0.25in}} &amp; 1 &amp; \underline{\hspace{0.25in}} \\ \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} &amp; 1 \end{pmatrix} \]</span></li>
</ol>
<p><span class="math display">\[ L_3 \left( L_2 L_1 A \right) = \begin{pmatrix} \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} \\ \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} \\ \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} \end{pmatrix} \]</span></p>
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-320" class="exercise"><strong>Exercise 4.19  </strong></span>Apply the same idea from the previous problem to do the first three steps of row reduction to the matrix
<span class="math display">\[ A = \begin{pmatrix} 2 &amp; 6 &amp; 9 \\ -6 &amp; 8 &amp; 1 \\ 2 &amp; 2 &amp; 10 \end{pmatrix} \]</span>
</div>

<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-321" class="exercise"><strong>Exercise 4.20  </strong></span>Now let’s make a few observations about the two previous problems.</p>
<ol style="list-style-type: lower-alpha">
<li>What will multiplying <span class="math inline">\(A\)</span> by a matrix of the form
<span class="math display">\[ \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ c &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix} \]</span>
do?</li>
<li>What will multiplying <span class="math inline">\(A\)</span> by a matrix of the form
<span class="math display">\[ \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ c &amp; 0 &amp; 1 \end{pmatrix} \]</span>
do?</li>
<li>What will multiplying <span class="math inline">\(A\)</span> by a matrix of the form
<span class="math display">\[ \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; c &amp; 1 \end{pmatrix} \]</span>
do?</li>
<li>More generally: If you wanted to multiply row <span class="math inline">\(j\)</span> of an <span class="math inline">\(n\times n\)</span> matrix by <span class="math inline">\(c\)</span> and add it to row <span class="math inline">\(k\)</span>, that is the same as multiplying by what matrix?</li>
</ol>
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-322" class="exercise"><strong>Exercise 4.21  </strong></span>After doing all of the matrix products, <span class="math inline">\(L_3 L_2 L_1 A\)</span>, the resulting matrix will have zeros in the entire lower triangle. That is, all of the nonzero entries of the resulting matrix will be on the main diagonal or above. We call this matrix <span class="math inline">\(U\)</span>, for upper triangular. Hence, we have formed a matrix
<span class="math display">\[ L_3 L_2 L_1 A = U \]</span>
and if we want to solve for <span class="math inline">\(A\)</span> we would get
<span class="math display">\[ A = (\underline{\hspace{0.5in}})^{-1} (\underline{\hspace{0.5in}})^{-1} (\underline{\hspace{0.5in}})^{-1} U \]</span>
(Take care that everything is in the right order in your answer.)
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-323" class="exercise"><strong>Exercise 4.22  </strong></span>It would be nice, now, if the inverses of the <span class="math inline">\(L\)</span> matrices were easy to find. Use <code>np.linalg.inv()</code> to directly compute the inverse of <span class="math inline">\(L_1\)</span>, <span class="math inline">\(L_2\)</span>, and <span class="math inline">\(L_3\)</span> for each of the example matrices. Then complete the statement:
If <span class="math inline">\(L_k\)</span> is an identity matrix with some nonzero <span class="math inline">\(c\)</span> in row <span class="math inline">\(i\)</span> and column <span class="math inline">\(j\)</span> then <span class="math inline">\(L_k^{-1}\)</span> is what matrix?
</div>

<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-324" class="exercise"><strong>Exercise 4.23  </strong></span>We started this discussion with <span class="math inline">\(A\)</span> as
<span class="math display">\[ A = \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 0 \end{pmatrix} \]</span>
and we defined
<span class="math display">\[ L_1 = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ -4 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix}, \quad L_2 = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ -7 &amp; 0 &amp; 1 \end{pmatrix}, \quad \text{and} \quad L_3 = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; -2 &amp; 1 \end{pmatrix}.  \]</span>
Based on your answer to the previous exercises we know that
<span class="math display">\[ A = L_1^{-1} L_2^{-1} L_3^{-1} U. \]</span>
Explicitly write down the matrices <span class="math inline">\(L_1^{-1}\)</span>, <span class="math inline">\(L_2^{-1}\)</span>, and <span class="math inline">\(L_3^{-1}\)</span>.</p>
Now explicitly find the product <span class="math inline">\(L_1^{-1} L_2^{-1} L_3^{-1}\)</span> and call this product <span class="math inline">\(L\)</span>. Verify that <span class="math inline">\(L\)</span> itself is also a lower triangular matrix with ones on the main diagonal. Moreover, take note of exactly the form of the matrix. The answer should be super surprising to you!!
</div>

<hr />
<p>Throughout all of the preceding exercises, our final result is that we have factored the matrix <span class="math inline">\(A\)</span> into the product of a lower triangular matrix and an upper triangular matrix. Stop and think about that for a minute … we just factored a matrix!</p>
<p>Let’s return now to our discussion of solving the system of equations <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span>. If <span class="math inline">\(A\)</span> can be factored into <span class="math inline">\(A = LU\)</span> then the system of equations can be rewritten as <span class="math inline">\(LU \boldsymbol{x} = \boldsymbol{b}\)</span>. As we will see in the next subsection, solving systems of equations with triangular matrices is super fast and relatively simple! Hence, we have partially achieved our modified goal of reducing the row reduction into some simpler case.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></p>
<p>It remains to implement the <span class="math inline">\(LU\)</span> decomposition (also called the <span class="math inline">\(LU\)</span> factorization) in Python.</p>
<hr />

<div class="definition">
<span id="def:lu" class="definition"><strong>Definition 4.4  (The LU Factorization)  </strong></span>The following Python function takes a square matrix <span class="math inline">\(A\)</span> and outputs the matrices <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span> such that <span class="math inline">\(A = LU\)</span>. The entire code is given to you. It will be up to you in the next exercise to pick apart every step of the function.
</div>

<div class="sourceCode" id="cb107"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb107-1"><a href="ch-linearalgebra.html#cb107-1"></a><span class="kw">def</span> myLU(A):</span>
<span id="cb107-2"><a href="ch-linearalgebra.html#cb107-2"></a>    n <span class="op">=</span> A.shape[<span class="dv">0</span>] <span class="co"># get the dimension of the matrix A</span></span>
<span id="cb107-3"><a href="ch-linearalgebra.html#cb107-3"></a>    L <span class="op">=</span> np.matrix( np.identity(n) ) <span class="co"># Build the identity part L</span></span>
<span id="cb107-4"><a href="ch-linearalgebra.html#cb107-4"></a>    U <span class="op">=</span> np.copy(A) <span class="co"># start the U matrix as a copy of A</span></span>
<span id="cb107-5"><a href="ch-linearalgebra.html#cb107-5"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,n<span class="dv">-1</span>):</span>
<span id="cb107-6"><a href="ch-linearalgebra.html#cb107-6"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(j<span class="op">+</span><span class="dv">1</span>,n):</span>
<span id="cb107-7"><a href="ch-linearalgebra.html#cb107-7"></a>            mult <span class="op">=</span> A[i,j] <span class="op">/</span> A[j,j]</span>
<span id="cb107-8"><a href="ch-linearalgebra.html#cb107-8"></a>            U[i, j<span class="op">+</span><span class="dv">1</span>:n] <span class="op">=</span> U[i, j<span class="op">+</span><span class="dv">1</span>:n] <span class="op">-</span> mult <span class="op">*</span> U[j,j<span class="op">+</span><span class="dv">1</span>:n]</span>
<span id="cb107-9"><a href="ch-linearalgebra.html#cb107-9"></a>            L[i,j] <span class="op">=</span> mult</span>
<span id="cb107-10"><a href="ch-linearalgebra.html#cb107-10"></a>            U[i,j] <span class="op">=</span> <span class="dv">0</span> <span class="co"># why are we doing this?</span></span>
<span id="cb107-11"><a href="ch-linearalgebra.html#cb107-11"></a>    <span class="cf">return</span> L,U</span></code></pre></div>
<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-326" class="exercise"><strong>Exercise 4.24  </strong></span>Go to Definition <a href="ch-linearalgebra.html#def:lu">4.4</a> and go through every iteration of every loop <strong>by hand</strong> starting with the matrix
<span class="math display">\[ A = \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 0 \end{pmatrix}. \]</span>
Give details of what happens at every step of the algorithm. I’ll get you started.</p>
<ul>
<li><code>n=3</code>, <code>L</code> starts as an identity matrix of the correct size, and <code>U</code> starts as a copy of <code>A</code>.</li>
<li>Start the outer loop: <code>j=0</code>: (<code>j</code> is the counter for the column)
<ul>
<li>Start the inner loop: <code>i=1</code>: (<code>i</code> is the counter for the row)
<ul>
<li><code>mult = A[1,0] / A[0,0]</code> so <code>mult</code><span class="math inline">\(=4/1\)</span>.</li>
<li><code>A[1, 1:3] = A[1, 1:3] - 4 * A[0,1:3]</code>. Translated, this states that columns 1 and 2 of matrix <span class="math inline">\(A\)</span> took their original value minus 4 times the corresponding values in row 0.</li>
<li><code>U[1, 1:3] = A[1, 1:3]</code>. Now we replace the locations in <span class="math inline">\(U\)</span> with the updated information from our first step of row reduction.</li>
<li><code>L[1,0]=4</code>. We now fill the <span class="math inline">\(L\)</span> matrix with the proper value.</li>
<li><code>U[1,0]=0</code>. Finally, we zero out the lower triangle piece of the <span class="math inline">\(U\)</span> matrix which we’ve now taken care of.</li>
</ul></li>
<li><code>i=2</code>:
<ul>
<li>… keep going from here …</li>
</ul></li>
</ul></li>
</ul>
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-327" class="exercise"><strong>Exercise 4.25  </strong></span>Apply your new <code>myLU</code> code to other square matrices and verify that indeed <span class="math inline">\(A\)</span> is the product of the resulting <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span> matrices. You can produce a random matrix with <code>np.random.randn(n,n)</code> where <code>n</code> is the number of rows and columns of the matrix. For example, <code>np.random.randn(10,10)</code> will produce a random <span class="math inline">\(10 \times 10\)</span> matrix with entries chosen from the normal distribution with center 0 and standard deviation 1. Random matrices are just as good as any other when testing your algorithm.
</div>

<hr />
</div>
<div id="solving-triangular-systems" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Solving Triangular Systems</h3>
<p>We now know that row reduction is just a collection of sneaky matrix multiplications. In the previous exercises we saw that we can often turn our system of equations <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span> into the system <span class="math inline">\(LU \boldsymbol{x} = \boldsymbol{b}\)</span> where <span class="math inline">\(L\)</span> us lower triangular (with ones on the main diagonal) and <span class="math inline">\(U\)</span> is upper triangular. But why was this important?</p>
<p>Well, if <span class="math inline">\(LU \boldsymbol{x} = \boldsymbol{b}\)</span> then we can rewrite our system of equations as two systems:
<span class="math display">\[ \text{An upper triangular system: } U \boldsymbol{x} = \boldsymbol{y} \]</span>
and
<span class="math display">\[ \text{A lower triangular system: } L \boldsymbol{y} = \boldsymbol{b}. \]</span></p>
<p>In the following exercises we will devise algorithms for solving triangular systems. After we know how to work with triangular systems we’ll put all of the pieces together and show how to leverage the <span class="math inline">\(LU\)</span> decomposition and the solution techniques for triangular systems to quickly and efficiently solve linear systems.</p>
<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-328" class="exercise"><strong>Exercise 4.26  </strong></span>Outline a fast algorithm (without formal row reduction) for solving the lower triangular system
<span class="math display">\[\begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 4 &amp; 1 &amp; 0 \\ 7 &amp; 2 &amp; 1 \end{pmatrix} \begin{pmatrix}
        y_0 \\ y_1 \\ y_2 \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \\ 2\end{pmatrix}.\]</span>
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-329" class="exercise"><strong>Exercise 4.27  </strong></span>As a convention we will always write our lower triangular matrices with ones on the main diagonal. Generalize your steps from the previous exercise so that you have an algorithm for solving any lower triangular system. The most natural algorithm that most people devise here is called <strong>forward substitution</strong>.
</div>

<hr />

<div class="definition">
<p><span id="def:lsolve" class="definition"><strong>Definition 4.5  (The Forward Substutition Algorithm (<code>lsolve</code>))  </strong></span>The general statement of the Forward Substitution Algorithm is:</p>
<p><em>Solve <span class="math inline">\(L \boldsymbol{y} = \boldsymbol{b}\)</span> for <span class="math inline">\(\boldsymbol{y}\)</span>, where the matrix <span class="math inline">\(L\)</span> is assumed to be lower triangular with ones on the main diagonal.</em></p>
The code below gives a full implementation of the <strong>Forward Substitution</strong> algorithm (also called the <code>lsolve</code> algorithm).
</div>

<div class="sourceCode" id="cb108"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb108-1"><a href="ch-linearalgebra.html#cb108-1"></a><span class="kw">def</span> lsolve(L, b):</span>
<span id="cb108-2"><a href="ch-linearalgebra.html#cb108-2"></a>    L <span class="op">=</span> np.matrix(L) <span class="co"># make sure L is the correct data type</span></span>
<span id="cb108-3"><a href="ch-linearalgebra.html#cb108-3"></a>    n <span class="op">=</span> b.size <span class="co"># what does this do?</span></span>
<span id="cb108-4"><a href="ch-linearalgebra.html#cb108-4"></a>    y <span class="op">=</span> np.matrix( np.zeros( (n,<span class="dv">1</span>)) ) <span class="co"># what does this do?</span></span>
<span id="cb108-5"><a href="ch-linearalgebra.html#cb108-5"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb108-6"><a href="ch-linearalgebra.html#cb108-6"></a>        <span class="co"># start the loop by assigning y to the value on the right</span></span>
<span id="cb108-7"><a href="ch-linearalgebra.html#cb108-7"></a>        y[i] <span class="op">=</span> b[i] </span>
<span id="cb108-8"><a href="ch-linearalgebra.html#cb108-8"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i): <span class="co"># now adjust y </span></span>
<span id="cb108-9"><a href="ch-linearalgebra.html#cb108-9"></a>            y[i] <span class="op">=</span> y[i] <span class="op">-</span> L[i,j] <span class="op">*</span> y[j]</span>
<span id="cb108-10"><a href="ch-linearalgebra.html#cb108-10"></a>    <span class="cf">return</span>(y)</span></code></pre></div>
<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-331" class="exercise"><strong>Exercise 4.28  </strong></span>Work with your partner(s) to apply the <code>lsolve()</code> code to the lower triangular system<br />
<span class="math display">\[\begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 4 &amp; 1 &amp; 0 \\ 7 &amp; 2 &amp; 1 \end{pmatrix} \begin{pmatrix} y_0 \\ y_1 \\ y_2 \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \\ 2\end{pmatrix}\]</span>
<strong>by hand</strong>. It is incredibly important to impelement numerical linear algebra routines by hand a few times so that you truly understand how everything is being tracked and calculated.</p>
<p>I’ll get you started.</p>
<ul>
<li>Start: <code>i=0</code>:
<ul>
<li><code>y[0]=1</code> since <code>b[0]=1</code>.</li>
<li>The next <code>for</code> loop does not start since <code>range(0)</code> has no elements (stop and think about why this is).</li>
</ul></li>
<li>Next step in the loop: <code>i=1</code>:
<ul>
<li><code>y[1]</code> is initialized as 0 since <code>b[1]=0</code>.</li>
<li>Now we enter the inner loop at <code>j=0</code>:
<ul>
<li>What does <code>y[1]</code> become when <code>j=0</code>?</li>
</ul></li>
<li>Does <code>j</code> increment to anything larger?</li>
</ul></li>
<li>Finally we increment <code>i</code> to <code>i=2</code>:
<ul>
<li>What does <code>y[2]</code> get initialized to?</li>
<li>Enter the inner loop at <code>j=0</code>:
<ul>
<li>What does <code>y[2]</code> become when <code>j=0</code>?</li>
</ul></li>
<li>Increment the inner loop to <code>j=1</code>:
<ul>
<li>What does <code>y[2]</code> become when <code>j=1</code>?</li>
</ul></li>
</ul></li>
<li>Stop</li>
</ul>
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-332" class="exercise"><strong>Exercise 4.29  </strong></span>Copy the code from Definition <a href="ch-linearalgebra.html#def:lsolve">4.5</a> into a Python function but in your code write a comment on every line stating what it is doing. Write a test script that creates a lower triangular matrix of the correct form and a right-hand side <span class="math inline">\(\boldsymbol{b}\)</span> and solve for <span class="math inline">\({\bf y}\)</span>. Test your code by giving it a large lower triangular system.
</div>

<hr />
<p>Now that we have a method for solving lower triangular systems, let’s build a similar method for solving upper triangular systems. The merging of lower and upper triangular systems will play an important role in solving systems of equations.</p>
<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-333" class="exercise"><strong>Exercise 4.30  </strong></span>Outline a fast algorithm (without formal row reduction) for solving the upper triangular system
<span class="math display">\[\begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 0 &amp; -3 &amp; -6 \\ 0 &amp; 0 &amp; -9 \end{pmatrix} \begin{pmatrix} x_0 \\ x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} 1 \\ -4 \\ 3\end{pmatrix}\]</span>
The most natural algorithm that most people devise here is called <strong>backward substitution</strong>. Notice that in our upper
triangular matrix we do not have a diagonal containing all ones.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-334" class="exercise"><strong>Exercise 4.31  </strong></span>Generalize your backward substitution algorithm from the previous problem so that it could be applied to any upper triangular system.
</div>

<hr />

<div class="definition">
<span id="def:usolve" class="definition"><strong>Definition 4.6  (Backward Substitution Algorithm)  </strong></span>The following code solves the problem <span class="math inline">\(U \boldsymbol{x} = \boldsymbol{y}\)</span> using backward substitution. The matrix <span class="math inline">\(U\)</span> is assumed to be upper triangular. You’ll notice that most of this code is incomplete. It is your job to complete this code, and the next exercise should help.
</div>

<div class="sourceCode" id="cb109"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb109-1"><a href="ch-linearalgebra.html#cb109-1"></a><span class="kw">def</span> usolve(U, y):</span>
<span id="cb109-2"><a href="ch-linearalgebra.html#cb109-2"></a>    U <span class="op">=</span> np.matrix(U)</span>
<span id="cb109-3"><a href="ch-linearalgebra.html#cb109-3"></a>    n <span class="op">=</span> y.size</span>
<span id="cb109-4"><a href="ch-linearalgebra.html#cb109-4"></a>    x <span class="op">=</span> np.matrix( np.zeros( (n,<span class="dv">1</span>)))</span>
<span id="cb109-5"><a href="ch-linearalgebra.html#cb109-5"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>( ??? ):     <span class="co"># what should we be looping over?</span></span>
<span id="cb109-6"><a href="ch-linearalgebra.html#cb109-6"></a>        x[i] <span class="op">=</span> y[i] <span class="op">/</span> ???      <span class="co"># what should we be dividing by?</span></span>
<span id="cb109-7"><a href="ch-linearalgebra.html#cb109-7"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>( ??? ): <span class="co"># what should we be looping over:</span></span>
<span id="cb109-8"><a href="ch-linearalgebra.html#cb109-8"></a>            x[i] <span class="op">=</span> x[i] <span class="op">-</span> U[i,j] <span class="op">*</span> x[j] <span class="op">/</span> ??? <span class="co"># complete this line </span></span>
<span id="cb109-9"><a href="ch-linearalgebra.html#cb109-9"></a>            <span class="co"># ... what does the previous line do?</span></span>
<span id="cb109-10"><a href="ch-linearalgebra.html#cb109-10"></a>    <span class="cf">return</span>(x)</span></code></pre></div>
<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-336" class="exercise"><strong>Exercise 4.32  </strong></span>Now we will work through the backward substitution algorithm to help fill in the blanks in the code. Consider the upper triangular system
<span class="math display">\[ \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 0 &amp; -3 &amp; -6 \\ 0 &amp; 0 &amp; -9 \end{pmatrix} \begin{pmatrix} x_0 \\ x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} 1 \\ -4 \\ 3\end{pmatrix} \]</span></p>
<p>Work the code from Definition <a href="ch-linearalgebra.html#def:usolve">4.6</a> to solve the system. Keep track of all of the indices as you work through the code. You may want to work this problem in conjunction with the previous two problems to unpack all of the parts of the <em>backward substitution</em> algorithm.</p>
<p>I’ll get you started.</p>
<ul>
<li>In your backward substitution algorithm you should have started with the last row, therefore the outer loop starts at <code>n-1</code> and reads backward to 0. (Why are we starting at <code>n-1</code> and not <code>n</code>?)</li>
<li>Outer loop: <code>i=2</code>:
<ul>
<li>We want to solve the equation <span class="math inline">\(-9x_2 = 3\)</span> so the clear solution is to divide by <span class="math inline">\(-9\)</span>. In code this means that <code>x[2]=y[2]/U[2,2]</code>.<br />
</li>
<li>There is nothing else to do for row 3 of the matrix, so we should not enter the inner loop. How can we keep from entering the inner loop?<br />
</li>
</ul></li>
<li>Outer loop: <code>i=1</code>:
<ul>
<li>Now we are solving the algebraic equation <span class="math inline">\(-3x_1 - 6x_2 = -4\)</span>. If we follow the high school algebra we see that <span class="math inline">\(x_1 = \frac{-4 - (-6)x_2}{-3}\)</span> but this can be rearranged to
<span class="math display">\[ x_1 = \frac{-4}{-3} - \frac{-6x_2}{-3}. \]</span>
So we can initialize <span class="math inline">\(x_1\)</span> with <span class="math inline">\(x_1 = \frac{-4}{-3}\)</span>. In code, this means that we initialize with <code>x[1] = y[1] / U[1,1]</code>.</li>
<li>Now we need to enter the inner loop at <code>j=2</code>: (why are we entering the loop at <code>j=2</code>?)
<ul>
<li>To complete the algebra we need to take our initialized value of <code>x[1]</code> and subtract off <span class="math inline">\(\frac{-6x_2}{-3}\)</span>. In code this is <code>x[1] = x[1] - U[1,2] * x[2] / U[1,1]</code></li>
</ul></li>
<li>There is nothing else to do so the inner loop should end.</li>
</ul></li>
<li>Outer loop: <code>i=0</code>:
<ul>
<li>Finally, we are solving the algebraic equation <span class="math inline">\(x_0 + 2x_1 + 3 x_2 = 1\)</span> for <span class="math inline">\(x_0\)</span>. The clear and obvious solution is <span class="math inline">\(x_0 = \frac{1 - 2x_1 - 3x_2}{1}\)</span> (why am I explicitly showing the division by <span class="math inline">\(1\)</span> here?).<br />
</li>
<li>Initialize <span class="math inline">\(x_0\)</span> at <code>x[0] = ???</code></li>
<li>Enter the inner loop at <code>j=2</code>:
<ul>
<li>Adjust the value of <code>x[0]</code> by subtracting off <span class="math inline">\(\frac{3x_2}{1}\)</span>. In code we have `x[0] = x[0] - ??? * ??? / ???</li>
</ul></li>
<li>Increment <code>j</code> to <code>j=1</code>:
<ul>
<li>Adjust the value of <code>x[0]</code> by subtracting off <span class="math inline">\(\frac{2x_1}{1}\)</span>. In code we have `x[0] = x[0] - ??? * ??? / ???</li>
</ul></li>
</ul></li>
<li>Stop.</li>
<li>You should now have a solution to the equation <span class="math inline">\(U \boldsymbol{x} = \boldsymbol{y}\)</span>. Substitute your solution in and verify that your solution is correct.</li>
</ul>
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-337" class="exercise"><strong>Exercise 4.33  </strong></span>Copy the code from Definition <a href="ch-linearalgebra.html#def:usolve">4.6</a> into a Python function but in your code write a comment on every line stating what it is doing. Write a test script that creates an upper triangular matrix of the correct form and a right-hand side <span class="math inline">\(\boldsymbol{y}\)</span> and solve for <span class="math inline">\(\boldsymbol{x}\)</span>. Your code needs to work on
systems of arbitrarily large size.
</div>

<hr />
</div>
<div id="solving-systems-with-lu" class="section level3">
<h3><span class="header-section-number">4.4.4</span> Solving Systems with LU</h3>
<p>We are finally ready for the punch line of this whole <span class="math inline">\(LU\)</span> and triangular systems business!</p>
<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-338" class="exercise"><strong>Exercise 4.34  </strong></span>If we want to solve <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span> then</p>
<ol style="list-style-type: lower-alpha">
<li>If we can, write the system of equations as <span class="math inline">\(LU \boldsymbol{x} = \boldsymbol{b}\)</span>.</li>
<li>Solve <span class="math inline">\(L \boldsymbol{y} = \boldsymbol{b}\)</span> for <span class="math inline">\(\boldsymbol{y}\)</span> using forward substitution.</li>
<li>Solve <span class="math inline">\(U \boldsymbol{x} = \boldsymbol{y}\)</span> for <span class="math inline">\(\boldsymbol{x}\)</span> using backward substitution.</li>
</ol>
<p>Pick a matrix <span class="math inline">\(A\)</span> and a right-hand side <span class="math inline">\(\boldsymbol{b}\)</span> and solve the system using this process.</p>
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-339" class="exercise"><strong>Exercise 4.35  </strong></span>Try the process again on the <span class="math inline">\(3\times 3\)</span> system of equations
<span class="math display">\[\begin{pmatrix}
        3 &amp; 6 &amp; 8\\
        2 &amp; 7 &amp; -1 \\
        5 &amp; 2 &amp; 2 
    \end{pmatrix} \begin{pmatrix} x_0 \\ x_1 \\ x_2 \end{pmatrix} = 
        \begin{pmatrix} -13 \\ 4 \\ 1 \end{pmatrix}\]</span> That is: Find
matrices <span class="math inline">\(L\)</span> and <span class="math inline">\(U\)</span> such that <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span> can be written as <span class="math inline">\(LU\boldsymbol{x} =  \boldsymbol{b}\)</span>. Then do two triangular solves to determine <span class="math inline">\(\boldsymbol{x}\)</span>.
</div>

<hr />
<p>Let’s take stock of what we have done so far.</p>
<ul>
<li>Solving lower triangular systems is super fast and easy!</li>
<li>Solving upper triangular systems is super fast and easy (so long as we never divide by zero).<br />
</li>
<li>It is often possible to rewrite the matrix <span class="math inline">\(A\)</span> as the product of a lower triangular matrix <span class="math inline">\(L\)</span> and an upper triangular matrix <span class="math inline">\(U\)</span> so <span class="math inline">\(A=LU\)</span>.</li>
<li>Now we can re-frame the equation <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span> as <span class="math inline">\(LU \boldsymbol{x} = \boldsymbol{b}\)</span>.<br />
</li>
<li>Substitute <span class="math inline">\(\boldsymbol{y} = U \boldsymbol{x}\)</span> so the system becomes <span class="math inline">\(L \boldsymbol{y} = \boldsymbol{b}\)</span>. Solve for <span class="math inline">\(\boldsymbol{y}\)</span> with forward substitution.</li>
<li>Now solve <span class="math inline">\(U \boldsymbol{x} = \boldsymbol{y}\)</span> using backward substitution.</li>
</ul>
<p>We have successfully take row reduction and turned into some fast matrix multiplications and then two very quick triangular solves. Ultimately this will be a faster algorithm for solving a system of linear equations.</p>
<hr />

<div class="definition">
<p><span id="def:unnamed-chunk-340" class="definition"><strong>Definition 4.7  (Solving Linear Systems with the LU Decomposition)  </strong></span>Let <span class="math inline">\(A\)</span> be a square matrix in <span class="math inline">\(\mathbb{R}^{n \times n}\)</span> and let
<span class="math inline">\(\boldsymbol{x}, \boldsymbol{b} \in \mathbb{R}^n\)</span>. To solve the problem <span class="math inline">\(A \boldsymbol{x} =\boldsymbol{b}\)</span>,</p>
<ol style="list-style-type: decimal">
<li><p>Factor <span class="math inline">\(A\)</span> into lower and upper triangular matrices <span class="math inline">\(A = LU\)</span>.<br />
<code>L, U = myLU(A)</code></p></li>
<li><p>The system can now be written as <span class="math inline">\(LU \boldsymbol{x} = \boldsymbol{b}\)</span>. Substitute <span class="math inline">\(U \boldsymbol{x} = \boldsymbol{y}\)</span> and solve the system <span class="math inline">\(L \boldsymbol{y} = \boldsymbol{b}\)</span> with forward substitution. 
<code>y = lsolve(L,b)</code></p></li>
<li><p>Finally, solve the system <span class="math inline">\(U \boldsymbol{x} = \boldsymbol{y}\)</span> with backward substitution.<br />
<code>x = usolve(U,y)</code></p></li>
</ol>
</div>

<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-341" class="exercise"><strong>Exercise 4.36  </strong></span>Test your <code>lsolve</code>, <code>usolve</code>, and <code>myLU</code> functions on a linear system for which you know the answer. Then test your problem on a system that you don’t know the solution to. As a way to compare your solutions you should:</p>
<ul>
<li>Find Python’s solution using <code>np.linalg.solve()</code> and compare your answer to that one using <code>np.linalg.norm()</code> to give the error between the two.</li>
<li>Time your code using the <code>time</code> library as follows
<ul>
<li>use the code <code>starttime = time.time()</code> before you start the main computation</li>
<li>use the code <code>endtime = time.time()</code> after the main computation</li>
<li>then calculate the total elapsed time with <code>totaltime = endtime - starttime</code></li>
</ul></li>
<li>Compare the timing of your <span class="math inline">\(LU\)</span> solve against <code>np.linalg.solve()</code> and against the RREF algorithm in the <code>sympy</code> library.</li>
</ul>
</div>

<div class="sourceCode" id="cb110"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb110-1"><a href="ch-linearalgebra.html#cb110-1"></a>A <span class="op">=</span> <span class="co"># Define your matrix</span></span>
<span id="cb110-2"><a href="ch-linearalgebra.html#cb110-2"></a>b <span class="op">=</span> <span class="co"># Defind your right-hand side vector</span></span>
<span id="cb110-3"><a href="ch-linearalgebra.html#cb110-3"></a></span>
<span id="cb110-4"><a href="ch-linearalgebra.html#cb110-4"></a><span class="co"># build a symbolic augmented matrix</span></span>
<span id="cb110-5"><a href="ch-linearalgebra.html#cb110-5"></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb110-6"><a href="ch-linearalgebra.html#cb110-6"></a>Ab <span class="op">=</span> sp.Matrix(np.c_[A,b]) </span>
<span id="cb110-7"><a href="ch-linearalgebra.html#cb110-7"></a><span class="co"># note that np.c_[A,b] does a column concatenation of A with b</span></span>
<span id="cb110-8"><a href="ch-linearalgebra.html#cb110-8"></a></span>
<span id="cb110-9"><a href="ch-linearalgebra.html#cb110-9"></a>t0 <span class="op">=</span> time.time()</span>
<span id="cb110-10"><a href="ch-linearalgebra.html#cb110-10"></a>Abrref <span class="op">=</span> <span class="co"># row reduce the symbolic augmented matrix</span></span>
<span id="cb110-11"><a href="ch-linearalgebra.html#cb110-11"></a>t1 <span class="op">=</span> time.time()</span>
<span id="cb110-12"><a href="ch-linearalgebra.html#cb110-12"></a>RREFTime <span class="op">=</span> t1<span class="op">-</span>t0</span>
<span id="cb110-13"><a href="ch-linearalgebra.html#cb110-13"></a></span>
<span id="cb110-14"><a href="ch-linearalgebra.html#cb110-14"></a></span>
<span id="cb110-15"><a href="ch-linearalgebra.html#cb110-15"></a>t0<span class="op">=</span>time.time()</span>
<span id="cb110-16"><a href="ch-linearalgebra.html#cb110-16"></a>exact <span class="op">=</span> <span class="co"># use np.linalg.solve() to solve the linear system</span></span>
<span id="cb110-17"><a href="ch-linearalgebra.html#cb110-17"></a>t1<span class="op">=</span>time.time()</span>
<span id="cb110-18"><a href="ch-linearalgebra.html#cb110-18"></a>exactTime <span class="op">=</span> t1<span class="op">-</span>t0</span>
<span id="cb110-19"><a href="ch-linearalgebra.html#cb110-19"></a></span>
<span id="cb110-20"><a href="ch-linearalgebra.html#cb110-20"></a>t0 <span class="op">=</span> time.time()</span>
<span id="cb110-21"><a href="ch-linearalgebra.html#cb110-21"></a>L, U <span class="op">=</span> <span class="co"># get L and U from your myLU</span></span>
<span id="cb110-22"><a href="ch-linearalgebra.html#cb110-22"></a>y <span class="op">=</span> <span class="co"># use forward substitution to get y</span></span>
<span id="cb110-23"><a href="ch-linearalgebra.html#cb110-23"></a>x <span class="op">=</span> <span class="co"># use bacckward substituation to get x</span></span>
<span id="cb110-24"><a href="ch-linearalgebra.html#cb110-24"></a>t1 <span class="op">=</span> time.time()</span>
<span id="cb110-25"><a href="ch-linearalgebra.html#cb110-25"></a>LUTime <span class="op">=</span> t1<span class="op">-</span>t0</span>
<span id="cb110-26"><a href="ch-linearalgebra.html#cb110-26"></a></span>
<span id="cb110-27"><a href="ch-linearalgebra.html#cb110-27"></a><span class="bu">print</span>(<span class="st">&quot;Time for symbolic RREF:</span><span class="ch">\t\t\t</span><span class="st">&quot;</span>,RREFTime)</span>
<span id="cb110-28"><a href="ch-linearalgebra.html#cb110-28"></a><span class="bu">print</span>(<span class="st">&quot;Time for np.linalg.solve() solution:</span><span class="ch">\t</span><span class="st">&quot;</span>,exactTime)</span>
<span id="cb110-29"><a href="ch-linearalgebra.html#cb110-29"></a><span class="bu">print</span>(<span class="st">&quot;Time for LU solution:</span><span class="ch">\t\t\t</span><span class="st">&quot;</span>,LUTime)</span>
<span id="cb110-30"><a href="ch-linearalgebra.html#cb110-30"></a>err <span class="op">=</span> np.linalg.norm(x<span class="op">-</span>exact)</span>
<span id="cb110-31"><a href="ch-linearalgebra.html#cb110-31"></a><span class="bu">print</span>(<span class="st">&quot;Error between LU and np.linalg.solve():&quot;</span>,err)</span></code></pre></div>
<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-343" class="exercise"><strong>Exercise 4.37  </strong></span>The <span class="math inline">\(LU\)</span> decomposition is not perfect. Discuss where the algorithm will fail.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-344" class="exercise"><strong>Exercise 4.38  </strong></span>What happens when you try to solve the system of equations
<span class="math display">\[ \begin{pmatrix} 0 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 0 \end{pmatrix} \begin{pmatrix} x_0 \\ x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} 7 \\ 9\\-3\end{pmatrix} \]</span>
with the <span class="math inline">\(LU\)</span> decomposition algorithm? Discuss.
</div>

<hr />
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="the-qr-factorization" class="section level2">
<h2><span class="header-section-number">4.5</span> The QR Factorization</h2>
<p>In this section we will try to find an improvement on the <span class="math inline">\(LU\)</span> factorization scheme from the previous section. What we’ll do here is leverage the geometry of the column space of the <span class="math inline">\(A\)</span> matrix instead of leveraging the row reduction process.</p>
<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-345" class="exercise"><strong>Exercise 4.39  </strong></span>We want to solve the system of equations
<span class="math display">\[ \begin{pmatrix} 1/3 &amp; 2/3 &amp; 2/3 \\ 2/3 &amp; 1/3 &amp; -2/3 \\ -2/3 &amp; 2/3 &amp; -1/3 \end{pmatrix} \begin{pmatrix} x_0 \\ x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} 6 \\ 12\\ -9 \end{pmatrix}. \]</span></p>
<ol style="list-style-type: lower-alpha">
<li>We could do row reduction by hand … yuck … don’t do this.</li>
<li>We could apply our new-found skills with the <span class="math inline">\(LU\)</span> decomposition to solve the system, so go ahead and do that with your Python code.</li>
<li>What do you get if you compute the product <span class="math inline">\(A^T A\)</span>?
<ol style="list-style-type: lower-roman">
<li>Why do you get what you get? In other words, what was special about <span class="math inline">\(A\)</span> that gave such an nice result?</li>
<li>What does this mean about the matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(A^T\)</span>?</li>
</ol></li>
<li>Now let’s leverage what we found in part (c) to solve the system of equations <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span> much faster. Multiply both sides of the matrix equation by <span class="math inline">\(A^T\)</span>, and now you should be able to just read off the solution. This seems amazing!!</li>
<li>What was it about this particular problem that made part (d) so elegant and easy?</li>
</ol>
</div>

<hr />

<div class="theorem">
<span id="thm:orthog-matrices" class="theorem"><strong>Theorem 4.1  (Orthonomal Matrices)  </strong></span>The previous exercise tells us something amazing: If <span class="math inline">\(A\)</span> is an orthonormal matrix where the columns are mutually orthogonal and every column is a unit vector, then <span class="math inline">\(A^T = A^{-1}\)</span> and to solve the system of equation <span class="math inline">\(A\boldsymbol{x} = \boldsymbol{b}\)</span> we simply need to multiply both sides of the equation by <span class="math inline">\(A^T\)</span>. Hence, the solution to <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span> is just <span class="math inline">\(\boldsymbol{x} = A^T \boldsymbol{b}\)</span> in this special case.
</div>

<hr />
<p>Theorem <a href="ch-linearalgebra.html#thm:orthog-matrices">4.1</a> begs an obvious question: <em>Is there a way to turn any matrix <span class="math inline">\(A\)</span> into an orthogonal matrix so that we can solve <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span> in this same very efficient and fast way?</em></p>
<p>The answer: Yes. Kind of.</p>
<p>In essence, if we can factor our coefficient matrix into an orthonormal matrix and some other nicely formatted matrix (like a triangular matrix, perhaps) then the job of solving the linear system of equations comes down to matrix multiplication and a quick triangular solve – both of which are extremely extremely fast!</p>
<p>What we will study in this section is a new matrix factorization called the <span class="math inline">\(QR\)</span> factorization who’s goal is to convert the matrix <span class="math inline">\(A\)</span> into a product of two matrices, <span class="math inline">\(Q\)</span> and <span class="math inline">\(R\)</span>, where <span class="math inline">\(Q\)</span> is orthonormal and <span class="math inline">\(R\)</span> is upper triangular.</p>
<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-346" class="exercise"><strong>Exercise 4.40  </strong></span>Let’s say that we have a matrix <span class="math inline">\(A\)</span> and we know that it can be factored into <span class="math inline">\(A = QR\)</span> where <span class="math inline">\(Q\)</span> is an orthonormal matrix and <span class="math inline">\(R\)</span> is an upper triangular matrix. How would we then leverage this factorization to solve the system of equation <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span> for <span class="math inline">\(\boldsymbol{x}\)</span>?
</div>

<hr />
<p>Before proceeding to the algorithm for the <span class="math inline">\(QR\)</span> factorization let’s pause for a moment and review scalar and vector projections from Linear Algebra. In Figure <a href="ch-linearalgebra.html#fig:projection2dimage">4.1</a> we see a graphical depiction of the vector <span class="math inline">\(\boldsymbol{u}\)</span> projected onto vector <span class="math inline">\(\boldsymbol{v}\)</span>. Notice that the projection is indeed the perpendicular projection as this is what seems natural geometrically.</p>
<p>The <strong>vector projection</strong> of <span class="math inline">\(\boldsymbol{u}\)</span> onto <span class="math inline">\(\boldsymbol{v}\)</span> is the vector <span class="math inline">\(c \boldsymbol{v}\)</span>. That is, the vector projection of <span class="math inline">\(\boldsymbol{u}\)</span> onto <span class="math inline">\(\boldsymbol{v}\)</span> is a scalar multiple of the vector <span class="math inline">\(\boldsymbol{v}\)</span>. The value of the scalar <span class="math inline">\(c\)</span> is called the <strong>scalar projection</strong> of <span class="math inline">\(\boldsymbol{u}\)</span> onto <span class="math inline">\(\boldsymbol{v}\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:projection2dimage"></span>
<img src="images/Ch04Projection.png" alt="Projection of one vector onto another." width="25%" />
<p class="caption">
Figure 4.1: Projection of one vector onto another.
</p>
</div>
<p>We can arrive at a formula for the scalar projection rather easily is we consider that the vector <span class="math inline">\(\boldsymbol{w}\)</span> in Figure <a href="ch-linearalgebra.html#fig:projection2dimage">4.1</a> must be perpendicular to <span class="math inline">\(c\boldsymbol{v}\)</span>. Hence
<span class="math display">\[ \boldsymbol{w} \cdot \left( c\boldsymbol{v} \right) = 0. \]</span>
From vector geometry we also know that <span class="math inline">\(\boldsymbol{w} = \boldsymbol{u}-c\boldsymbol{v}\)</span>. Therefore
<span class="math display">\[ \left( \boldsymbol{u} - c\boldsymbol{v} \right) \cdot \left( c \boldsymbol{v} \right) = 0. \]</span>
If we distribute we can see that
<span class="math display">\[ c \boldsymbol{u} \cdot \boldsymbol{v} - c^2 \boldsymbol{v} \cdot \boldsymbol{v} = 0 \]</span>
and therefore either <span class="math inline">\(c=0\)</span>, which is only true if <span class="math inline">\(\boldsymbol{u} \perp \boldsymbol{v}\)</span>, or
<span class="math display">\[ c = \frac{\boldsymbol{u} \cdot \boldsymbol{v}}{\boldsymbol{v} \cdot \boldsymbol{v}} = \frac{\boldsymbol{u} \cdot \boldsymbol{v}}{\|\boldsymbol{v} \|^2}. \]</span></p>
<p>Therefore,</p>
<ul>
<li>the <strong>scalar projection of <span class="math inline">\(\boldsymbol{u}\)</span> onto <span class="math inline">\(\boldsymbol{v}\)</span></strong> is
<span class="math display">\[ c = \frac{\boldsymbol{u} \cdot \boldsymbol{v}}{\|\boldsymbol{v} \|^2} \]</span></li>
<li>the <strong>vector projection of <span class="math inline">\(\boldsymbol{u}\)</span> onto <span class="math inline">\(\boldsymbol{v}\)</span></strong> is
<span class="math display">\[ c \boldsymbol{v} = \left( \frac{\boldsymbol{u} \cdot \boldsymbol{v}}{\|\boldsymbol{v} \|^2} \right) \boldsymbol{v} \]</span></li>
</ul>
<p>Another problem related to scalar and vector projections is to take a basis for the column space of a matrix and transform that basis into an orthogonal (or orthonormal) basis. Indeed, in Figure <a href="ch-linearalgebra.html#fig:projection2dimage">4.1</a> if we have the matrix
<span class="math display">\[A = \begin{pmatrix} | &amp; | \\ \boldsymbol{u} &amp; \boldsymbol{v} \\ | &amp; | \end{pmatrix}\]</span>
it should be clear from the picture that the columns of this matrix are not perpendicular. However, if we take the vector <span class="math inline">\(\boldsymbol{v}\)</span> and the vector <span class="math inline">\(\boldsymbol{w}\)</span> we do arrive at two orthogonal vector that form a basis for the same space. Moreover, if we normalize these vectors (by dividing by their respective lengths) then we can easily transform the original basis for the column space of <span class="math inline">\(A\)</span> into an orthonormal basis. This process is called the Gramm-Schmidt process, and you may have encountered it in your Linear Algebra class.</p>
<p>Now we return to our goal of finding a way to factor a matrix <span class="math inline">\(A\)</span> into an orthonormal matrix <span class="math inline">\(Q\)</span> and an upper triangular matrix <span class="math inline">\(R\)</span>. The algorithm that we are about to build depends greatly on the ideas of scalar and vector projections.</p>
<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-347" class="exercise"><strong>Exercise 4.41  </strong></span>We want to build a <span class="math inline">\(QR\)</span> factorization of the matrix <span class="math inline">\(A\)</span> in the matrix equation <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span> so that we can leverage the fact that solving the equation <span class="math inline">\(QR\boldsymbol{x} = \boldsymbol{b}\)</span> is easy. Consider the matrix <span class="math inline">\(A\)</span> defined as
<span class="math display">\[ A = \begin{pmatrix} 3 &amp; 1 \\ 4 &amp; 1 \end{pmatrix}. \]</span>
Notice that the columns of <span class="math inline">\(A\)</span> are NOT othonormal (they are not unit vectors and they are not perpendicular to each other).</p>
<ol style="list-style-type: lower-alpha">
<li>Draw a picture of the two column vectors of <span class="math inline">\(A\)</span> in <span class="math inline">\(\mathbb{R}^2\)</span>. We’ll use this picture to build geometric intuition for the rest of the <span class="math inline">\(QR\)</span> factorization process.</li>
<li>Define <span class="math inline">\(\boldsymbol{a}_0\)</span> as the first column of <span class="math inline">\(A\)</span> and <span class="math inline">\(\boldsymbol{a}_1\)</span> as the second column of <span class="math inline">\(A\)</span>. That is
<span class="math display">\[ \boldsymbol{a}_0 = \begin{pmatrix} 3\\4\end{pmatrix} \quad \text{and} \quad \boldsymbol{a}_1 = \begin{pmatrix} 1\\1\end{pmatrix}. \]</span>
Turn <span class="math inline">\(\boldsymbol{a}_0\)</span> into a unit vector and call this unit vector <span class="math inline">\(\boldsymbol{q}_0\)</span>
<span class="math display">\[ \boldsymbol{q}_0 = \frac{\boldsymbol{a}_0}{\|\boldsymbol{a}_0\|} = \begin{pmatrix} \underline{\hspace{0.5in}} \\ \underline{\hspace{0.5in}} \end{pmatrix}. \]</span>
This vector <span class="math inline">\(\boldsymbol{q}_0\)</span> will be the first column of the <span class="math inline">\(2 \times 2\)</span> matrix <span class="math inline">\(Q\)</span>. Why is this a nice place to start building the <span class="math inline">\(Q\)</span> matrix (think about the desired structure of <span class="math inline">\(Q\)</span>)?</li>
<li>In your picture of <span class="math inline">\(\boldsymbol{a}_0\)</span> and <span class="math inline">\(\boldsymbol{a}_1\)</span> mark where <span class="math inline">\(\boldsymbol{q}_0\)</span> is. Then draw the orthogonal projection from <span class="math inline">\(\boldsymbol{a}_1\)</span> onto <span class="math inline">\(\boldsymbol{q}_0\)</span>. In your picture you should now see a right triangle with <span class="math inline">\(\boldsymbol{a}_1\)</span> on the hypotenuse, the projection of <span class="math inline">\(\boldsymbol{a}_1\)</span> onto <span class="math inline">\(\boldsymbol{q}_0\)</span> on one leg, and the second leg is the vector difference of the hypotenuse and the first leg. Simplify the projection formula for leg 1 and write the formula for leg 2.<br />
<span class="math display">\[ \text{hypotenuse } = \boldsymbol{a}_1 \]</span>
<span class="math display">\[ \text{leg 1 } = \left( \frac{\boldsymbol{a}_1 \cdot \boldsymbol{q}_0}{\boldsymbol{q}_0 \cdot \boldsymbol{q}_0} \right) \boldsymbol{q}_0 = \underline{\hspace{1in}} \]</span>
<span class="math display">\[ \text{leg 2 } = \underline{\hspace{1in}} - \underline{\hspace{1in}}. \]</span></li>
<li>Compute the vector for leg 2 and then normalize it to turn it into a unit vector. Call this vector <span class="math inline">\(\boldsymbol{q}_1\)</span> and put it in the second column of <span class="math inline">\(Q\)</span>.<br />
</li>
<li>Verify that the columns of <span class="math inline">\(Q\)</span> are now orthogonal and are both unit vectors.</li>
<li>The matrix <span class="math inline">\(R\)</span> is supposed to complete the matrix factorization <span class="math inline">\(A = QR\)</span>. We have built <span class="math inline">\(Q\)</span> as an orthonormal matrix. How can we use this fact to solve for the matrix <span class="math inline">\(R\)</span>?</li>
<li>You should now have an orthonormal matrix <span class="math inline">\(Q\)</span> and an upper triangular matrix <span class="math inline">\(R\)</span>. Verify that <span class="math inline">\(A = QR\)</span>.</li>
<li>An alternate way to build the <span class="math inline">\(R\)</span> matrix is to observe that
<span class="math display">\[ R = \begin{pmatrix} \boldsymbol{a}_0 \cdot \boldsymbol{q}_0 &amp; \boldsymbol{a}_1 \cdot \boldsymbol{q}_0 \\ 0 &amp; \boldsymbol{a}_1 \cdot \boldsymbol{q}_1 \end{pmatrix}. \]</span>
Show that this is indeed true for the matrix <span class="math inline">\(A\)</span> from this problem.</li>
</ol>
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-348" class="exercise"><strong>Exercise 4.42  </strong></span>Keeping track of all of the arithmetic in the <span class="math inline">\(QR\)</span> factorization process is quite challenging, so let’s leverage Python to do some of the work for us. The following block of code walks through the previous exercise without any looping (that way we can see every step transparently). Some of the code is missing so you’ll need to fill it in.
</div>

<div class="sourceCode" id="cb111"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb111-1"><a href="ch-linearalgebra.html#cb111-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb111-2"><a href="ch-linearalgebra.html#cb111-2"></a><span class="co"># Define the matrix $A$</span></span>
<span id="cb111-3"><a href="ch-linearalgebra.html#cb111-3"></a>A <span class="op">=</span> np.matrix([[<span class="dv">3</span>,<span class="dv">1</span>],[<span class="dv">4</span>,<span class="dv">1</span>]])</span>
<span id="cb111-4"><a href="ch-linearalgebra.html#cb111-4"></a>n <span class="op">=</span> A.shape[<span class="dv">0</span>]</span>
<span id="cb111-5"><a href="ch-linearalgebra.html#cb111-5"></a><span class="co"># Build the vectors a0 and a1</span></span>
<span id="cb111-6"><a href="ch-linearalgebra.html#cb111-6"></a>a0 <span class="op">=</span> A[??? , ???] <span class="co"># ... write code to get column 0 from A</span></span>
<span id="cb111-7"><a href="ch-linearalgebra.html#cb111-7"></a>a1 <span class="op">=</span> A[??? , ???] <span class="co"># ... write code to get column 1 from A</span></span>
<span id="cb111-8"><a href="ch-linearalgebra.html#cb111-8"></a><span class="co"># Set up storage for Q</span></span>
<span id="cb111-9"><a href="ch-linearalgebra.html#cb111-9"></a>Q <span class="op">=</span> np.matrix( np.zeros( (n,n) ) )</span>
<span id="cb111-10"><a href="ch-linearalgebra.html#cb111-10"></a><span class="co"># build the vector q0 by normalizing a0</span></span>
<span id="cb111-11"><a href="ch-linearalgebra.html#cb111-11"></a>q0 <span class="op">=</span> a0 <span class="op">/</span> np.linalg.norm(a0)</span>
<span id="cb111-12"><a href="ch-linearalgebra.html#cb111-12"></a><span class="co"># Put q0 as the first column of Q</span></span>
<span id="cb111-13"><a href="ch-linearalgebra.html#cb111-13"></a>Q[:,<span class="dv">0</span>] <span class="op">=</span> q0</span>
<span id="cb111-14"><a href="ch-linearalgebra.html#cb111-14"></a><span class="co"># Calculate the lengths of the two legs of the triangle</span></span>
<span id="cb111-15"><a href="ch-linearalgebra.html#cb111-15"></a>leg1 <span class="op">=</span> <span class="co"># write code to get the vector for leg 1 of the triangle</span></span>
<span id="cb111-16"><a href="ch-linearalgebra.html#cb111-16"></a>leg2 <span class="op">=</span> <span class="co"># write code to get the vector for leg 2 of the triangle</span></span>
<span id="cb111-17"><a href="ch-linearalgebra.html#cb111-17"></a><span class="co"># normalize leg2 and call it q1</span></span>
<span id="cb111-18"><a href="ch-linearalgebra.html#cb111-18"></a>q1 <span class="op">=</span> <span class="co"># write code to normalize leg2</span></span>
<span id="cb111-19"><a href="ch-linearalgebra.html#cb111-19"></a>Q[:,<span class="dv">1</span>] <span class="op">=</span> q1 <span class="co"># What does this line do?</span></span>
<span id="cb111-20"><a href="ch-linearalgebra.html#cb111-20"></a>R <span class="op">=</span> <span class="co"># ... build the R matrix out of A and Q</span></span>
<span id="cb111-21"><a href="ch-linearalgebra.html#cb111-21"></a></span>
<span id="cb111-22"><a href="ch-linearalgebra.html#cb111-22"></a><span class="bu">print</span>(<span class="st">&quot;The Q matrix is </span><span class="ch">\n</span><span class="st">&quot;</span>,Q,<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb111-23"><a href="ch-linearalgebra.html#cb111-23"></a><span class="bu">print</span>(<span class="st">&quot;The R matrix is </span><span class="ch">\n</span><span class="st">&quot;</span>,R,<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb111-24"><a href="ch-linearalgebra.html#cb111-24"></a><span class="bu">print</span>(<span class="st">&quot;The A matrix is </span><span class="ch">\n</span><span class="st">&quot;</span>,A,<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb111-25"><a href="ch-linearalgebra.html#cb111-25"></a><span class="bu">print</span>(<span class="st">&quot;The product QR is</span><span class="ch">\n</span><span class="st">&quot;</span>,Q<span class="op">*</span>R)</span></code></pre></div>
<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-350" class="exercise"><strong>Exercise 4.43  </strong></span>You should notice that the code in the previous exercise does not depend on the specific matrix <span class="math inline">\(A\)</span> that we used? Put in a different <span class="math inline">\(2 \times 2\)</span> matrix and verify that the process still works. That is, verify that <span class="math inline">\(Q\)</span> is orthonormal, <span class="math inline">\(R\)</span> is upper triangular, and <span class="math inline">\(A = QR\)</span>. Be sure, however, that your matrix <span class="math inline">\(A\)</span> is full rank.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-351" class="exercise"><strong>Exercise 4.44  </strong></span>Draw two generic vectors in <span class="math inline">\(\mathbb{R}^2\)</span> and demonstrate the process outlined in the previous problem to build the vectors for the <span class="math inline">\(Q\)</span> matrix starting from your generic vectors.
</div>

<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-352" class="exercise"><strong>Exercise 4.45  </strong></span>Now we’ll extend the process from the previous exercises to three dimensions. This time we will seek a matrix <span class="math inline">\(Q\)</span> that has three othonormal vectors starting from the three original columns of a <span class="math inline">\(3 \times 3\)</span> matrix <span class="math inline">\(A\)</span>. Perform each of the following steps <strong>by hand</strong> on the matrix
<span class="math display">\[ A = \begin{pmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 1 \end{pmatrix}. \]</span>
In the end you should end up with an orthonormal matrix <span class="math inline">\(Q\)</span> and an upper triangular matrix <span class="math inline">\(R\)</span>.</p>
<ul>
<li><strong>Step 1</strong>: Pick column <span class="math inline">\(\boldsymbol{a}_0\)</span> from the matrix <span class="math inline">\(A\)</span> and normalize it. Call this new vector <span class="math inline">\(\boldsymbol{q}_0\)</span> and make that the first column of the matrix <span class="math inline">\(Q\)</span>.</li>
<li><strong>Step 2</strong>: Project column <span class="math inline">\(\boldsymbol{a}_1\)</span> of <span class="math inline">\(A\)</span> onto <span class="math inline">\(\boldsymbol{q}_0\)</span>. This forms a right triangle with <span class="math inline">\(\boldsymbol{a}_1\)</span> as the hypotenuse, the projection of <span class="math inline">\(\boldsymbol{a}_1\)</span> onto <span class="math inline">\(\boldsymbol{q}_0\)</span> as one of the legs, and the vector difference between these two as the second leg. Notice that the second leg of the newly formed right triangle is perpendicular to <span class="math inline">\(\boldsymbol{q}_0\)</span> by design. If we normalize this vector then we have the second column of <span class="math inline">\(Q\)</span>, <span class="math inline">\(\boldsymbol{q}_1\)</span>.</li>
<li><strong>Step 3:</strong> Now we need a vector that is perpendicular to both <span class="math inline">\(\boldsymbol{q}_0\)</span> AND <span class="math inline">\(\boldsymbol{q}_1\)</span>. To achieve this we are going to project column <span class="math inline">\(\boldsymbol{a}_2\)</span> from <span class="math inline">\(A\)</span> onto the plane formed by <span class="math inline">\(\boldsymbol{q}_0\)</span> and <span class="math inline">\(\boldsymbol{q}_1\)</span>. We’ll do this in two steps:
<ul>
<li><strong>Step 3a</strong>: We first project <span class="math inline">\(\boldsymbol{a}_2\)</span> down onto both <span class="math inline">\(\boldsymbol{q}_0\)</span> and <span class="math inline">\(\boldsymbol{q}_1\)</span>.<br />
</li>
<li><strong>Step 3b</strong>: The vector that is perpendicular to both <span class="math inline">\(\boldsymbol{q}_0\)</span> and <span class="math inline">\(\boldsymbol{q}_1\)</span> will be the difference between <span class="math inline">\(\boldsymbol{a}_2\)</span> the projection of <span class="math inline">\(\boldsymbol{a}_2\)</span> onto <span class="math inline">\(\boldsymbol{q}_0\)</span> and the projection of <span class="math inline">\(\boldsymbol{a}_2\)</span> onto <span class="math inline">\(\boldsymbol{q}_1\)</span>. That is, we form the vector <span class="math inline">\(\boldsymbol{w} = \boldsymbol{a}_2 - (\boldsymbol{a}_2 \cdot \boldsymbol{q}_0 ) \boldsymbol{q}_0 - (\boldsymbol{a}_2 \cdot \boldsymbol{q}_1) \boldsymbol{q}_1.\)</span> Normalizing this vector will give us <span class="math inline">\(\boldsymbol{q}_2\)</span>. (Stop now and prove that <span class="math inline">\(\boldsymbol{q}_2\)</span> is indeed perpendicular to both <span class="math inline">\(\boldsymbol{q}_1\)</span> and <span class="math inline">\(\boldsymbol{q}_0\)</span>.)</li>
</ul></li>
</ul>
<p>The result should be the matrix <span class="math inline">\(Q\)</span> which contains orthonormal columns. To build the matrix <span class="math inline">\(R\)</span> we simply recall that <span class="math inline">\(A = QR\)</span> and <span class="math inline">\(Q^{-1} = Q^T\)</span> so <span class="math inline">\(R = Q^T A\)</span>.</p>
</div>

<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-353" class="exercise"><strong>Exercise 4.46  </strong></span>Repeat the previous exercise but write code for each step so that Python can handle all of the computations. Again use the matrix
<span class="math display">\[ A = \begin{pmatrix} 1 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 1 \end{pmatrix}. \]</span></p>
</div>

<hr />

<div class="example">
<p><span id="exm:unnamed-chunk-354" class="example"><strong>Example 4.7  (QR for <span class="math inline">\(n=3\)</span>)  </strong></span>For the sake of clarity let’s now write down the full <span class="math inline">\(QR\)</span> factorization for a <span class="math inline">\(3 \times 3\)</span> matrix.</p>
<p>If the columns of <span class="math inline">\(A\)</span> are <span class="math inline">\(\boldsymbol{a}_0\)</span>, <span class="math inline">\(\boldsymbol{a}_1\)</span>, and <span class="math inline">\(\boldsymbol{a}_2\)</span> then
<span class="math display">\[ \boldsymbol{q}_0 = \frac{\boldsymbol{a}_0}{\|\boldsymbol{a}_0\|} \]</span>
<span class="math display">\[ \boldsymbol{q}_1 = \frac{ \boldsymbol{a}_1 - \left( \boldsymbol{a}_1 \cdot \boldsymbol{q}_0 \right) \boldsymbol{q}_0 }{\| \boldsymbol{a}_1 - \left( \boldsymbol{a}_1 \cdot \boldsymbol{q}_0 \right) \boldsymbol{q}_0 \|} \]</span>
<span class="math display">\[ \boldsymbol{q}_2 = \frac{ \boldsymbol{a}_2 - \left( \boldsymbol{a}_2 \cdot \boldsymbol{q}_0 \right) \boldsymbol{q}_0 - \left( \boldsymbol{a}_2 \cdot \boldsymbol{q}_1 \right) \boldsymbol{q}_1}{\| \boldsymbol{a}_2 - \left( \boldsymbol{a}_2 \cdot \boldsymbol{q}_0 \right) \boldsymbol{q}_0  - \left( \boldsymbol{a}_2 \cdot \boldsymbol{q}_1 \right) \boldsymbol{q}_1 \|} \]</span></p>
<p>and
<span class="math display">\[ R = \begin{pmatrix} \boldsymbol{a}_0 \cdot \boldsymbol{q}_0 &amp; \boldsymbol{a}_1 \cdot \boldsymbol{q}_0 &amp; \boldsymbol{a}_2 \cdot \boldsymbol{q}_0 \\ 
0 &amp; \boldsymbol{a}_1 \cdot \boldsymbol{q}_1 &amp; \boldsymbol{a}_2 \cdot \boldsymbol{q}_1 \\ 0 &amp; 0 &amp; \boldsymbol{a}_2 \cdot \boldsymbol{q}_2 \end{pmatrix} \]</span></p>
</div>

<hr />

<div class="exercise">
<span id="exr:qr" class="exercise"><strong>Exercise 4.47  (The QR Factorization)  </strong></span>Now we’re ready to build general code for the <span class="math inline">\(QR\)</span> factorization. The following Python function definition is partially complete. Fill in the missing pieces of code and then test your code on square matrices of many different sizes. The easiest way to check if you have an error is to find the normed difference between <span class="math inline">\(A\)</span> and <span class="math inline">\(QR\)</span> with <code>np.linalg.norm(A - Q*R)</code>.
</div>

<div class="sourceCode" id="cb112"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb112-1"><a href="ch-linearalgebra.html#cb112-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb112-2"><a href="ch-linearalgebra.html#cb112-2"></a><span class="kw">def</span> myQR(A):</span>
<span id="cb112-3"><a href="ch-linearalgebra.html#cb112-3"></a>    n <span class="op">=</span> A.shape[<span class="dv">0</span>]</span>
<span id="cb112-4"><a href="ch-linearalgebra.html#cb112-4"></a>    Q <span class="op">=</span> np.matrix( np.zeros( (n,n) ) )</span>
<span id="cb112-5"><a href="ch-linearalgebra.html#cb112-5"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>( ??? ): <span class="co"># The outer loop goes over the columns</span></span>
<span id="cb112-6"><a href="ch-linearalgebra.html#cb112-6"></a>        q <span class="op">=</span> A[:,j]</span>
<span id="cb112-7"><a href="ch-linearalgebra.html#cb112-7"></a>        <span class="co"># The next loop is meant to do all of the projections.</span></span>
<span id="cb112-8"><a href="ch-linearalgebra.html#cb112-8"></a>        <span class="co"># When do you start the inner loop and how far do you go?</span></span>
<span id="cb112-9"><a href="ch-linearalgebra.html#cb112-9"></a>        <span class="co"># Hint: You don&#39;t need to enter this loop the first time </span></span>
<span id="cb112-10"><a href="ch-linearalgebra.html#cb112-10"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>( ??? ): </span>
<span id="cb112-11"><a href="ch-linearalgebra.html#cb112-11"></a>            length_of_leg <span class="op">=</span> np.<span class="bu">sum</span>(A[:,j].T <span class="op">*</span> Q[:,i])</span>
<span id="cb112-12"><a href="ch-linearalgebra.html#cb112-12"></a>            q <span class="op">=</span> q <span class="op">-</span>  ??? <span class="op">*</span> ??? <span class="co"># This is where we do projections</span></span>
<span id="cb112-13"><a href="ch-linearalgebra.html#cb112-13"></a>        Q[:,j] <span class="op">=</span> q <span class="op">/</span> np.linalg.norm(q)</span>
<span id="cb112-14"><a href="ch-linearalgebra.html#cb112-14"></a>    R <span class="op">=</span> <span class="co"># finally build the R matrix</span></span>
<span id="cb112-15"><a href="ch-linearalgebra.html#cb112-15"></a>    <span class="cf">return</span> Q, R</span></code></pre></div>
<div class="sourceCode" id="cb113"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb113-1"><a href="ch-linearalgebra.html#cb113-1"></a><span class="co"># Test Code</span></span>
<span id="cb113-2"><a href="ch-linearalgebra.html#cb113-2"></a>A <span class="op">=</span> np.matrix( ... ) </span>
<span id="cb113-3"><a href="ch-linearalgebra.html#cb113-3"></a><span class="co"># or you can build A with use np.random.randn() </span></span>
<span id="cb113-4"><a href="ch-linearalgebra.html#cb113-4"></a><span class="co"># Often time random matrices are good test cases</span></span>
<span id="cb113-5"><a href="ch-linearalgebra.html#cb113-5"></a>Q, R <span class="op">=</span> myQR(A)</span>
<span id="cb113-6"><a href="ch-linearalgebra.html#cb113-6"></a>error <span class="op">=</span> np.linalg.norm(A <span class="op">-</span> Q<span class="op">*</span>R)</span>
<span id="cb113-7"><a href="ch-linearalgebra.html#cb113-7"></a><span class="bu">print</span>(error)</span></code></pre></div>
<hr />
<p>We now have a robust algorithm for doing <span class="math inline">\(QR\)</span> factorization of square matrices we can finally return to solving systems of equations.</p>

<div class="theorem">
<span id="thm:unnamed-chunk-357" class="theorem"><strong>Theorem 4.2  (Solving Systems with <span class="math inline">\(QR\)</span>)  </strong></span>Remember that we want to solve <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span> and since <span class="math inline">\(A = QR\)</span> we can rewrite it with <span class="math inline">\(QR \boldsymbol{x} = \boldsymbol{b}\)</span>. Since we know that <span class="math inline">\(Q\)</span> is orthonormal by design we can multiply both sides of the equation by <span class="math inline">\(Q^T\)</span> to get <span class="math inline">\(R \boldsymbol{x} = Q^T \boldsymbol{b}\)</span>. Finally, since <span class="math inline">\(R\)</span> is upper triangular we can use our <code>usolve</code> code from the previous section to solve the resulting triangular system.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-358" class="exercise"><strong>Exercise 4.48  </strong></span>Solve the system of equations
<span class="math display">\[ \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 0 \end{pmatrix} \begin{pmatrix} x_0 \\ x_1 \\ x_2 \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \\ 2 \end{pmatrix} \]</span>
by first computing the <span class="math inline">\(QR\)</span> factorization of <span class="math inline">\(A\)</span> and then solving the resulting upper triangular system.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-359" class="exercise"><strong>Exercise 4.49  </strong></span>Write code that builds a random <span class="math inline">\(n \times n\)</span> matrix and a random <span class="math inline">\(n \times 1\)</span> vector. Solve the equation <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span> using the <span class="math inline">\(QR\)</span> factorization and compare the answer to what we find from <code>np.linalg.solve()</code>. Do this many times for various values of <span class="math inline">\(n\)</span> and create a plot with <span class="math inline">\(n\)</span> on the horizontal axis and the normed error between Python’s answer and your answer from the <span class="math inline">\(QR\)</span> algorithm on the vertical axis. It would be wise to use a <code>plt.semilogy()</code> plot. To find the normed difference you should use <code>np.linalg.norm()</code>. What do you notice?
</div>

<hr />
<div style="page-break-after: always;"></div>
</div>
<div id="over-determined-systems-and-curve-fitting" class="section level2">
<h2><span class="header-section-number">4.6</span> Over Determined Systems and Curve Fitting</h2>

<div class="exercise">
<p><span id="exr:unnamed-chunk-360" class="exercise"><strong>Exercise 4.50  </strong></span>In Exercise <a href="ch-calculus.html#exr:first-least-squares">3.81</a> we considered finding the quadratic function <span class="math inline">\(f(x) = ax^2 + bx + c\)</span> that <em>best fits</em> the points
<span class="math display">\[ (0, 1.07), (1, 3.9), (2, 14.8), (3, 26.8). \]</span>
Back in Exercise <a href="ch-calculus.html#exr:first-least-squares">3.81</a> and the subsequent problems we approached this problem using an optimization tool in Python. You might be surprised to learn that there is a way to do this same optimization with linear algebra!!</p>
<p>We don’t know the values of <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, or <span class="math inline">\(c\)</span> but we do have four different <span class="math inline">\((x,y)\)</span> ordered pairs. Hence, we have four equations:
<span class="math display">\[ 1.07 = a(0)^2 + b(0) + c \]</span>
<span class="math display">\[ 3.9 = a(1)^2 + b(1) + c  \]</span>
<span class="math display">\[ 14.8 = a(2)^2 + b(2) + c \]</span>
<span class="math display">\[ 26.8 = a(3)^2 + b(3) + c. \]</span></p>
<p>There are four equations and only three unknowns. This is what is called an <strong>over determined systems</strong> – when there are more equations than unknowns. Let’s play with this problem.</p>
<ol style="list-style-type: lower-alpha">
<li>First turn the system of equations into a matrix equation.
<span class="math display">\[ \begin{pmatrix} 0 &amp; 0 &amp; 1 \\ \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} \\ \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} \\ \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} &amp; \underline{\hspace{0.25in}} \end{pmatrix} \begin{pmatrix} a \\ b \\ c \end{pmatrix} = \begin{pmatrix} 1.07 \\ 3.9 \\ 14.8 \\ 26.8 \end{pmatrix}. \]</span></li>
<li>None of our techniques for solving systems will likely work here since it is highly unlikely that the vector on the right-hand side of the equation is in the column space of the coefficient matrix. Discuss this.</li>
<li>One solution to the unfortunate fact from part (b) is that we can project the vector on the right-hand side into the subspace spanned by the columns of the coefficient matrix. Think of this as casting the shadow of the right-hand vector down onto the space spanned by the columns. If we do this projection we will be able to solve the equation for the values of <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, and <span class="math inline">\(c\)</span> that will create the projection exactly – and hence be as close as we can get to the actual right-hand side. Draw a picture of what we’ve said here.</li>
<li>Now we need to project the right-hand side, call it <span class="math inline">\(\boldsymbol{b}\)</span>, onto the column space of the the coefficient matrix <span class="math inline">\(A\)</span>. Recall the following facts:
<ul>
<li>Projections are dot products</li>
<li>Matrix multiplication is nothing but a bunch of dot products.</li>
<li>The projections of <span class="math inline">\(\boldsymbol{b}\)</span> onto the columns of <span class="math inline">\(A\)</span> are the dot products of <span class="math inline">\(\boldsymbol{b}\)</span> with each of the columns of <span class="math inline">\(A\)</span>.</li>
<li>What matrix can we multiply both sides of the equation <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span> by in order for the right-hand side to become the projection that we want? (Now do the projection in Python)</li>
</ul></li>
<li>If you have done part (d) correctly then you should now have a square system (i.e. the matrix on the left-hand side should now be square). Solve this system for <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, and <span class="math inline">\(c\)</span>. Compare your answers to what you found way back in Exercise <a href="ch-calculus.html#exr:first-least-squares">3.81</a>.</li>
</ol>
</div>

<hr />

<div class="theorem">
<p><span id="thm:overdetermined" class="theorem"><strong>Theorem 4.3  (Solving Overdetermined Systems)  </strong></span>If <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span> is an overdetermined system (i.e. <span class="math inline">\(A\)</span> has more rows than columns) then we first multiply both sides of the equation by <span class="math inline">\(A^T\)</span> (why do we do this?) and then solve the square system of equations <span class="math inline">\((A^T A) \boldsymbol{x} = A^T \boldsymbol{b}\)</span> using a system solving like <span class="math inline">\(LU\)</span> or <span class="math inline">\(QR\)</span>. The answer to this new system is interpreted as the vector <span class="math inline">\(\boldsymbol{x}\)</span> which solves exactly for the projection of <span class="math inline">\(\boldsymbol{b}\)</span> onto the column space of <span class="math inline">\(A\)</span>.</p>
The equation <span class="math inline">\((A^T A) \boldsymbol{x} = A^T \boldsymbol{b}\)</span> is called <strong>the normal equations</strong> and arises often in Statistics and Machine Learning.
</div>

<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-361" class="exercise"><strong>Exercise 4.51  </strong></span>Fit a linear function to the following data. Solve for the slope and intercept using the technique outlined in Theorem <a href="ch-linearalgebra.html#thm:overdetermined">4.3</a>. Make a plot of the points along with your best fit curve.</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(x\)</span></th>
<th><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>4.6</td>
</tr>
<tr class="even">
<td>1</td>
<td>11</td>
</tr>
<tr class="odd">
<td>2</td>
<td>12</td>
</tr>
<tr class="even">
<td>3</td>
<td>19.1</td>
</tr>
<tr class="odd">
<td>4</td>
<td>18.8</td>
</tr>
<tr class="even">
<td>5</td>
<td>39.5</td>
</tr>
<tr class="odd">
<td>6</td>
<td>31.1</td>
</tr>
<tr class="even">
<td>7</td>
<td>43.4</td>
</tr>
<tr class="odd">
<td>8</td>
<td>40.3</td>
</tr>
<tr class="even">
<td>9</td>
<td>41.5</td>
</tr>
<tr class="odd">
<td>10</td>
<td>41.6</td>
</tr>
</tbody>
</table>
Code to download the data directly is given below.
</div>

<div class="sourceCode" id="cb114"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb114-1"><a href="ch-linearalgebra.html#cb114-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb114-2"><a href="ch-linearalgebra.html#cb114-2"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb114-3"><a href="ch-linearalgebra.html#cb114-3"></a>URL1 <span class="op">=</span> <span class="st">&#39;https://raw.githubusercontent.com/NumericalMethodsSullivan&#39;</span></span>
<span id="cb114-4"><a href="ch-linearalgebra.html#cb114-4"></a>URL2 <span class="op">=</span> <span class="st">&#39;/NumericalMethodsSullivan.github.io/master/data/&#39;</span></span>
<span id="cb114-5"><a href="ch-linearalgebra.html#cb114-5"></a>URL <span class="op">=</span> URL1<span class="op">+</span>URL2</span>
<span id="cb114-6"><a href="ch-linearalgebra.html#cb114-6"></a>data <span class="op">=</span> np.array( pd.read_csv(URL<span class="op">+</span><span class="st">&#39;Exercise4_51.csv&#39;</span>) )</span>
<span id="cb114-7"><a href="ch-linearalgebra.html#cb114-7"></a><span class="co"># Exercise4_51.csv</span></span></code></pre></div>
<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-363" class="exercise"><strong>Exercise 4.52  </strong></span>Fit a quadratic function to the following data using the technique outlined in Theorem <a href="ch-linearalgebra.html#thm:overdetermined">4.3</a>. Make a plot of the points along with your best fit curve.</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(x\)</span></th>
<th><span class="math inline">\(y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>-6.8</td>
</tr>
<tr class="even">
<td>1</td>
<td>11.8</td>
</tr>
<tr class="odd">
<td>2</td>
<td>50.6</td>
</tr>
<tr class="even">
<td>3</td>
<td>94</td>
</tr>
<tr class="odd">
<td>4</td>
<td>224.3</td>
</tr>
<tr class="even">
<td>5</td>
<td>301.7</td>
</tr>
<tr class="odd">
<td>6</td>
<td>499.2</td>
</tr>
<tr class="even">
<td>7</td>
<td>454.7</td>
</tr>
<tr class="odd">
<td>8</td>
<td>578.5</td>
</tr>
<tr class="even">
<td>9</td>
<td>1102</td>
</tr>
<tr class="odd">
<td>10</td>
<td>1203.2</td>
</tr>
</tbody>
</table>
Code to download the data directly is given below.
</div>

<div class="sourceCode" id="cb115"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb115-1"><a href="ch-linearalgebra.html#cb115-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb115-2"><a href="ch-linearalgebra.html#cb115-2"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb115-3"><a href="ch-linearalgebra.html#cb115-3"></a>URL1 <span class="op">=</span> <span class="st">&#39;https://raw.githubusercontent.com/NumericalMethodsSullivan&#39;</span></span>
<span id="cb115-4"><a href="ch-linearalgebra.html#cb115-4"></a>URL2 <span class="op">=</span> <span class="st">&#39;/NumericalMethodsSullivan.github.io/master/data/&#39;</span></span>
<span id="cb115-5"><a href="ch-linearalgebra.html#cb115-5"></a>URL <span class="op">=</span> URL1<span class="op">+</span>URL2</span>
<span id="cb115-6"><a href="ch-linearalgebra.html#cb115-6"></a>data <span class="op">=</span> np.array( pd.read_csv(URL<span class="op">+</span><span class="st">&#39;Exercise4_52.csv&#39;</span>) )</span>
<span id="cb115-7"><a href="ch-linearalgebra.html#cb115-7"></a><span class="co"># Exercise4_52.csv</span></span></code></pre></div>
<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-365" class="exercise"><strong>Exercise 4.53  </strong></span>The Statistical technique of curve fitting is often called “linear regression”. This even holds when we are fitting quadratic functions, cubic functions, etc to the data … we still call that linear regression! Why?
</div>

<hr />
<p>This section of the text on solving over determined systems is just a bit of a teaser for a bit of higher-level statistics, data science, and machine learning. The normal equations and solving systems via projections is the starting point of many modern machine learning algorithms. For more information on this sort of problem look into taking some statistics, data science, and/or machine learning courses. You’ll love it!</p>
<hr />
<div style="page-break-after: always;"></div>
</div>
<div id="the-eigenvalue-eigenvector-problem" class="section level2">
<h2><span class="header-section-number">4.7</span> The Eigenvalue-Eigenvector Problem</h2>
<p>We finally turn our attention to the last major topic in numerical linear algebra in this course.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></p>

<div class="definition">
<span id="def:eig" class="definition"><strong>Definition 4.8  (The Eigenvalue Problem)  </strong></span>Recall that the eigenvectors, <span class="math inline">\(\boldsymbol{x}\)</span>, and the eigenvalues, <span class="math inline">\(\lambda\)</span> of a
square matrix satisfy the equation <span class="math inline">\(A\boldsymbol{x}=\lambda \boldsymbol{x}\)</span>. Geometrically, the eign-problem is the task of finding the special vectors <span class="math inline">\(\boldsymbol{x}\)</span> such that multiplication by the matrix <span class="math inline">\(A\)</span> only produces a scalar multiple of <span class="math inline">\(\boldsymbol{x}\)</span>.
</div>

<hr />
<p>Thinking about matrix multiplication, the geometric notion of the eigenvalue problem is rather peculiar since matrix-vector multiplication usually results in a scaling and a
rotation of the vector <span class="math inline">\(\boldsymbol{x}\)</span>. Therefore, in some sense the eigenvectors are
the only special vectors which avoid geometric rotation under matrix
multiplication. For a graphical exploration of this idea see:<br />
<a href="https://www.geogebra.org/m/JP2XZpzV" class="uri">https://www.geogebra.org/m/JP2XZpzV</a>.</p>
<hr />

<div class="theorem">
<p><span id="thm:unnamed-chunk-366" class="theorem"><strong>Theorem 4.4  </strong></span>Recall that to solve the eigen-problem for a square matrix <span class="math inline">\(A\)</span> we complete the following steps:</p>
<ol style="list-style-type: lower-alpha">
<li>First rearrange the definition of the eigenvalue-eigenvector pair to
<span class="math display">\[(A\boldsymbol{x}-\lambda \boldsymbol{x})=\boldsymbol{0}.\]</span></li>
<li>Next, factor the <span class="math inline">\(\boldsymbol{x}\)</span> on the right to get
<span class="math display">\[(A-\lambda I) \boldsymbol{x}=\boldsymbol{0}.\]</span></li>
<li>Now observe that since <span class="math inline">\(\boldsymbol{x} \ne 0\)</span> the matrix <span class="math inline">\(A-\lambda I\)</span> must NOT have an inverse. Therefore, <span class="math display">\[\det(A-\lambda I)=0.\]</span></li>
<li>Solve the equation <span class="math inline">\(\det(A-\lambda I)=0\)</span> for all of the values of <span class="math inline">\(\lambda\)</span>.</li>
<li>For each <span class="math inline">\(\lambda\)</span>, find a solution to the equation
<span class="math inline">\((A-\lambda I) \boldsymbol{x}=\boldsymbol{0}\)</span>. Note that there will be infinitely many solutions so you will need to make wise choices for the free variables.</li>
</ol>
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-367" class="exercise"><strong>Exercise 4.54  </strong></span>Find the eigenvalues and eigenvectors of
<span class="math display">\[ A = \begin{pmatrix} 1 &amp; 2 \\ 4 &amp; 3 \end{pmatrix}. \]</span>
</div>

<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-368" class="exercise"><strong>Exercise 4.55  </strong></span>In the matrix
<span class="math display">\[ A = \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end{pmatrix} \]</span>
one of the eigenvalues is <span class="math inline">\(\lambda_1 = 0\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>What does that tell us about the matrix <span class="math inline">\(A\)</span>?</li>
<li>What is the eigenvector <span class="math inline">\(\boldsymbol{v}_1\)</span> associated with <span class="math inline">\(\lambda_1 = 0\)</span>?</li>
<li>What is the null space of the matrix <span class="math inline">\(A\)</span>?</li>
</ol>
</div>

<hr />
<p>OK. Now that you recall some of the basics let’s play with a little limit problem. The following exercises are going to work us toward the <strong>power method</strong> for finding certain eigen-structure of a matrix.</p>
<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-369" class="exercise"><strong>Exercise 4.56  </strong></span>Consider the matrix
<span class="math display">\[ A = \begin{pmatrix} 8 &amp; 5 &amp; -6 \\ -12 &amp; -9 &amp; 12 \\ -3 &amp; -3 &amp; 5 \end{pmatrix}. \]</span>
This matrix has the following eigen-structure:
<span class="math display">\[ \boldsymbol{v}_1 = \begin{pmatrix} 1\\-1\\0\end{pmatrix} \quad \text{with} \quad \lambda_1 = 3 \]</span>
<span class="math display">\[ \boldsymbol{v}_2 = \begin{pmatrix} 2\\0\\2\end{pmatrix} \quad \text{with} \quad \lambda_2 = 2 \]</span>
<span class="math display">\[ \boldsymbol{v}_3 = \begin{pmatrix} -1\\3\\1\end{pmatrix} \quad \text{with} \quad \lambda_3 = -1 \]</span></p>
<p>If we have
<span class="math display">\[ \boldsymbol{x} = -2 \boldsymbol{v}_1 + 1 \boldsymbol{v}_2 - 3 \boldsymbol{v}_3 = \begin{pmatrix} 3 \\ -7 \\ -1 \end{pmatrix} \]</span>
then we want to do a bit of an experiment. What happens when we iteratively multiply <span class="math inline">\(\boldsymbol{x}\)</span> by <span class="math inline">\(A\)</span> but at the same time divide by the largest eigenvalue. Let’s see:</p>
<ul>
<li>What is <span class="math inline">\(A^1 \boldsymbol{x} / 3^1\)</span>?</li>
<li>What is <span class="math inline">\(A^2 \boldsymbol{x} / 3^2\)</span>?</li>
<li>What is <span class="math inline">\(A^3 \boldsymbol{x} / 3^3\)</span>?</li>
<li>What is <span class="math inline">\(A^4 \boldsymbol{x} / 3^4\)</span>?</li>
<li>…</li>
</ul>
<p>It might be nice now to go to some Python code to do the computations (if you haven’t already). Use your code to conjecture about the following limit.
<span class="math display">\[ \lim_{k \to \infty} \frac{A^k \boldsymbol{x}}{\lambda_{max}^k} = ???. \]</span>
In this limit we are really interested in the direction of the resulting vector, not the magnitude. Therefore, in the code below you will see that we normalize the resulting vector so that it is a unit vector.</p>
Note: be careful, computers don’t do infinity, so for powers that are too large you won’t get any results.
</div>

<div class="sourceCode" id="cb116"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb116-1"><a href="ch-linearalgebra.html#cb116-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb116-2"><a href="ch-linearalgebra.html#cb116-2"></a>A <span class="op">=</span> np.matrix([[<span class="dv">8</span>,<span class="dv">5</span>,<span class="op">-</span><span class="dv">6</span>],[<span class="op">-</span><span class="dv">12</span>,<span class="op">-</span><span class="dv">9</span>,<span class="dv">12</span>],[<span class="op">-</span><span class="dv">3</span>,<span class="op">-</span><span class="dv">3</span>,<span class="dv">5</span>]])</span>
<span id="cb116-3"><a href="ch-linearalgebra.html#cb116-3"></a>x <span class="op">=</span> np.matrix([[<span class="dv">3</span>],[<span class="op">-</span><span class="dv">7</span>],[<span class="op">-</span><span class="dv">1</span>]])</span>
<span id="cb116-4"><a href="ch-linearalgebra.html#cb116-4"></a>eigval_max <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb116-5"><a href="ch-linearalgebra.html#cb116-5"></a></span>
<span id="cb116-6"><a href="ch-linearalgebra.html#cb116-6"></a>k <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb116-7"><a href="ch-linearalgebra.html#cb116-7"></a>result <span class="op">=</span> A<span class="op">**</span>k <span class="op">*</span> x <span class="op">/</span> eigval_max<span class="op">**</span>k</span>
<span id="cb116-8"><a href="ch-linearalgebra.html#cb116-8"></a><span class="bu">print</span>(result <span class="op">/</span> np.linalg.norm(result) )</span></code></pre></div>
<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-371" class="exercise"><strong>Exercise 4.57  </strong></span>If a matrix <span class="math inline">\(A\)</span> has eigenvectors <span class="math inline">\(\boldsymbol{v}_1\)</span>, <span class="math inline">\(\boldsymbol{v}_2\)</span>, <span class="math inline">\(\boldsymbol{v}_3\)</span>, <span class="math inline">\(\cdots,\)</span> <span class="math inline">\(\boldsymbol{v}_n\)</span> with eigenvalues <span class="math inline">\(\lambda_1, \lambda_2, \lambda_3, \ldots, \lambda_n\)</span> and <span class="math inline">\(\boldsymbol{x}\)</span> is in the column space of <span class="math inline">\(A\)</span> then what will we get, approximately, if we evaluate <span class="math inline">\(A^k \boldsymbol{x} / \max_{j}(\lambda_j)^k\)</span> for very large values of <span class="math inline">\(k\)</span>?</p>
Discuss your conjecture with your peers. Then try to verify it with several numerical examples.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-372" class="exercise"><strong>Exercise 4.58  </strong></span>Explain your result from the previous exercise geometrically.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-373" class="exercise"><strong>Exercise 4.59  </strong></span>The algorithm that we’ve been toying with will find the dominant eigenvector of a matrix fairly quickly. Why might you be only interested in the dominant eigenvector of a matrix? Discuss.
</div>

<hr />

<div class="exercise">
<p><span id="exr:power-proof" class="exercise"><strong>Exercise 4.60  </strong></span>In this problem we will formally prove the conjecture that you just made. This conjecture will lead us to the <strong>power method</strong> for finding the dominant eigenvector and eigenvalue of a matrix.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Assume that <span class="math inline">\(A\)</span> has <span class="math inline">\(n\)</span> linearly independent eigenvectors
<span class="math inline">\(\boldsymbol{v}_1, \boldsymbol{v}_2, \dots, \boldsymbol{v}_n\)</span> and choose <span class="math inline">\(\boldsymbol{x} = \sum_{j=1}^n c_j \boldsymbol{v}_j\)</span>. You have proved in the past that
<span class="math display">\[A^k \boldsymbol{x} = c_1 \lambda_1^k \boldsymbol{v}_1 + c_2 \lambda_2^k \boldsymbol{v}_2 + \cdots c_n \lambda_n^k \boldsymbol{v}_n.\]</span>
Stop and sketch out the details of this proof now.</p></li>
<li><p>If we factor <span class="math inline">\(\lambda_1^k\)</span> out of the right-hand side we get
<span class="math display">\[A^k \boldsymbol{x} = \lambda_1^k \left( c_1 ??? + c_2 \left(
                \frac{???}{???} \right)^k \boldsymbol{v}_2 + c_3 \left(
                \frac{???}{???}
                \right)^k \boldsymbol{v}_3 + \cdots + c_n \left( \frac{???}{???}
                \right)^k \boldsymbol{v}_n \right)\]</span>
(fill in the question marks)</p></li>
<li><p>If <span class="math inline">\(|\lambda_1| &gt; |\lambda_2| \ge |\lambda_3| \ge \cdots \ge |\lambda_n|\)</span>
then what happens to each of the <span class="math inline">\((\lambda_j/\lambda_1)^k\)</span> terms as
<span class="math inline">\(k \to \infty\)</span>?</p></li>
<li><p>Using your answer to part (c), what is
<span class="math inline">\(\lim_{k \to \infty} A^k \boldsymbol{x} / \lambda_1^k\)</span>?</p></li>
</ol>
</div>

<hr />

<div class="theorem">
<p><span id="thm:power" class="theorem"><strong>Theorem 4.5  (The Power Method)  </strong></span>The following algorithm, called <strong>the power method</strong> will quickly find the eigenvalue of largest absolute value for a square matrix <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> as well as the associated (normalized) eigenvector. We are assuming that there are <span class="math inline">\(n\)</span> linearly independent eigenvectors of <span class="math inline">\(A\)</span>.</p>
<dl>
<dt>Step #1:</dt>
<dd><p>Given a nonzero vector <span class="math inline">\(\boldsymbol{x}\)</span>, set <span class="math inline">\(\boldsymbol{v}^{(1)} = \boldsymbol{x} / \|\boldsymbol{x}\|\)</span>. (Here
the superscript indicates the iteration number) Note that the initial vector <span class="math inline">\(\boldsymbol{x}\)</span> is pretty irrelevant to the process so it can just be a random vector of the correct size..</p>
</dd>
<dt>Step #2:</dt>
<dd><p>For <span class="math inline">\(k=2, 3, \ldots\)</span></p>
<dl>
<dt>Step #2a:</dt>
<dd><p>Compute <span class="math inline">\(\tilde{\boldsymbol{v}}^{(k)} = A \boldsymbol{v}^{(k-1)}\)</span> (this gives a
non-normalized version of the next estimate of the dominant
eigenvector.)</p>
</dd>
<dt>Step #2b:</dt>
<dd><p>Set <span class="math inline">\(\lambda^{(k)} = \tilde{\boldsymbol{v}}^{(k)} \cdot  \boldsymbol{v}^{(k-1)}\)</span>. (this gives an
approximation of the eigenvalue since if <span class="math inline">\(\boldsymbol{v}^{(k-1)}\)</span> was the
actual eigenvector we would have
<span class="math inline">\(\lambda = A \boldsymbol{v}^{(k-1)} \cdot \boldsymbol{v}^{(k-1)}\)</span>. Stop now and explain this.)</p>
</dd>
<dt>Step #2c:</dt>
<dd><p>Normalize <span class="math inline">\(\tilde{\boldsymbol{v}}^{(k)}\)</span> by computing <span class="math inline">\(\boldsymbol{v}^{(k)} =  \tilde{\boldsymbol{v}}^{(k)} / \| \tilde{\boldsymbol{v}}^{(k)} \|\)</span>.
(This guarantees that you will be sending a unit vector into the
next iteration of the loop)</p>
</dd>
</dl>
</dd>
</dl>
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-374" class="exercise"><strong>Exercise 4.61  </strong></span>Go through Theorem <a href="ch-linearalgebra.html#thm:power">4.5</a> carefully and describe what we need to do in each step and why we’re doing it. Then complete all of the missing pieces of the following Python function.
</div>

<div class="sourceCode" id="cb117"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb117-1"><a href="ch-linearalgebra.html#cb117-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb117-2"><a href="ch-linearalgebra.html#cb117-2"></a><span class="kw">def</span> myPower(A, tol <span class="op">=</span> <span class="fl">1e-8</span>):</span>
<span id="cb117-3"><a href="ch-linearalgebra.html#cb117-3"></a>    n <span class="op">=</span> A.shape[<span class="dv">0</span>]</span>
<span id="cb117-4"><a href="ch-linearalgebra.html#cb117-4"></a>    x <span class="op">=</span> np.matrix( np.random.randn(n,<span class="dv">1</span>) )</span>
<span id="cb117-5"><a href="ch-linearalgebra.html#cb117-5"></a>    x <span class="op">=</span> <span class="co"># turn x into a unit vector</span></span>
<span id="cb117-6"><a href="ch-linearalgebra.html#cb117-6"></a>    <span class="co"># we don&#39;t actually need to keep track of the old iterates</span></span>
<span id="cb117-7"><a href="ch-linearalgebra.html#cb117-7"></a>    L <span class="op">=</span> <span class="dv">1</span> <span class="co"># initialize the dominant eigenvalue</span></span>
<span id="cb117-8"><a href="ch-linearalgebra.html#cb117-8"></a>    counter <span class="op">=</span> <span class="dv">0</span> <span class="co"># keep track of how many steps we&#39;ve taken</span></span>
<span id="cb117-9"><a href="ch-linearalgebra.html#cb117-9"></a>    <span class="co"># You can build a stopping rule from the definition</span></span>
<span id="cb117-10"><a href="ch-linearalgebra.html#cb117-10"></a>    <span class="co"># Ax = lambda x ... </span></span>
<span id="cb117-11"><a href="ch-linearalgebra.html#cb117-11"></a>    <span class="cf">while</span> (???) <span class="op">&gt;</span> tol <span class="kw">and</span> counter <span class="op">&lt;</span> <span class="dv">10000</span>:</span>
<span id="cb117-12"><a href="ch-linearalgebra.html#cb117-12"></a>        x <span class="op">=</span> A<span class="op">*</span>x <span class="co"># update the dominant eigenvector</span></span>
<span id="cb117-13"><a href="ch-linearalgebra.html#cb117-13"></a>        x <span class="op">=</span> ??? <span class="co"># normalize</span></span>
<span id="cb117-14"><a href="ch-linearalgebra.html#cb117-14"></a>        L <span class="op">=</span> ??? <span class="co"># approximate the eignevalue</span></span>
<span id="cb117-15"><a href="ch-linearalgebra.html#cb117-15"></a>        counter <span class="op">+=</span> <span class="dv">1</span> <span class="co"># increment the counter</span></span>
<span id="cb117-16"><a href="ch-linearalgebra.html#cb117-16"></a>    <span class="cf">return</span> x, L</span></code></pre></div>
<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-376" class="exercise"><strong>Exercise 4.62  </strong></span>Test your <code>myPower()</code> function on several matrices where you know the eigenstructure. Then try the <code>myPower()</code> function on larger random matrices. You can check that it is working using <code>np.linalg.eig()</code> (be sure to normalize the vectors in the same way so you can compare them.)
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-377" class="exercise"><strong>Exercise 4.63  </strong></span>In the Power Method iteration you may end up getting a different sign on your eigenvector as compared to <code>np.linalg.eig()</code>. Why might this happen? Generate a few examples so you can see this. You can avoid this issue if you use a <code>while</code> loop in your Power Method code and the logical check takes advantage of the fact that we are trying to solve the equation <span class="math inline">\(A\boldsymbol{x} = \lambda \boldsymbol{x}\)</span>. Hint: <span class="math inline">\(A\boldsymbol{x} = \lambda \boldsymbol{x}\)</span> is equivalent to <span class="math inline">\(A\boldsymbol{x} - \lambda \boldsymbol{x} = \boldsymbol{0}\)</span>.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-378" class="exercise"><strong>Exercise 4.64  </strong></span>What happens in the power method iterations when <span class="math inline">\(\lambda_1\)</span> is complex. The maximum eigenvalue can certainly be complex if <span class="math inline">\(|\lambda_1|\)</span> (the modulus of the complex number) is larger than all of the other eigenvalues. It may be helpful to build a matrix specifically with complex eigenvalues.<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a>
</div>

<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-379" class="exercise"><strong>Exercise 4.65  (Convergence Rate of the Power Method)  </strong></span>The proof that the power method will work hinges on the fact that <span class="math inline">\(|\lambda_1| &gt; |\lambda_2| \geq |\lambda_3| \geq \cdots \geq |\lambda_n|\)</span>. In Exercise <a href="ch-linearalgebra.html#exr:power-proof">4.60</a> we proved that the limit
<span class="math display">\[ \lim_{k \to \infty} \frac{A^k \boldsymbol{x}}{\lambda_1^k} \]</span>
converges to the dominant eigenvector, but how fast is the convergence? What does the speed of the convergence depend on?</p>
<p>Take note that since we’re assuming that the eigenvalues are ordered, the ratio <span class="math inline">\(\lambda_2 / \lambda_1\)</span> will be larger than <span class="math inline">\(\lambda_j / \lambda_1\)</span> for all <span class="math inline">\(j&gt;2\)</span>. Hence, the speed at which the power method converges depends mostly on the ratio <span class="math inline">\(\lambda_2 / \lambda_1\)</span>. Let’s build a numerical experiment to see how sensitive the power method is to this ratio.</p>
<p>Build a <span class="math inline">\(4 \times 4\)</span> matrix <span class="math inline">\(A\)</span> with dominant eigenvalue <span class="math inline">\(\lambda_1 = 1\)</span> and all other eigenvalues less than 1 in absolute value. Then choose several values of <span class="math inline">\(\lambda_2\)</span> and build an experiment to determine the number of iterations that it takes for the power method to converge to within a pre-determined tolerance to the dominant eigenvector. In the end you should produce a plot with the ratio <span class="math inline">\(\lambda_2 / \lambda_1\)</span> on the horizontal axis and the number of iterations to converge to a fixed tolerance on the vertical axis. Discuss what you see in your plot.</p>
<p>Hint: To build a matrix with specific eigen-structure use the matrix factorization <span class="math inline">\(A = PDP^{-1}\)</span> where the columns of <span class="math inline">\(P\)</span> contain the eigenvectors of <span class="math inline">\(A\)</span> and the diagonal of <span class="math inline">\(D\)</span> contains the eigenvalues. In this case the <span class="math inline">\(P\)</span> matrix can be random but you need to control the <span class="math inline">\(D\)</span> matrix. Moreover, remember that <span class="math inline">\(\lambda_3\)</span> and <span class="math inline">\(\lambda_4\)</span> should be smaller than <span class="math inline">\(\lambda_2\)</span>.</p>
</div>

<hr />
<div style="page-break-after: always;"></div>
</div>
<div id="exercises-3" class="section level2">
<h2><span class="header-section-number">4.8</span> Exercises</h2>
<div id="algorithm-summaries-2" class="section level3">
<h3><span class="header-section-number">4.8.1</span> Algorithm Summaries</h3>

<div class="exercise">
<span id="exr:unnamed-chunk-380" class="exercise"><strong>Exercise 4.66  </strong></span>Explain in clear language how to efficiently solve an upper triangular
system of linear equations.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-381" class="exercise"><strong>Exercise 4.67  </strong></span>Explain in clear language how to efficiently solve a lower triangular
system of linear equations.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-382" class="exercise"><strong>Exercise 4.68  </strong></span>Explain in clear language how to solve the equation <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span> using an <span class="math inline">\(LU\)</span> decomposition.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-383" class="exercise"><strong>Exercise 4.69  </strong></span>Explain in clear language how to solve an overdetermined system of linear equations (more equations than unknowns) numerically.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-384" class="exercise"><strong>Exercise 4.70  </strong></span>Explain in clear language the algorithm for finding the columns of the <span class="math inline">\(Q\)</span> matrix in the <span class="math inline">\(QR\)</span> factorization. Give all of the mathematical details.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-385" class="exercise"><strong>Exercise 4.71  </strong></span>Explain in clear language how to find the upper triangular matrix <span class="math inline">\(R\)</span> in the <span class="math inline">\(QR\)</span> factorization. Give all of the mathematical details.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-386" class="exercise"><strong>Exercise 4.72  </strong></span>Explain in clear language how to solve the equation <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span> using a <span class="math inline">\(QR\)</span> decomposition.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-387" class="exercise"><strong>Exercise 4.73  </strong></span>Explain in clear language how the power method works to find the dominant eigenvalue and eigenvector of a square matrix. Give all of the mathematical details.
</div>

<hr />
</div>
<div id="applying-what-youve-learned-3" class="section level3">
<h3><span class="header-section-number">4.8.2</span> Applying What You’ve Learned</h3>

<div class="exercise">
<p><span id="exr:unnamed-chunk-388" class="exercise"><strong>Exercise 4.74  </strong></span>As mentioned much earlier in this chapter, there is an <code>rref()</code> command in Python, but it is in the <code>sympy</code> library instead of the <code>numpy</code> library – it is implemented as a symbolic computation instead of a numerical computation. OK. So what? In this problem we want to compare the time to solve a system of equations <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span> with each of the following techniques:</p>
<ul>
<li>row reduction of an augmented matrix <span class="math inline">\(\begin{pmatrix} A &amp; | &amp; b\end{pmatrix}\)</span> with <code>sympy</code>,</li>
<li>our implementation of the <span class="math inline">\(LU\)</span> decomposition,</li>
<li>our implementation of the <span class="math inline">\(QR\)</span> decomposition, and</li>
<li>the <code>numpy.linalg.solve()</code> command.</li>
</ul>
<p>To time code in Python first import the <code>time</code> library. Then use <code>start = time.time()</code> at the start of your code and <code>stop = time.time()</code> and the end of your code. The difference between <code>stop</code> and <code>start</code> is the elapsed computation time.</p>
<p>Make observations about how the algorithms perform for different sized matrices. You can use random matrices and vectors for <span class="math inline">\(A\)</span> and <span class="math inline">\(\boldsymbol{b}\)</span>. The end result should be a plot showing how the average computation time for each algorithm behaves as a function of the size of the coefficient matrix.</p>
<p>The code below will compute the reduced row echelon form of a matrix (RREF). Implement the code so that you know how it works.</p>
</div>

<div class="sourceCode" id="cb118"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb118-1"><a href="ch-linearalgebra.html#cb118-1"></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb118-2"><a href="ch-linearalgebra.html#cb118-2"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb118-3"><a href="ch-linearalgebra.html#cb118-3"></a><span class="co"># in this problem it will be easiest to start with numpy matrices</span></span>
<span id="cb118-4"><a href="ch-linearalgebra.html#cb118-4"></a>A <span class="op">=</span> np.matrix([[<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>], [<span class="op">-</span><span class="dv">1</span>, <span class="dv">-3</span>, <span class="dv">-3</span>]]) </span>
<span id="cb118-5"><a href="ch-linearalgebra.html#cb118-5"></a>b <span class="op">=</span> np.matrix([[<span class="dv">3</span>],[<span class="dv">7</span>],[<span class="dv">3</span>]])</span>
<span id="cb118-6"><a href="ch-linearalgebra.html#cb118-6"></a>Augmented <span class="op">=</span> np.c_[A,b] <span class="co"># augment b onto the right hand side of A</span></span>
<span id="cb118-7"><a href="ch-linearalgebra.html#cb118-7"></a></span>
<span id="cb118-8"><a href="ch-linearalgebra.html#cb118-8"></a>Msymbolic <span class="op">=</span> sp.Matrix(Augmented)</span>
<span id="cb118-9"><a href="ch-linearalgebra.html#cb118-9"></a>MsymbolicRREF <span class="op">=</span> Msymbolic.rref()</span>
<span id="cb118-10"><a href="ch-linearalgebra.html#cb118-10"></a><span class="bu">print</span>(MsymbolicRREF)</span></code></pre></div>
<p>To time code you can use code like the following.</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb119-1"><a href="ch-linearalgebra.html#cb119-1"></a><span class="im">import</span> time</span>
<span id="cb119-2"><a href="ch-linearalgebra.html#cb119-2"></a>start <span class="op">=</span> time.time()</span>
<span id="cb119-3"><a href="ch-linearalgebra.html#cb119-3"></a><span class="co"># some code that you want to time</span></span>
<span id="cb119-4"><a href="ch-linearalgebra.html#cb119-4"></a>stop <span class="op">=</span>  time.time()</span>
<span id="cb119-5"><a href="ch-linearalgebra.html#cb119-5"></a>total_time <span class="op">=</span> stop <span class="op">-</span> start</span>
<span id="cb119-6"><a href="ch-linearalgebra.html#cb119-6"></a><span class="bu">print</span>(<span class="st">&quot;Total computation time=&quot;</span>,total_time)</span></code></pre></div>
<hr />

<div class="exercise">
<p><span id="exr:heated-rod-lin-alg" class="exercise"><strong>Exercise 4.75  </strong></span>Imagine that we have a 1 meter long thin metal rod that has been heated to 100<span class="math inline">\(^\circ\)</span> on the left-hand side and cooled to 0<span class="math inline">\(^\circ\)</span> on the right-hand side. We want to know the temperature every 10 cm from left to right on the rod.</p>
<ol style="list-style-type: lower-alpha">
<li><p>First we break the rod into equal 10cm increments as shown. See Figure <a href="ch-linearalgebra.html#fig:heated-rod">4.2</a>. How many unknowns are there in this picture?</p></li>
<li><p>The temperature at each point along the rod is the average of the
temperatures at the adjacent points. For example, if we let <span class="math inline">\(T_1\)</span> be
the temperature at point <span class="math inline">\(x_1\)</span> then <span class="math display">\[T_1 = \frac{T_0 + T_2}{2}.\]</span>
Write a system of equations for each of the unknown temperatures.</p></li>
<li><p>Solve the system for the temperature at each unknown node using either <span class="math inline">\(LU\)</span> or <span class="math inline">\(QR\)</span> decomposition.</p></li>
</ol>
</div>

<div class="figure" style="text-align: center"><span id="fig:heated-rod"></span>
<img src="images/Ch04_HeatedRod.png" alt="A rod to be heated broken into 10 equal-length segments." width="60%" />
<p class="caption">
Figure 4.2: A rod to be heated broken into 10 equal-length segments.
</p>
</div>
<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-391" class="exercise"><strong>Exercise 4.76  </strong></span>Write code to solve the following systems of equations via both LU and QR decompositions. If the algorithm fails then be sure to explain exactly why.</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math display">\[\begin{array}{rl} x + 2y + 3z &amp;= 4 \\ 2x + 4y + 3z &amp;= 5 \\ x + y &amp;= 4 \end{array}\]</span></p></li>
<li><p><span class="math display">\[\begin{array}{rl} 2y + 3z &amp;= 4 \\ 2x + 3z &amp;= 5 \\ y &amp;= 4 \end{array}\]</span></p></li>
<li><p><span class="math display">\[\begin{array}{rl} 2y + 3z &amp;= 4 \\ 2x + 4y + 3z &amp;= 5 \\ x+y &amp;= 4 \end{array}\]</span></p></li>
</ol>
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-392" class="exercise"><strong>Exercise 4.77  </strong></span>Give a specific example of a nonzero matrix which will NOT have an <span class="math inline">\(LU\)</span> decomposition. Give specific reasons why <span class="math inline">\(LU\)</span> will fail on your matrix.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-393" class="exercise"><strong>Exercise 4.78  </strong></span>Give a specific example of a nonzero matrix which will NOT have an <span class="math inline">\(QR\)</span> decomposition. Give specific reasons why <span class="math inline">\(QR\)</span> will fail on your matrix.
</div>

<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-394" class="exercise"><strong>Exercise 4.79  </strong></span>Have you ever wondered how scientific software computes a determinant? The formula that you learned for <a href="https://en.wikipedia.org/wiki/Determinant">calculating determinants by hand</a> is horribly cumbersome and computationally intractible for large matrices. This problem is meant to give you glimpse of what is <em>actually</em> going on under the hood.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a></p>
<p>If <span class="math inline">\(A\)</span> has an <span class="math inline">\(LU\)</span> decomposition then <span class="math inline">\(A = LU\)</span>. Use properties that you know about determinants to come up with a simple way to find the determinant for matrices that have an <span class="math inline">\(LU\)</span> decomposition. Show all of your work in developing your formula.</p>
Once you have your formula for calculating <span class="math inline">\(\det(A)\)</span>, write a Python function that accepts a matrix, produces the <span class="math inline">\(LU\)</span> decomposition, and returns the determinant of <span class="math inline">\(A\)</span>. Check your work against Python’s <code>np.linalg.det()</code> function.
</div>

<hr />

<div class="exercise">
<p><span id="exr:lu-error" class="exercise"><strong>Exercise 4.80  </strong></span>For this problem we are going to run a numerical experiment to see how the process of solving the equation <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span> using the <span class="math inline">\(LU\)</span> factorization performs on random coefficient matrices <span class="math inline">\(A\)</span> and random right-hand sides <span class="math inline">\(\boldsymbol{b}\)</span>. We will compare against Python’s algorithm for solving linear systems.</p>
<p>We will do the following:</p>
<p>Create a loop that does the following:</p>
<ol style="list-style-type: lower-alpha">
<li>Loop over the size of the matrix <span class="math inline">\(n\)</span>.</li>
<li>Build a random matrix <span class="math inline">\(A\)</span> of size <span class="math inline">\(n \times n\)</span>. You can do this with the code <code>A = np.matrix( np.random.randn(n,n) )</code></li>
<li>Build a random vector <span class="math inline">\(\boldsymbol{b}\)</span> in <span class="math inline">\(\mathbb{R}^{n}\)</span>. You can do this with the code <code>b = np.matrix( np.random.randn(n,1) )</code></li>
<li>Find Python’s answer to the problem <span class="math inline">\(A\boldsymbol{x}=\boldsymbol{b}\)</span> =0 using the command <code>exact = np.linalg.solve(A,b)</code></li>
<li>Write code that uses your three <span class="math inline">\(LU\)</span> functions (<code>myLU</code>, <code>lsolve</code>, <code>usolve</code>) to find a solution to the equation <span class="math inline">\(A\boldsymbol{x}=\boldsymbol{b}\)</span>.</li>
<li>Find the error between your answer and the exact answer using the code <code>np.linalg.norm(x - exact)</code></li>
<li>Make a plot (<code>plt.semilogy()</code>) that shows how the error behaves as the size of the problem changes. You should run this for matrices of larger and larger size but be warned that the loop will run for quite a long time if you go above <span class="math inline">\(300 \times 300\)</span> matrices. Just be patient.</li>
</ol>
<p><strong>Conclusions:</strong> What do you notice in your final plot. What does this tell you about the behavior of our <span class="math inline">\(LU\)</span> decomposition code?</p>
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-395" class="exercise"><strong>Exercise 4.81  </strong></span>Repeat Exercise <a href="ch-linearalgebra.html#exr:lu-error">4.80</a> for the <span class="math inline">\(QR\)</span> decomposition. Your final plot should show both the behavior of <span class="math inline">\(QR\)</span> and of <span class="math inline">\(LU\)</span> throughout the experiement. What do you notice?
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-396" class="exercise"><strong>Exercise 4.82  </strong></span>Find a least squares solution to the equation <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span> in two
different ways with
<span class="math display">\[A = \begin{pmatrix} 1 &amp; 3 &amp; 5 \\ 4 &amp; -2 &amp; 6 \\ 4 &amp; 7 &amp; 8 \\ 3 &amp; 7 &amp; 19
        \end{pmatrix} \quad \text{and} \quad \boldsymbol{b} = \begin{pmatrix} 5 \\ 2 \\ -2 \\
        8\end{pmatrix}.\]</span>
</div>

<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-397" class="exercise"><strong>Exercise 4.83  </strong></span>Let <span class="math inline">\(A\)</span> be defined as
<span class="math display">\[ A = \begin{pmatrix} 10^{-20} &amp; 1 \\ 1 &amp; 1 \end{pmatrix} \]</span>
and let <span class="math inline">\(\boldsymbol{b}\)</span> be the vector
<span class="math display">\[ \boldsymbol{b}=\begin{pmatrix} 2 \\ 3\end{pmatrix}. \]</span><br />
Notice that <span class="math inline">\(A\)</span> has a tiny, but nonzero, value in the first entry.</p>
<ol style="list-style-type: lower-alpha">
<li>Solve the linear system <span class="math inline">\(A \boldsymbol{x}=\boldsymbol{b}\)</span> by hand.<br />
</li>
<li>Use your <code>myLU</code>, <code>lsolve</code>, and <code>usolve</code> functions to solve this problem using the LU decomposition method.<br />
</li>
<li>Compare your answers to parts (a) and (b). What went wrong?</li>
</ol>
</div>

<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-398" class="exercise"><strong>Exercise 4.84  (Hilbert Matrices)  </strong></span>A Hilbert Matrix is a matrix of the form <span class="math inline">\(H_{ij} = 1 / (i+j+1)\)</span> where both <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> both start indexed at <span class="math inline">\(0\)</span>. For example, a <span class="math inline">\(4 \times 4\)</span> Hilbert Matrix is
<span class="math display">\[ H = \begin{pmatrix} 1 &amp; \frac{1}{2} &amp; \frac{1}{3} &amp; \frac{1}{4} \\
                       \frac{1}{2} &amp; \frac{1}{3} &amp; \frac{1}{4} &amp; \frac{1}{5} \\ 
                       \frac{1}{3} &amp; \frac{1}{4} &amp; \frac{1}{5} &amp; \frac{1}{6} \\ 
                       \frac{1}{4} &amp; \frac{1}{5} &amp; \frac{1}{6} &amp; \frac{1}{7} \end{pmatrix}. \]</span>
This type of matrix is often used to test numerical linear algebra algorithms since it is known to have some <em>odd</em> behaviors … which you’ll see in a moment.</p>
<ol style="list-style-type: lower-alpha">
<li>Write code to build a <span class="math inline">\(n\times n\)</span> Hilbert Matrix and call this matrix <span class="math inline">\(H\)</span>. Test your code for various values of <span class="math inline">\(n\)</span> to be sure that it is building the correct matrices.</li>
<li>Build a vector of ones called <span class="math inline">\(\boldsymbol{b}\)</span> with code <code>b = np.ones( (n,1) )</code>. We will use <span class="math inline">\(\boldsymbol{b}\)</span> as the right hand side of the system of equations <span class="math inline">\(H\boldsymbol{x} = \boldsymbol{b}\)</span>.</li>
<li>Solve the system of equations <span class="math inline">\(H \boldsymbol{x} = \boldsymbol{b}\)</span> using any technique you like from this chapter.</li>
<li>Now let’s say that you change the first entry of <span class="math inline">\(\boldsymbol{b}\)</span> by just a little bit, say <span class="math inline">\(10^{-15}\)</span>. If we were to now solve the equation <span class="math inline">\(H \boldsymbol{x}_{new} = \boldsymbol{b}_{new}\)</span> what would you expect as compared to solving <span class="math inline">\(H \boldsymbol{x} = \boldsymbol{b}\)</span>.</li>
<li>Now let’s actually make the change suggested in part (d). Use the code <code>bnew = np.ones( (n,1) )</code> and then <code>bnew[0] = bnew[0] + 1e-15</code> to build a new <span class="math inline">\(\boldsymbol{b}\)</span> vector with this small change. Solve <span class="math inline">\(H\boldsymbol{x}=\boldsymbol{b}\)</span> and <span class="math inline">\(H\boldsymbol{x}_{new}=\boldsymbol{b}_{new}\)</span> and then compare the maximum absolute difference <code>np.max( np.abs( x - xnew ) )</code>. What do you notice? Make a plot with <span class="math inline">\(n\)</span> on the horizontal axis and the maximum absolute difference on the vertical axis. What does this plot tell you about the solution to the equation <span class="math inline">\(H \boldsymbol{x} = \boldsymbol{b}\)</span>?</li>
<li>We know that <span class="math inline">\(H H^{-1}\)</span> should be the identity matrix. As we’ll see, however, Hilbert matrices are particularly poorly behaved! Write a loop over <span class="math inline">\(n\)</span> that (i) builds a Hilbert matrix of size <span class="math inline">\(n\)</span>, (ii) calculates <span class="math inline">\(H H^{-1}\)</span> (using <code>np.linalg.inv()</code> to compute the inverse directly), (iii) calculates the norm of the difference between the identity matrix (<code>np.identity(n)</code>) and your calculated identity matrix from part (ii). Finally. Build a plot that shows <span class="math inline">\(n\)</span> on the horizontal axis and the normed difference on the vertical axis. What do you see? What does this mean about the matrix inversion of the Hilbert matrix.</li>
<li>There are cautionary tales hiding in this problem. Write a paragraph explaining what you can learn by playing with pathological matrices like the Hilbert Matrix.</li>
</ol>
</div>

<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-399" class="exercise"><strong>Exercise 4.85  </strong></span>Now that you have <span class="math inline">\(QR\)</span> and <span class="math inline">\(LU\)</span> code we’re going to use both of them! The problem is as follows:<br />
We are going to find the polynomial of degree 4 that best fits the function
$<span class="math display">\[ y = \cos(4t) + 0.1 \varepsilon(t) \]</span>
at 50 equally spaced points <span class="math inline">\(t\)</span> between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. Here we are using <span class="math inline">\(\varepsilon(t)\)</span> as a function that outputs normally distributed random white noise. In Python you will build <span class="math inline">\(y\)</span> as <code>y = np.cos(4*t) + 0.1*np.random.randn(t.shape[0])</code></p>
<p>Build the <span class="math inline">\(t\)</span> vector and the <span class="math inline">\(y\)</span> vector (these are your data). We need to set up the least squares problems <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span> by setting up the matrix <span class="math inline">\(A\)</span> as we did in the other least squares curve fitting problems and by setting up the <span class="math inline">\(\boldsymbol{b}\)</span> vector using the <span class="math inline">\(y\)</span> data you just built. Solve the problem of finding the coefficients of the best degree 4 polynomial that fits this data. Report the sum of squared error and show a plot of the data along with the best fit curve.</p>
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-400" class="exercise"><strong>Exercise 4.86  </strong></span>Find the largest eigenvalue and the associated eigenvector of the matrix <span class="math inline">\(A\)</span> WITHOUT using <code>np.linalg.eig()</code>. (Don’t do this by hand either)
<span class="math display">\[A = \begin{pmatrix} 1 &amp; 2 &amp; 3 &amp; 4 \\ 5 &amp; 6 &amp; 7 &amp; 8 \\ 9 &amp; 0 &amp; 1 &amp; 2 \\ 3 &amp; 4 &amp; 5 &amp;
        6 \end{pmatrix}\]</span>
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-401" class="exercise"><strong>Exercise 4.87  </strong></span>It is possible in a matrix that the eigenvalues <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span> are equal but with the corresponding eigenvectors not equal. Before you experiment with matrices of this sort, write a conjecture about what will happen to the power method in this case (look back to our proof in Exercise <a href="ch-linearalgebra.html#exr:power-proof">4.60</a> of how the power method works). Now build several specific matrices where this is the case and see what happens to the power method.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-402" class="exercise"><strong>Exercise 4.88  </strong></span>Will the power method fail, slow down, or be uneffected if one (or more) of the non-dominant eigenvalues is zero? Give sufficient mathematical evidence or show several numerical experiments to support your answer.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-403" class="exercise"><strong>Exercise 4.89  </strong></span>Find a cubic function that best fits the following data. you can download the data directly with the code below.
</div>

<table>
<thead>
<tr class="header">
<th><span class="math inline">\(x\)</span> Data</th>
<th><span class="math inline">\(y\)</span> Data</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1.0220</td>
</tr>
<tr class="even">
<td>0.0500</td>
<td>1.0174</td>
</tr>
<tr class="odd">
<td>0.1000</td>
<td>1.0428</td>
</tr>
<tr class="even">
<td>0.1500</td>
<td>1.0690</td>
</tr>
<tr class="odd">
<td>0.2000</td>
<td>1.0505</td>
</tr>
<tr class="even">
<td>0.2500</td>
<td>1.0631</td>
</tr>
<tr class="odd">
<td>0.3000</td>
<td>1.0458</td>
</tr>
<tr class="even">
<td>0.3500</td>
<td>1.0513</td>
</tr>
<tr class="odd">
<td>0.4000</td>
<td>1.0199</td>
</tr>
<tr class="even">
<td>0.4500</td>
<td>1.0180</td>
</tr>
<tr class="odd">
<td>0.5000</td>
<td>1.0156</td>
</tr>
<tr class="even">
<td>0.5500</td>
<td>0.9817</td>
</tr>
<tr class="odd">
<td>0.6000</td>
<td>0.9652</td>
</tr>
<tr class="even">
<td>0.6500</td>
<td>0.9429</td>
</tr>
<tr class="odd">
<td>0.7000</td>
<td>0.9393</td>
</tr>
<tr class="even">
<td>0.7500</td>
<td>0.9266</td>
</tr>
<tr class="odd">
<td>0.8000</td>
<td>0.8959</td>
</tr>
<tr class="even">
<td>0.8500</td>
<td>0.9014</td>
</tr>
<tr class="odd">
<td>0.9000</td>
<td>0.8990</td>
</tr>
<tr class="even">
<td>0.9500</td>
<td>0.9038</td>
</tr>
<tr class="odd">
<td>1.0000</td>
<td>0.8989</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb120"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb120-1"><a href="ch-linearalgebra.html#cb120-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb120-2"><a href="ch-linearalgebra.html#cb120-2"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb120-3"><a href="ch-linearalgebra.html#cb120-3"></a>URL1 <span class="op">=</span> <span class="st">&#39;https://raw.githubusercontent.com/NumericalMethodsSullivan&#39;</span></span>
<span id="cb120-4"><a href="ch-linearalgebra.html#cb120-4"></a>URL2 <span class="op">=</span> <span class="st">&#39;/NumericalMethodsSullivan.github.io/master/data/&#39;</span></span>
<span id="cb120-5"><a href="ch-linearalgebra.html#cb120-5"></a>URL <span class="op">=</span> URL1<span class="op">+</span>URL2</span>
<span id="cb120-6"><a href="ch-linearalgebra.html#cb120-6"></a>data <span class="op">=</span> np.array( pd.read_csv(URL<span class="op">+</span><span class="st">&#39;Exercise4_89.csv&#39;</span>) )</span>
<span id="cb120-7"><a href="ch-linearalgebra.html#cb120-7"></a><span class="co"># Exercise4_89.csv</span></span></code></pre></div>
<hr />

<div class="theorem">
<span id="thm:symmetric-eigenvalues" class="theorem"><strong>Theorem 4.6  </strong></span>If <span class="math inline">\(A\)</span> is a symmetric matrix with eigenvalues <span class="math inline">\(\lambda_1, \lambda_2, \ldots, \lambda_n\)</span> then <span class="math inline">\(|\lambda_1| &gt; |\lambda_2| &gt; \cdots &gt; |\lambda_n|\)</span>. Furthermore, the eigenvectors will be orthogonal to each other.
</div>


<div class="exercise">
<p><span id="exr:unnamed-chunk-405" class="exercise"><strong>Exercise 4.90  (The Deflation Method)  </strong></span>
For symmetric matrices we can build an extension to the power method in order to find the second most dominant eigen-pair for a matrix <span class="math inline">\(A\)</span>. Theorem <a href="ch-linearalgebra.html#thm:symmetric-eigenvalues">4.6</a> suggests the following method for finding the second dominant eigen-pair for a symmetric matrix. This method is called the <strong>deflation method</strong>.</p>
<ul>
<li>Use the power method to find the dominant eigenvalue and eigenvector.</li>
<li>Start with a random unit vector of the correct shape.</li>
<li>Multiplying your vector by <span class="math inline">\(A\)</span> will <em>pull it toward</em> the dominant eigenvector. After you multiply, project your vector onto the dominant eigenvector and find the projection error.</li>
<li>Use the projection error as the new approximation for the eigenvector (Why should we do this? What are we really finding here?)</li>
</ul>
<p>Note that the deflation method is really exactly the same as the power method with the exception that we orthogonalize at every step. Hence, when you write your code expect to only change a few lines from your power method.</p>
<p>Write a function to find the second largest eigenvalue and eigenvector pair by putting the deflation method into practice. Test your code on a matrix <span class="math inline">\(A\)</span> and compare against Python’s <code>np.linalg.eig()</code> command. Your code needs to work on symmetric matrices of arbitrary size and you need to write test code that clearly shows the error between your calculated eigenvalue and Python’s eigenvalue as well as your calculated eigenvector and ’s eigenvector.</p>
To guarantee that you start with a symmetric matrix you can use the
following code.
</div>

<div class="sourceCode" id="cb121"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb121-1"><a href="ch-linearalgebra.html#cb121-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb121-2"><a href="ch-linearalgebra.html#cb121-2"></a>N <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb121-3"><a href="ch-linearalgebra.html#cb121-3"></a>A <span class="op">=</span> np.random.randn(N,N)</span>
<span id="cb121-4"><a href="ch-linearalgebra.html#cb121-4"></a>A <span class="op">=</span> np.matrix(A)</span>
<span id="cb121-5"><a href="ch-linearalgebra.html#cb121-5"></a>A <span class="op">=</span> np.transpose(A) <span class="op">*</span> A <span class="co"># why should this build a symmetric matrix</span></span></code></pre></div>
<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-407" class="exercise"><strong>Exercise 4.91  </strong></span>(This concept for this problem is modified from <span class="citation">[<a href="#ref-meerschaert" role="doc-biblioref">6</a>]</span>. The data is taken from <a href="https://www.weather.gov/arx/mississippi_river">NOAA and the National Weather Service</a> with the specific values associated with La Crosse, WI.)</p>
<p>Floods in the Mississippi River Valleys of the upper midwest have somewhat predictable day-to-day behavior in that the flood stage today has high predictive power for the flood stage tomorrow. Assume that the flood stages are:</p>
<ul>
<li>Stage 0 (Normal): Average daily flow is below 90,000 <span class="math inline">\(ft^3/sec\)</span> (cubic feet per second = cfs). This is the <em>normal</em> river level.</li>
<li>Stage 1 (Action Level): Average daily flow is between 90,000 cfs and 124,000 cfs.</li>
<li>Stage 2 (Minor Flood): Average daily flow is between 124,000 cfs and 146,000 cfs.</li>
<li>Stage 3 (Moderate Flood): Average daily flow is between 146,000 cfs and 170,000 cfs.</li>
<li>Stage 4 (Extreme Flood): Average daily flow is above 170,000 cfs.</li>
</ul>
<p>The following table shows the probability of one stage transitioning into another stage from one day to the next.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>0 Today</th>
<th>1 Today</th>
<th>2 Today</th>
<th>3 Today</th>
<th>4 Today</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0 Tomorrow</td>
<td>0.9</td>
<td>0.3</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>1 Tomorrow</td>
<td>0.05</td>
<td>0.7</td>
<td>0.4</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>2 Tomorrow</td>
<td>0.025</td>
<td>0</td>
<td>0.6</td>
<td>0.6</td>
<td>0</td>
</tr>
<tr class="even">
<td>3 Tomorrow</td>
<td>0.015</td>
<td>0</td>
<td>0</td>
<td>0.4</td>
<td>0.8</td>
</tr>
<tr class="odd">
<td>4 Tomorrow</td>
<td>0.01</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0.2</td>
</tr>
</tbody>
</table>
<p>Mathematically, if <span class="math inline">\(\boldsymbol{s}_k\)</span> is the state at day <span class="math inline">\(k\)</span> and <span class="math inline">\(A\)</span> is the matrix given in the table above then the difference equation <span class="math inline">\(\boldsymbol{s}_{k+1} = A \boldsymbol{s}_k\)</span> shows how a state will transition from day to day. For example, if we are currently in Stage 0 then <span class="math display">\[ \boldsymbol{s}_0 = \begin{pmatrix} 1 \\ 0 \\ 0 \\ 0 \\ 0 \end{pmatrix}. \]</span>
We can interpret this as “<em>there is a probability of 1 that we are in Stage 0 today and there is a probability of 0 that we are in any other stage today.</em>”</p>
<p>If we want to advance this model forward in time then we just need to iterate. In our example, the state tomorrow would be <span class="math inline">\(\boldsymbol{s}_1 = A \boldsymbol{s}_0\)</span>. The state two days from now would be <span class="math inline">\(\boldsymbol{s}_2 = A \boldsymbol{s}_1\)</span>, and if we use the expression for <span class="math inline">\(\boldsymbol{s}_1\)</span> we can simplify to <span class="math inline">\(\boldsymbol{s}_2 = A^2 \boldsymbol{s}_0\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>Prove that the state at day <span class="math inline">\(n\)</span> is <span class="math inline">\(\boldsymbol{s}_n = A^n \boldsymbol{s}_0\)</span>.</li>
<li>If <span class="math inline">\(n\)</span> is large then the steady state solution to the difference equation in part (a) is given exactly by the power method iteration that we have studied in this chapter. Hence, as the iterations proceed they will be pulled toward the dominant eigenvector. Use the power method to find the dominant eigenvector of the matrix <span class="math inline">\(A\)</span>.<br />
</li>
<li>The vectors in this problem are called <strong>probability vectors</strong> in the sense that the vectors sum to 1 and every entry can be interpreted as a probability. Re-scale your answer from part (b) so that we can interpret the entries as probabilities. That is, ensure that the sum of the vector from part (b) is 1.<br />
</li>
<li>Interpret your answer to part (c) in the context of the problem. Be sure that your interpretation could be well understood by someone that does not know the mathematics that you just did.</li>
</ol>
</div>

<hr />
<div style="page-break-after: always;"></div>
</div>
</div>
<div id="projects-2" class="section level2">
<h2><span class="header-section-number">4.9</span> Projects</h2>
<p>In this section we propose several ideas for projects related to
numerical linear algebra. These projects are meant to be open ended, to
encourage creative mathematics, to push your coding skills, and to
require you to write and communicate your mathematics. Take the time to
read Appendix <a href="ch-writing.html#ch:writing">B</a> before you write your final solution.</p>
<div id="the-google-page-rank-algorithm" class="section level3">
<h3><span class="header-section-number">4.9.1</span> The Google Page Rank Algorithm</h3>
<p>In this project you will discover how the Page Rank algorithm works to
give the most relevant information as the top hit on a Google search.</p>
<p>Search engines compile large indexes of the dynamic information on the
Internet so they are easily searched. This means that when you do a
Google search, you are not actually searching the Internet; instead, you
are searching the indexes at Google.</p>
<p>When you type a query into Google the following two steps take place:</p>
<ol style="list-style-type: decimal">
<li><p>Query Module: The query module at Google converts your natural
language into a language that the search system can understand and
consults the various indexes at Google in order to answer the query.
This is done to find the list of relevant pages.</p></li>
<li><p>Ranking Module: The ranking module takes the set of relevant pages
and ranks them. The outcome of the ranking is an ordered list of web
pages such that the pages near the top of the list are most likely
to be what you desire from your search. This ranking is the same as
assigning a <em>popularity score</em> to each web site and then listing the
relevant sites by this score.</p></li>
</ol>
<p>This section focuses on the Linear Algebra behind the Ranking Module
developed by the founders of Google: Sergey Brin and Larry Page. Their
algorithm is called the <em>Page Rank algorithm</em>, and you use it every
single time you use Google’s search engine.</p>
<p>In simple terms: <em>A webpage is important if it is pointed to by other
important pages</em>.</p>
<p>The Internet can be viewed as a directed graph (look up this term <a href="https://en.wikipedia.org/wiki/Directed_graph">here
on Wikipedia</a>) where the
nodes are the web pages and the edges are the hyperlinks between the
pages. The hyperlinks into a page are called <em>in links</em>, and the ones
pointing out of a page are called <em>out links</em>. In essence, a hyperlink
from my page to yours is my endorsement of your page. Thus, a page with
more recommendations must be more important than a page with a few
links. However, the status of the recommendation is also important.</p>
<p>Let us now translate this into mathematics. To help understand this we
first consider the small web of six pages shown in Figure
<a href="ch-linearalgebra.html#fig:example-graph">4.3</a> (a graph of the router level of the
internet can be found
<a href="https://personalpages.manchester.ac.uk/staff/m.dodge/cybergeography/atlas/lumeta_large.jpg">here</a>).
The links between the pages are shown by arrows. An arrow pointing into
a node is an <em>in link</em> and an arrow pointing out of a node is an
<em>out link</em>. In Figure <a href="ch-linearalgebra.html#fig:example-graph">4.3</a>, node 3 has three out links (to nodes 1,
2, and 5) and 1 in link (from node 1).</p>
<div class="figure" style="text-align: center"><span id="fig:example-graph"></span>
<img src="images/Ch04_GoogleGraph1.png" alt="Example web graph." width="20%" />
<p class="caption">
Figure 4.3: Example web graph.
</p>
</div>
<p>We will first define some notation in the Page Rank algorithm:</p>
<ul>
<li><p><span class="math inline">\(|P_i|\)</span> is the number of out links from page <span class="math inline">\(P_i\)</span></p></li>
<li><p><span class="math inline">\(H\)</span> is the <em>hyperlink</em> matrix defined as
<span class="math display">\[H_{ij} = \left\{ \begin{array}{cl} \frac{1}{|P_j|}, &amp; \text{if there is a link
            from node $j$ to node $i$} \\ 0, &amp; \text{otherwise} \end{array} \right.\]</span>
where the “<span class="math inline">\(i\)</span>” and “<span class="math inline">\(j\)</span>” are the row and column indices
respectively.</p></li>
<li><p><span class="math inline">\(\boldsymbol{x}\)</span> is a vector that contains all of the Page Ranks for the
individual pages.</p></li>
</ul>
<p>The Page Rank algorithm works as follows:</p>
<ol style="list-style-type: decimal">
<li><p>Initialize the page ranks to all be equal. This means that our
initial assumption is that all pages are of equal rank. In the case
of Figure <a href="ch-linearalgebra.html#fig:example-graph">4.3</a> we would take <span class="math inline">\(\boldsymbol{x}_0\)</span> to be
<span class="math display">\[\boldsymbol{x}_0 = \begin{pmatrix} 1/6 \\ 1/6 \\ 1/6 \\ 1/6 \\ 1/6 \\ 1/6 \end{pmatrix}.\]</span></p></li>
<li><p>Build the hyperlink matrix.<br />
As an example we’ll consider node 3 in Figure <a href="ch-linearalgebra.html#fig:example-graph">4.3</a>. There are three out links from node 3
(to nodes 1, 2, and 5). Hence <span class="math inline">\(H_{13}=1/3\)</span>, <span class="math inline">\(H_{23} = 1/3\)</span>, and
<span class="math inline">\(H_{53} = 1/3\)</span> and the partially complete hyperlink matrix is
<span class="math display">\[H = \begin{pmatrix} 
                - &amp; - &amp; 1/3 &amp; - &amp; - &amp; - \\
                - &amp; - &amp; 1/3 &amp; - &amp; - &amp; - \\
                - &amp; - &amp; 0   &amp; - &amp; - &amp; - \\
                - &amp; - &amp; 0   &amp; - &amp; - &amp; - \\
                - &amp; - &amp; 1/3 &amp; - &amp; - &amp; - \\
                - &amp; - &amp; 0   &amp; - &amp; - &amp; - 
            \end{pmatrix}\]</span></p></li>
<li><p>The difference equation <span class="math inline">\(\boldsymbol{x}_{n+1} = H \boldsymbol{x}_n\)</span> is used to iteratively
refine the estimates of the page ranks. You can view the iterations
as a person visiting a page and then following a link at random,
then following a random link on the next page, and the next, and the
next, etc. Hence we see that the iterations evolve exactly as
expected for a difference equation.</p></li>
</ol>
<table>
<thead>
<tr class="header">
<th>Iteration</th>
<th>New Page Rank Estimation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td><span class="math inline">\(\boldsymbol{x}_0\)</span></td>
</tr>
<tr class="even">
<td>1</td>
<td><span class="math inline">\(\boldsymbol{x}_1 = H \boldsymbol{x}_0\)</span></td>
</tr>
<tr class="odd">
<td>2</td>
<td><span class="math inline">\(\boldsymbol{x}_2 = H \boldsymbol{x}_1 = H^2 \boldsymbol{x}_0\)</span></td>
</tr>
<tr class="even">
<td>3</td>
<td><span class="math inline">\(\boldsymbol{x}_3 = H \boldsymbol{x}_2 = H^3 \boldsymbol{x}_0\)</span></td>
</tr>
<tr class="odd">
<td>4</td>
<td><span class="math inline">\(\boldsymbol{x}_4 = H \boldsymbol{x}_3 = H^4 \boldsymbol{x}_0\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(k\)</span></td>
<td><span class="math inline">\(\boldsymbol{x}_k = H^k \boldsymbol{x}_0\)</span></td>
</tr>
</tbody>
</table>
<ol start="4" style="list-style-type: decimal">
<li>When a steady state is reached we sort the resulting vector <span class="math inline">\(\boldsymbol{x}_k\)</span>
to give the page rank. The node (web page) with the highest rank
will be the top search result, the second highest rank will be the
second search result, and so on.</li>
</ol>
<p>It doesn’t take much to see that this process can be very time
consuming. Think about your typical web search with hundreds of
thousands of hits; that makes a square matrix <span class="math inline">\(H\)</span> that has a size of
hundreds of thousands of entries by hundreds of thousands of entries!
The matrix multiplications alone would take many minutes (or possibly
many hours) for every search! …but Brin and Page were pretty smart
dudes!!</p>
<p>We now state a few theorems and definitions that will help us simplify
the iterative Page Rank process.</p>
<hr />

<div class="theorem">
<span id="thm:eigen-expand" class="theorem"><strong>Theorem 4.7  </strong></span>If
<span class="math inline">\(A\)</span> is an <span class="math inline">\(n \times n\)</span> matrix with <span class="math inline">\(n\)</span> linearly independent eigenvectors
<span class="math inline">\(\boldsymbol{v}_1,  \boldsymbol{v}_2, \boldsymbol{v}_3,\)</span> <span class="math inline">\(\ldots, \boldsymbol{v}_n\)</span> and associated eigenvalues
<span class="math inline">\(\lambda_1, \lambda_2,  \lambda_3, \ldots, \lambda_n\)</span> then for any initial vector
<span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^n\)</span> we can write <span class="math inline">\(A^k \boldsymbol{x}\)</span> as
<span class="math display">\[A^k \boldsymbol{x} = c_1 \lambda_1^k \boldsymbol{v}_1 + c_2 \lambda_2^k \boldsymbol{v}_2 + c_3 \lambda_3^k \boldsymbol{v}_3 +
        \cdots c_n \lambda_n^k \boldsymbol{v}_n\]</span> where
<span class="math inline">\(c_1, c_2, c_3, \ldots, c_n\)</span> are the constants found by expressing <span class="math inline">\(\boldsymbol{x}\)</span>
as a linear combination of the eigenvectors.<br />
Note: We can assume that the eigenvalues are ordered such that
<span class="math inline">\(|\lambda_1| &gt; |\lambda_2| \ge |\lambda_3| \ge \cdots \ge |\lambda_n|\)</span>.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-408" class="exercise"><strong>Exercise 4.92  </strong></span>Prove the preceding theorem.
</div>

<hr />
<p>A <strong>probability vector</strong> is a vector with entries on the interval
<span class="math inline">\([0,1]\)</span> that add up to 1.</p>
<p>A <strong>stochastic matrix</strong> is a square matrix whose columns are probability
vectors.</p>
<hr />

<div class="theorem">
<span id="thm:largest-ev-stochastic" class="theorem"><strong>Theorem 4.8  </strong></span>If <span class="math inline">\(A\)</span> is a stochastic <span class="math inline">\(n \times n\)</span>
matrix then <span class="math inline">\(A\)</span> will have <span class="math inline">\(n\)</span> linearly independent eigenvectors.
Furthermore, the largest eigenvalue of a stochastic matrix will be
<span class="math inline">\(\lambda_1 = 1\)</span> and the smallest eigenvalue will always be nonnegative:
<span class="math inline">\(0 \le |\lambda_n| &lt; 1\)</span>.
</div>

<p>Some of the following tasks will ask you to <em>prove</em> a statement or a
theorem. This means to clearly write all of the logical and mathematical
reasons why the statement is true. Your proof should be absolutely
crystal clear to anyone with a similar mathematical background …if you
are in doubt then have a peer from a different group read your proof to
you .</p>
<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-409" class="exercise"><strong>Exercise 4.93  </strong></span>Finish writing the hyperlink matrix <span class="math inline">\(H\)</span> from Figure <a href="ch-linearalgebra.html#fig:example-graph">4.3</a>.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-410" class="exercise"><strong>Exercise 4.94  </strong></span>Write code to implement the iterative process defined previously. Make a
plot that shows how the rank evolves over the iterations.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-411" class="exercise"><strong>Exercise 4.95  </strong></span>What must be true about a collection of <span class="math inline">\(n\)</span> pages such that an
<span class="math inline">\(n\times n\)</span> hyperlink matrix <span class="math inline">\(H\)</span> is a stochastic matrix.
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-412" class="exercise"><strong>Exercise 4.96  </strong></span>The statement of the next theorem is incomplete, but the proof is given
to you. Fill in the blank in the statement of the theorem and provide a
few sentences supporting your answer.
</div>

<hr />

<div class="theorem">
<span id="thm:steady" class="theorem"><strong>Theorem 4.9  </strong></span>If <span class="math inline">\(A\)</span> is an
<span class="math inline">\(n \times n\)</span> stochastic matrix and <span class="math inline">\(\boldsymbol{x}_0\)</span> is some initial vector for
the difference equation <span class="math inline">\(\boldsymbol{x}_{n+1} = A \boldsymbol{x}_n\)</span>, then the steady state
vector is
<span class="math display">\[\boldsymbol{x}_{equilib} = \lim_{k \to\infty} A^k \boldsymbol{x}_0 = \underline{\hspace{1in}}.\]</span>
</div>

<p><strong>Proof:</strong></p>
<p>First note that <span class="math inline">\(A\)</span> is an <span class="math inline">\(n \times n\)</span> stochastic matrix so from Theorem <a href="ch-linearalgebra.html#thm:largest-ev-stochastic">4.8</a> we know that there are <span class="math inline">\(n\)</span>
linearly independent eigenvectors. We can then substitute the
eigenvalues from Theorem <a href="ch-linearalgebra.html#thm:largest-ev-stochastic">4.8</a> in Theorem <a href="ch-linearalgebra.html#thm:eigen-expand">4.7</a>. Noting that if <span class="math inline">\(0&lt;\lambda_j&lt;1\)</span> we have
<span class="math inline">\(\lim_{k \to  \infty} \lambda_j^k = 0\)</span> the result follows immediately.</p>
<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-413" class="exercise"><strong>Exercise 4.97  </strong></span>Discuss how Theorem <a href="ch-linearalgebra.html#thm:steady">4.9</a> greatly simplifies the PageRank iterative
process described previously. In other words: there is no reason to
iterate at all. Instead, just find … what?
</div>

<hr />

<div class="exercise">
<span id="exr:unnamed-chunk-414" class="exercise"><strong>Exercise 4.98  </strong></span>Now use the previous two problems to find the resulting PageRank vector
from the web in Figure <a href="ch-linearalgebra.html#fig:example-graph">4.3</a>? Be sure to rank the pages in order of
importance. Compare your answer to the one that you got in problem 2.
</div>

<hr />
<div class="figure" style="text-align: center"><span id="fig:graph2"></span>
<img src="images/Ch04_GoogleGraph2.png" alt="A second example web graph." width="20%" />
<p class="caption">
Figure 4.4: A second example web graph.
</p>
</div>

<div class="exercise">
<p><span id="exr:unnamed-chunk-415" class="exercise"><strong>Exercise 4.99  </strong></span>Consider the web in Figure <a href="ch-linearalgebra.html#fig:graph2">4.4</a>.</p>
<ol style="list-style-type: decimal">
<li><p>Write the <span class="math inline">\(H\)</span> matrix and find the initial state <span class="math inline">\(\boldsymbol{x}_0\)</span>,</p></li>
<li><p>Find steady state PageRank vector using the two different methods
described: one using the iterative difference equation and the other
using Theorem <a href="ch-linearalgebra.html#thm:steady">4.9</a> and the dominant eigenvector.</p></li>
<li><p>Rank the pages in order of importance.</p></li>
</ol>
</div>

<hr />

<div class="exercise">
<p><span id="exr:unnamed-chunk-416" class="exercise"><strong>Exercise 4.100  </strong></span>One thing that we didn’t consider in this version of the Google Page
Rank algorithm is the random behavior of humans. One, admittedly
slightly naive, modification that we can make to the present algorithm
is to assume that the person surfing the web will randomly jump to any
other page in the web at any time. For example, if someone is on page 1
in Figure <a href="ch-linearalgebra.html#fig:graph2">4.4</a> then they could randomly jump to any page 2 - 8.
They also have links to pages 2, 3, and 7. That is a total of 10
possible next steps for the web surfer. There is a <span class="math inline">\(2/10\)</span> chance of
heading to page 2. One of those is following the link from page 1 to
page 2 and the other is a random jump to page 2 without following the
link. Similarly, there is a <span class="math inline">\(2/10\)</span> chance of heading to page 3, <span class="math inline">\(2/10\)</span>
chance of heading to page 7, and a <span class="math inline">\(1/10\)</span> chance of randomly heading to
any other page.</p>
Implement this new algorithm, called the <em>random surfer algorithm</em>, on
the web in Figure <a href="ch-linearalgebra.html#fig:graph2">4.4</a>. Compare your ranking to the non-random surfer
results from the previous problem.
</div>

<hr />
</div>
<div id="alternative-methods-to-solving-a-boldsymbolx-boldsymbolb" class="section level3">
<h3><span class="header-section-number">4.9.2</span> Alternative Methods To Solving <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span></h3>
<p>Throughout most of the linear algebra chapter we have studied ways to solve systems of
equations of the form <span class="math inline">\(A\boldsymbol{x} = \boldsymbol{b}\)</span> where <span class="math inline">\(A\)</span> is a square <span class="math inline">\(n \times n\)</span> matrix, <span class="math inline">\(\boldsymbol{x} \in \mathbb{R}^n\)</span>, and <span class="math inline">\(\boldsymbol{b} \in\mathbb{R}^n\)</span>. We have reviewed by-hand row reduction and learned new techniques such as the <span class="math inline">\(LU\)</span>
decomposition and the <span class="math inline">\(QR\)</span> decomposition – all of which are great in their own right and
all of which have their shortcomings.</p>
<p>Both <span class="math inline">\(LU\)</span> and <span class="math inline">\(QR\)</span> are great solution techniques and they generally work very very well.
However (no surprise), we can build algorithms that will<em>usually</em> be faster!</p>
<p>In the following new algorithms we want to solve the linear system of equations
<span class="math display">\[ A \boldsymbol{x} = \boldsymbol{b} \]</span>
but in each we will do so iteratively by applying an algorithm over and over until the algorithm converges to an approximation of the solution vector <span class="math inline">\(\boldsymbol{x}\)</span>. Convergence here means that <span class="math inline">\(\|A\boldsymbol{{x} - \boldsymbol{b}\|\)</span> is less than some pre-determined tolerance.</p>
<p><strong>Method 1:</strong> Start by ``factoring’’<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> the matrix <span class="math inline">\(A\)</span> into <span class="math inline">\(A = L + U\)</span> where
<span class="math inline">\(L\)</span> is a lower triangular matrix and <span class="math inline">\(U\)</span> is an upper triangular matrix. Take note
that this time we will not force the diagonal entries of <span class="math inline">\(L\)</span> to be <span class="math inline">\(1\)</span> like we
did in the classical <span class="math inline">\(LU\)</span> factorization . The <span class="math inline">\(U\)</span> in the factorization <span class="math inline">\(A = L + U\)</span>
is an upper triangular matrix where the entries on the main diagonal are exactly 0.</p>
<p>Specifically,
<span class="math display">\[ A = L + U = \begin{pmatrix} a_{00} &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
                                       a_{10} &amp; a_{11} &amp; 0 &amp; \cdots &amp; 0 \\
                                       a_{20} &amp; a_{21} &amp; a_{22} &amp; \cdots &amp; 0 \\ 
                                       \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                                   a_{n0} &amp; a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{n-1,n-1}
                           \end{pmatrix} + 
                        \begin{pmatrix}
                            0 &amp; a_{01} &amp; a_{02} &amp; \cdots &amp; a_{0,n-1} \\
                            0 &amp; 0 &amp; a_{12} &amp; \cdots &amp; a_{1,n-1} \\
                            0 &amp; 0 &amp; 0 &amp; a_{23} &amp; \cdots &amp; \\
                            \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
                            0 &amp; 0 &amp; 0 &amp; &amp; 0
                        \end{pmatrix}.
                       \]</span></p>
<p>As an example,</p>
<p><span class="math display">\[ \begin{pmatrix} 2 &amp; 3 &amp; 4 \\ 
                           5 &amp; 6 &amp; 7 \\
                       8 &amp; 9 &amp; 1 \end{pmatrix} = \begin{pmatrix} 2 &amp; 0 &amp; 0 \\ 5 &amp; 6 &amp; 0 \\
                   8 &amp; 9 &amp; 1\end{pmatrix} + \begin{pmatrix} 0 &amp; 3 &amp; 4 \\ 0 &amp; 0 &amp; 7 \\ 0 &amp;
           0 &amp; 0 \end{pmatrix}. \]</span></p>
<p>After factoring the system of equations can be rewritten as
<span class="math display">\[ A \boldsymbol{x} = \boldsymbol{b} \implies (L+U)x = \boldsymbol{b} 
        \implies L\boldsymbol{x} + U\boldsymbol{x} = \boldsymbol{b}. \]</span>
Moving the term <span class="math inline">\(U\boldsymbol{x}\)</span> to the right-hand side gives <span class="math inline">\(L\boldsymbol{x} = b-U\boldsymbol{x}\)</span>, and if we solve for the unknown <span class="math inline">\(\boldsymbol{x}\)</span> we get <span class="math inline">\(\boldsymbol{x} = L^{-1}(\boldsymbol{b}-U\boldsymbol{x}).\)</span></p>
<p>Of course we would never (<em>ever</em>!) actually compute the inverse of <span class="math inline">\(L\)</span>, and consequently we have to do something else in place of the matrix inverse. Stop and think here for a moment. We’ve run into this problem earlier in this chapter and you have some code that you will need to modify for
this job (but take very careful note that the <span class="math inline">\(L\)</span> matrix here does not quite have the same structure as the <span class="math inline">\(L\)</span> matrix we used in the past). Moreover, notice that we have the unknown <span class="math inline">\(\boldsymbol{x}\)</span> on both sides of the equation. Initially this may seem like nonsense, but if we treat this as an iterative scheme by first making a <strong>guess</strong> about <span class="math inline">\(x\)</span> and then iteratively find better approximations of solutions via the difference equation
<span class="math display">\[ \boldsymbol{x}_{k+1} = L^{-1} (b-U\boldsymbol{x}_k) \]</span>
we may, under moderate conditions on <span class="math inline">\(A\)</span>, quickly be able to approximate the solution to <span class="math inline">\(A\boldsymbol{x}=\boldsymbol{b}\)</span>. The subscripts in the iterative scheme represet the iteration number. Hence,
<span class="math display">\[ \boldsymbol{x}_{1} = L^{-1} (b-U\boldsymbol{x}_0) \]</span>
<span class="math display">\[ \boldsymbol{x}_{2} = L^{-1} (b-U\boldsymbol{x}_1) \]</span>
<span class="math display">\[ \vdots \]</span></p>
<p>What we need to pay attention to is that the method is not guaranteed to converge to the actual solution to the equation <span class="math inline">\(A \boldsymbol{x} = \boldsymbol{b}\)</span> unless some conditions on <span class="math inline">\(A\)</span> are met, and you will need to experiemnt with the algorithm to come up with a conjecture about the appropriate conditions.</p>
<p><strong>Method 2:</strong> Start by factoring the matrix <span class="math inline">\(A\)</span> into <span class="math inline">\(A = L + D + U\)</span> where <span class="math inline">\(L\)</span> is strictly lower triangular (0’s on the main diagonal and in the entire upper triangle), <span class="math inline">\(D\)</span> is a diagonal matrix, and <span class="math inline">\(U\)</span> is a strictly upper triangular matrix (0’s on the main diagonal and in the entire lower triangle). In this new factorization, the diagonal matrix <span class="math inline">\(D\)</span> simply contains the entries from the main diagonal of <span class="math inline">\(A\)</span>. The <span class="math inline">\(L\)</span> matrix is the lower triangle of <span class="math inline">\(A\)</span>, and the <span class="math inline">\(U\)</span> matrix is the upper triangle of <span class="math inline">\(A\)</span>.</p>
<p>Considering the system of equations <span class="math inline">\(A\boldsymbol{x} = \boldsymbol{b}\)</span> we get
<span class="math display">\[  (L+D+U)\boldsymbol{x} = \boldsymbol{b} \]</span>
and after simplifying, rearranging, and solving for <span class="math inline">\(\boldsymbol{x}\)</span> we get <span class="math inline">\(\boldsymbol{x} = D^{-1} (b-L\boldsymbol{x}-U\boldsymbol{x}).\)</span> A moment’s relection should reveal that the inverse of <span class="math inline">\(D\)</span> is really easy to find (no heavy-duty linear algebra necessary) if some mild conditions on the diagonal entries of <span class="math inline">\(A\)</span> are met. Like before there is an <span class="math inline">\(\boldsymbol{x}\)</span> on both sides of the equation, but if we again make the algorithm iterative we can get successive approximations of the solution with
<span class="math display">\[ \boldsymbol{x}_{k+1} = D^{-1}(b - L\boldsymbol{x}_k - U\boldsymbol{x}_k). \]</span></p>
<p><strong>Your Tasks:</strong></p>

<ol style="list-style-type: decimal">
<li>Pick a small (larger than <span class="math inline">\(3 \times 3\)</span>) matrix and an appropriate right-hand side <span class="math inline">\(\boldsymbol{b}\)</span> and work each of the algorithms by hand. You do not need to write this step up in the final product, but this exercise will help you locate where things may go wrong in the algorithms and what conditions we might need on <span class="math inline">\(A\)</span> in order to get convergent sequences of approximate solutions.</li>
<li>Build Python functions that accept a square matrix <span class="math inline">\(A\)</span> and complete the factorizations <span class="math inline">\(A = L+U\)</span> and <span class="math inline">\(A = L+D+U\)</span>.</li>
<li>Build functions to implement the two methods and then demonstrate that the methods work on a handful of carefully chosen test examples. As part of these functions you need to build a way to deal with the matrix inversions as well as build a stopping rule for the iterative schemes. Hint: You should use a <code>while</code> loop with a proper logical condition. Think carefully about what we’re finding at each iteration and what we can use to check our accuracy at each iteration. It would also be wise to write your code in such a way that it checks to see if the sequence of
approximations is diverging.</li>
<li>Discuss where each method might fail and then demonstrate the possible failures with
several carefully chosen examples. Stick to small examples and work these out by
hand to clearly show the failure.</li>
<li>Iterative methods such as these will produce a sequence of approximations, but there is no guranatee that either method will actually produce a convergent sequence. Experiment with several examples and propose a condition on the matrix <span class="math inline">\(A\)</span> which will likely result in a convergent sequence. Demonstrate that the methods fail if your condition is violated and that the methods converge if your condition is met. Take care that it is tempting to think that your code is broken if it doesn’t converge. The more likely scenario is that the problem that you have chosen to solve will result in a non-convergent sequence of iterations, and you need to think and experiment carefully when choosing the example problems to solve. One such convergence criterion has something to do with the diagonal entries of <span class="math inline">\(A\)</span> relative to the other entries, but that doesn’t mean that you shouldn’t explore other features of the matrices as well (I-gen can’t give you any more hints than that). This task is not asking for a proof; just a conjecture and convincing numerical evidence that the conjecture holds. The actual proofs are beyond the scope of this project and this course.</li>
<li>Devise a way to demonstrate how the time to solve a large linear system <span class="math inline">\(A\boldsymbol{x} = \boldsymbol{b}\)</span> compares between our two new methods, the <span class="math inline">\(LU\)</span> algorithm, and the <span class="math inline">\(QR\)</span> algorithm that we built earlier in the chapter. Conclude this demonstration with apropriate plots and ample discussion.</li>
</ol>
<p>You need to do this project without the help of your old buddy Google. All code must be
originally yours or be modified from code that we built in class. You can ask Google how Python works with matrices and the like, but searching directly for the algorithms (which are actually well-known, well-studied, and named algorithms) is not allowed.</p>
<p>Finally, solving systems of equations with the <code>|np.linalg.solve()</code> command can only be done to verify or check your answer(s).</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-meerschaert">
<p>[6] M. Meerschaert, <em>Mathematical modeling, 4ed</em>. Academic Press, 2013.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="5">
<li id="fn5"><p>You should also note that <span class="math inline">\(\|\boldsymbol{u}\| = \sqrt{\boldsymbol{u} \cdot \boldsymbol{u}}\)</span> is not the only definition of distance. More generally, if you let <span class="math inline">\(\left&lt; \boldsymbol{u}, \boldsymbol{v}\right&gt;\)</span> be an inner product for <span class="math inline">\(\boldsymbol{u}\)</span> and <span class="math inline">\(\boldsymbol{v}\)</span> in some vector space <span class="math inline">\(\mathcal{V}\)</span> then <span class="math inline">\(\|\boldsymbol{u}\| = \sqrt{\left&lt; \boldsymbol{u}, \boldsymbol{u}\right&gt;}\)</span>. In most cases in this text we will be using the dot product as our prefered inner product so we won’t have to worry much about this particular natural extension of the definition of the length of a vector. <a href="ch-linearalgebra.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>You might have thought that <em>naive multiplication</em> was a much more natural way to do matrix multiplication when you first saw it. Hopefully now you see the power in the definition of matrix multiplication that we actually use. If not, then I give you this moment to ponder that (a) matrix multiplication is just a bunch of dot products, and (b) dot products can be seen as projections. Hence, matrix multiplication is really just a projection of the rows of <span class="math inline">\(A\)</span> onto the columns of <span class="math inline">\(B\)</span>. This has much more rich geometric flavor than <em>naive multiplication</em>.<a href="ch-linearalgebra.html#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>Take careful note here. We have actually just built a special case of the <span class="math inline">\(LU\)</span> decomposition. Remember that in row reduction you are allowed to swap the order of the rows, but in our <span class="math inline">\(LU\)</span> algorithm we don’t have any row swaps. The version of <span class="math inline">\(LU\)</span> with row swaps is called <span class="math inline">\(LU\)</span> with partial pivoting. We won’t built the full partial pivoting algorithm in this text but feel free to look it up. The <a href="https://en.wikipedia.org/wiki/LU_decomposition#LU_factorization_with_partial_pivoting">wikipedia page</a> is a decent place to start. What you’ll find is that there are indeed many different versions of the <span class="math inline">\(LU\)</span> decomposition.<a href="ch-linearalgebra.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>Numerical Linear Algebra is a huge field and there is way more to say … but alas, this is an introductory course in numerical methods so we can’t do everything. Sigh.<a href="ch-linearalgebra.html#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>To build a matrix with specific eigenvalues it may be helpful to recall the matrix factorization <span class="math inline">\(A = PDP^{-1}\)</span> where the columns of <span class="math inline">\(P\)</span> are the eigenvectors of <span class="math inline">\(A\)</span> and the diagonal entries of <span class="math inline">\(D\)</span> are the eigenvalues. If you choose <span class="math inline">\(P\)</span> and <span class="math inline">\(D\)</span> then you can build <span class="math inline">\(A\)</span> with your specific eigen-structure. If you are looking for complex eigenvalues then remember that the eigenvectors may well be complex too.<a href="ch-linearalgebra.html#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>Actually, the determinant computation uses LU with partial pivoting which we did not cover here in the text. What we are looking at in this exercise is a smaller subcase of what happens when you have a matrix <span class="math inline">\(A\)</span> that does not require any row swaps in the row reduction process.<a href="ch-linearalgebra.html#fnref10" class="footnote-back">↩︎</a></p></li>
<li id="fn11"><p>Technically speaking we should not call this a “factorization” since we have not split the matrix <span class="math inline">\(A\)</span> into a product of two matrices. Instead we should call it a “partition” since in number theory we call the process of breaking an integer into the sum of two integers is called a “partition”. Even so, we will still use the word factorization here for simpllicity.<a href="ch-linearalgebra.html#fnref11" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ch-calculus.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch-odes.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
